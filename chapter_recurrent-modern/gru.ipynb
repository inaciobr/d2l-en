{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04392c0b",
   "metadata": {},
   "source": [
    "# Gated Recurrent Units (GRU)\n",
    ":label:`sec_gru`\n",
    "\n",
    "\n",
    "As RNNs and particularly the LSTM architecture (:numref:`sec_lstm`)\n",
    "rapidly gained popularity during the 2010s,\n",
    "a number of researchers began to experiment \n",
    "with simplified architectures in hopes \n",
    "of retaining the key idea of incorporating\n",
    "an internal state and multiplicative gating mechanisms\n",
    "but with the aim of speeding up computation.\n",
    "The gated recurrent unit (GRU) :cite:`Cho.Van-Merrienboer.Bahdanau.ea.2014` \n",
    "offered a streamlined version of the LSTM memory cell\n",
    "that often achieves comparable performance\n",
    "but with the advantage of being faster \n",
    "to compute :cite:`Chung.Gulcehre.Cho.ea.2014`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaba6e97",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext d2lbook.tab\n",
    "tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "748c83fb",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "%%tab mxnet\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import np, npx\n",
    "from mxnet.gluon import rnn\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d23ce334",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [],
   "source": [
    "%%tab pytorch\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22723e46",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "%%tab tensorflow\n",
    "from d2l import tensorflow as d2l\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97da5e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab jax\n",
    "from d2l import jax as d2l\n",
    "from flax import linen as nn\n",
    "import jax\n",
    "from jax import numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805cd87f",
   "metadata": {},
   "source": [
    "## Reset Gate and Update Gate\n",
    "\n",
    "Here, the LSTM's three gates are replaced by two:\n",
    "the *reset gate* and the *update gate*.\n",
    "As with LSTMs, these gates are given sigmoid activations,\n",
    "forcing their values to lie in the interval $(0, 1)$.\n",
    "Intuitively, the reset gate controls how much of the previous state \n",
    "we might still want to remember.\n",
    "Likewise, an update gate would allow us to control \n",
    "how much of the new state is just a copy of the old one.\n",
    ":numref:`fig_gru_1` illustrates the inputs for both\n",
    "the reset and update gates in a GRU, \n",
    "given the input of the current time step\n",
    "and the hidden state of the previous time step.\n",
    "The outputs of the gates are given \n",
    "by two fully connected layers\n",
    "with a sigmoid activation function.\n",
    "\n",
    "![Computing the reset gate and the update gate in a GRU model.](../img/gru-1.svg)\n",
    ":label:`fig_gru_1`\n",
    "\n",
    "Mathematically, for a given time step $t$,\n",
    "suppose that the input is a minibatch\n",
    "$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$ \n",
    "(number of examples $=n$; number of inputs $=d$)\n",
    "and the hidden state of the previous time step \n",
    "is $\\mathbf{H}_{t-1} \\in \\mathbb{R}^{n \\times h}$ \n",
    "(number of hidden units $=h$). \n",
    "Then the reset gate $\\mathbf{R}_t \\in \\mathbb{R}^{n \\times h}$ \n",
    "and update gate $\\mathbf{Z}_t \\in \\mathbb{R}^{n \\times h}$ are computed as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{R}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{\\textrm{xr}} + \\mathbf{H}_{t-1} \\mathbf{W}_{\\textrm{hr}} + \\mathbf{b}_\\textrm{r}),\\\\\n",
    "\\mathbf{Z}_t = \\sigma(\\mathbf{X}_t \\mathbf{W}_{\\textrm{xz}} + \\mathbf{H}_{t-1} \\mathbf{W}_{\\textrm{hz}} + \\mathbf{b}_\\textrm{z}),\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{W}_{\\textrm{xr}}, \\mathbf{W}_{\\textrm{xz}} \\in \\mathbb{R}^{d \\times h}$ \n",
    "and $\\mathbf{W}_{\\textrm{hr}}, \\mathbf{W}_{\\textrm{hz}} \\in \\mathbb{R}^{h \\times h}$ \n",
    "are weight parameters and $\\mathbf{b}_\\textrm{r}, \\mathbf{b}_\\textrm{z} \\in \\mathbb{R}^{1 \\times h}$ \n",
    "are bias parameters.\n",
    "\n",
    "\n",
    "## Candidate Hidden State\n",
    "\n",
    "Next, we integrate the reset gate $\\mathbf{R}_t$ \n",
    "with the regular updating mechanism\n",
    "in :eqref:`rnn_h_with_state`,\n",
    "leading to the following\n",
    "*candidate hidden state*\n",
    "$\\tilde{\\mathbf{H}}_t \\in \\mathbb{R}^{n \\times h}$ at time step $t$:\n",
    "\n",
    "$$\\tilde{\\mathbf{H}}_t = \\tanh(\\mathbf{X}_t \\mathbf{W}_{\\textrm{xh}} + \\left(\\mathbf{R}_t \\odot \\mathbf{H}_{t-1}\\right) \\mathbf{W}_{\\textrm{hh}} + \\mathbf{b}_\\textrm{h}),$$\n",
    ":eqlabel:`gru_tilde_H`\n",
    "\n",
    "where $\\mathbf{W}_{\\textrm{xh}} \\in \\mathbb{R}^{d \\times h}$ and $\\mathbf{W}_{\\textrm{hh}} \\in \\mathbb{R}^{h \\times h}$\n",
    "are weight parameters,\n",
    "$\\mathbf{b}_\\textrm{h} \\in \\mathbb{R}^{1 \\times h}$\n",
    "is the bias,\n",
    "and the symbol $\\odot$ is the Hadamard (elementwise) product operator.\n",
    "Here we use a tanh activation function.\n",
    "\n",
    "The result is a *candidate*, since we still need \n",
    "to incorporate the action of the update gate.\n",
    "Comparing with :eqref:`rnn_h_with_state`,\n",
    "the influence of the previous states\n",
    "can now be reduced with the\n",
    "elementwise multiplication of\n",
    "$\\mathbf{R}_t$ and $\\mathbf{H}_{t-1}$\n",
    "in :eqref:`gru_tilde_H`.\n",
    "Whenever the entries in the reset gate $\\mathbf{R}_t$ are close to 1, \n",
    "we recover a vanilla RNN such as that in :eqref:`rnn_h_with_state`.\n",
    "For all entries of the reset gate $\\mathbf{R}_t$ that are close to 0, \n",
    "the candidate hidden state is the result of an MLP with $\\mathbf{X}_t$ as input. \n",
    "Any pre-existing hidden state is thus *reset* to defaults.\n",
    "\n",
    ":numref:`fig_gru_2` illustrates the computational flow after applying the reset gate.\n",
    "\n",
    "![Computing the candidate hidden state in a GRU model.](../img/gru-2.svg)\n",
    ":label:`fig_gru_2`\n",
    "\n",
    "\n",
    "## Hidden State\n",
    "\n",
    "Finally, we need to incorporate the effect of the update gate $\\mathbf{Z}_t$.\n",
    "This determines the extent to which the new hidden state $\\mathbf{H}_t \\in \\mathbb{R}^{n \\times h}$ \n",
    "matches the old state $\\mathbf{H}_{t-1}$ compared with how much \n",
    "it resembles the new candidate state $\\tilde{\\mathbf{H}}_t$.\n",
    "The update gate $\\mathbf{Z}_t$ can be used for this purpose, \n",
    "simply by taking elementwise convex combinations \n",
    "of $\\mathbf{H}_{t-1}$ and $\\tilde{\\mathbf{H}}_t$.\n",
    "This leads to the final update equation for the GRU:\n",
    "\n",
    "$$\\mathbf{H}_t = \\mathbf{Z}_t \\odot \\mathbf{H}_{t-1}  + (1 - \\mathbf{Z}_t) \\odot \\tilde{\\mathbf{H}}_t.$$\n",
    "\n",
    "\n",
    "Whenever the update gate $\\mathbf{Z}_t$ is close to 1,\n",
    "we simply retain the old state. \n",
    "In this case the information from $\\mathbf{X}_t$ is ignored, \n",
    "effectively skipping time step $t$ in the dependency chain. \n",
    "By contrast, whenever $\\mathbf{Z}_t$ is close to 0,\n",
    "the new latent state $\\mathbf{H}_t$ approaches the candidate latent state $\\tilde{\\mathbf{H}}_t$. \n",
    ":numref:`fig_gru_3` shows the computational flow after the update gate is in action.\n",
    "\n",
    "![Computing the hidden state in a GRU model.](../img/gru-3.svg)\n",
    ":label:`fig_gru_3`\n",
    "\n",
    "\n",
    "In summary, GRUs have the following two distinguishing features:\n",
    "\n",
    "* Reset gates help capture short-term dependencies in sequences.\n",
    "* Update gates help capture long-term dependencies in sequences.\n",
    "\n",
    "## Implementation from Scratch\n",
    "\n",
    "To gain a better understanding of the GRU model, let's implement it from scratch.\n",
    "\n",
    "### (**Initializing Model Parameters**)\n",
    "\n",
    "The first step is to initialize the model parameters.\n",
    "We draw the weights from a Gaussian distribution\n",
    "with standard deviation to be `sigma` and set the bias to 0. \n",
    "The hyperparameter `num_hiddens` defines the number of hidden units.\n",
    "We instantiate all weights and biases relating to the update gate, \n",
    "the reset gate, and the candidate hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87712a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab pytorch, mxnet, tensorflow\n",
    "class GRUScratch(d2l.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        if tab.selected('mxnet'):\n",
    "            init_weight = lambda *shape: d2l.randn(*shape) * sigma\n",
    "            triple = lambda: (init_weight(num_inputs, num_hiddens),\n",
    "                              init_weight(num_hiddens, num_hiddens),\n",
    "                              d2l.zeros(num_hiddens))            \n",
    "        if tab.selected('pytorch'):\n",
    "            init_weight = lambda *shape: nn.Parameter(d2l.randn(*shape) * sigma)\n",
    "            triple = lambda: (init_weight(num_inputs, num_hiddens),\n",
    "                              init_weight(num_hiddens, num_hiddens),\n",
    "                              nn.Parameter(d2l.zeros(num_hiddens)))\n",
    "        if tab.selected('tensorflow'):\n",
    "            init_weight = lambda *shape: tf.Variable(d2l.normal(shape) * sigma)\n",
    "            triple = lambda: (init_weight(num_inputs, num_hiddens),\n",
    "                              init_weight(num_hiddens, num_hiddens),\n",
    "                              tf.Variable(d2l.zeros(num_hiddens)))            \n",
    "            \n",
    "        self.W_xz, self.W_hz, self.b_z = triple()  # Update gate\n",
    "        self.W_xr, self.W_hr, self.b_r = triple()  # Reset gate\n",
    "        self.W_xh, self.W_hh, self.b_h = triple()  # Candidate hidden state        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6814d359",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab jax\n",
    "class GRUScratch(d2l.Module):\n",
    "    num_inputs: int\n",
    "    num_hiddens: int\n",
    "    sigma: float = 0.01\n",
    "\n",
    "    def setup(self):\n",
    "        init_weight = lambda name, shape: self.param(name,\n",
    "                                                     nn.initializers.normal(self.sigma),\n",
    "                                                     shape)\n",
    "        triple = lambda name : (\n",
    "            init_weight(f'W_x{name}', (self.num_inputs, self.num_hiddens)),\n",
    "            init_weight(f'W_h{name}', (self.num_hiddens, self.num_hiddens)),\n",
    "            self.param(f'b_{name}', nn.initializers.zeros, (self.num_hiddens)))\n",
    "\n",
    "        self.W_xz, self.W_hz, self.b_z = triple('z')  # Update gate\n",
    "        self.W_xr, self.W_hr, self.b_r = triple('r')  # Reset gate\n",
    "        self.W_xh, self.W_hh, self.b_h = triple('h')  # Candidate hidden state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6883b16c",
   "metadata": {},
   "source": [
    "### Defining the Model\n",
    "\n",
    "Now we are ready to [**define the GRU forward computation**].\n",
    "Its structure is the same as that of the basic RNN cell, \n",
    "except that the update equations are more complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31900092",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab pytorch, mxnet, tensorflow\n",
    "@d2l.add_to_class(GRUScratch)\n",
    "def forward(self, inputs, H=None):\n",
    "    if H is None:\n",
    "        # Initial state with shape: (batch_size, num_hiddens)\n",
    "        if tab.selected('mxnet'):\n",
    "            H = d2l.zeros((inputs.shape[1], self.num_hiddens),\n",
    "                          ctx=inputs.ctx)\n",
    "        if tab.selected('pytorch'):\n",
    "            H = d2l.zeros((inputs.shape[1], self.num_hiddens),\n",
    "                          device=inputs.device)\n",
    "        if tab.selected('tensorflow'):\n",
    "            H = d2l.zeros((inputs.shape[1], self.num_hiddens))\n",
    "    outputs = []\n",
    "    for X in inputs:\n",
    "        Z = d2l.sigmoid(d2l.matmul(X, self.W_xz) +\n",
    "                        d2l.matmul(H, self.W_hz) + self.b_z)\n",
    "        R = d2l.sigmoid(d2l.matmul(X, self.W_xr) + \n",
    "                        d2l.matmul(H, self.W_hr) + self.b_r)\n",
    "        H_tilde = d2l.tanh(d2l.matmul(X, self.W_xh) + \n",
    "                           d2l.matmul(R * H, self.W_hh) + self.b_h)\n",
    "        H = Z * H + (1 - Z) * H_tilde\n",
    "        outputs.append(H)\n",
    "    return outputs, H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9350f57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab jax\n",
    "@d2l.add_to_class(GRUScratch)\n",
    "def forward(self, inputs, H=None):\n",
    "    # Use lax.scan primitive instead of looping over the\n",
    "    # inputs, since scan saves time in jit compilation\n",
    "    def scan_fn(H, X):\n",
    "        Z = d2l.sigmoid(d2l.matmul(X, self.W_xz) + d2l.matmul(H, self.W_hz) +\n",
    "                        self.b_z)\n",
    "        R = d2l.sigmoid(d2l.matmul(X, self.W_xr) +\n",
    "                        d2l.matmul(H, self.W_hr) + self.b_r)\n",
    "        H_tilde = d2l.tanh(d2l.matmul(X, self.W_xh) +\n",
    "                           d2l.matmul(R * H, self.W_hh) + self.b_h)\n",
    "        H = Z * H + (1 - Z) * H_tilde\n",
    "        return H, H  # return carry, y\n",
    "\n",
    "    if H is None:\n",
    "        batch_size = inputs.shape[1]\n",
    "        carry = jnp.zeros((batch_size, self.num_hiddens))\n",
    "    else:\n",
    "        carry = H\n",
    "\n",
    "    # scan takes the scan_fn, initial carry state, xs with leading axis to be scanned\n",
    "    carry, outputs = jax.lax.scan(scan_fn, carry, inputs)\n",
    "    return outputs, carry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2063da72",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "[**Training**] a language model on *The Time Machine* dataset\n",
    "works in exactly the same manner as in :numref:`sec_rnn-scratch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4019e2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab all\n",
    "data = d2l.TimeMachine(batch_size=1024, num_steps=32)\n",
    "if tab.selected('mxnet', 'pytorch', 'jax'):\n",
    "    gru = GRUScratch(num_inputs=len(data.vocab), num_hiddens=32)\n",
    "    model = d2l.RNNLMScratch(gru, vocab_size=len(data.vocab), lr=4)\n",
    "    trainer = d2l.Trainer(max_epochs=50, gradient_clip_val=1, num_gpus=1)\n",
    "if tab.selected('tensorflow'):\n",
    "    with d2l.try_gpu():\n",
    "        gru = GRUScratch(num_inputs=len(data.vocab), num_hiddens=32)\n",
    "        model = d2l.RNNLMScratch(gru, vocab_size=len(data.vocab), lr=4)\n",
    "    trainer = d2l.Trainer(max_epochs=50, gradient_clip_val=1)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfca93d",
   "metadata": {},
   "source": [
    "## [**Concise Implementation**]\n",
    "\n",
    "In high-level APIs, we can directly instantiate a GRU model.\n",
    "This encapsulates all the configuration detail that we made explicit above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf9f4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab pytorch, mxnet, tensorflow\n",
    "class GRU(d2l.RNN):\n",
    "    def __init__(self, num_inputs, num_hiddens):\n",
    "        d2l.Module.__init__(self)\n",
    "        self.save_hyperparameters()\n",
    "        if tab.selected('mxnet'):\n",
    "            self.rnn = rnn.GRU(num_hiddens)\n",
    "        if tab.selected('pytorch'):\n",
    "            self.rnn = nn.GRU(num_inputs, num_hiddens)\n",
    "        if tab.selected('tensorflow'):\n",
    "            self.rnn = tf.keras.layers.GRU(num_hiddens, return_sequences=True, \n",
    "                                           return_state=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c6db9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab jax\n",
    "class GRU(d2l.RNN):\n",
    "    num_hiddens: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, inputs, H=None, training=False):\n",
    "        if H is None:\n",
    "            batch_size = inputs.shape[1]\n",
    "            H = nn.GRUCell.initialize_carry(jax.random.PRNGKey(0),\n",
    "                                            (batch_size,), self.num_hiddens)\n",
    "\n",
    "        GRU = nn.scan(nn.GRUCell, variable_broadcast=\"params\",\n",
    "                      in_axes=0, out_axes=0, split_rngs={\"params\": False})\n",
    "\n",
    "        H, outputs = GRU()(H, inputs)\n",
    "        return outputs, H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7f0c8a",
   "metadata": {},
   "source": [
    "The code is significantly faster in training as it uses compiled operators \n",
    "rather than Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1847a6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab all\n",
    "if tab.selected('mxnet', 'pytorch', 'tensorflow'):\n",
    "    gru = GRU(num_inputs=len(data.vocab), num_hiddens=32)\n",
    "if tab.selected('jax'):\n",
    "    gru = GRU(num_hiddens=32)\n",
    "if tab.selected('mxnet', 'pytorch', 'jax'):\n",
    "    model = d2l.RNNLM(gru, vocab_size=len(data.vocab), lr=4)\n",
    "if tab.selected('tensorflow'):\n",
    "    with d2l.try_gpu():\n",
    "        model = d2l.RNNLM(gru, vocab_size=len(data.vocab), lr=4)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05674d0e",
   "metadata": {},
   "source": [
    "After training, we print out the perplexity on the training set\n",
    "and the predicted sequence following the provided prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7387ce85",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab mxnet, pytorch\n",
    "model.predict('it has', 20, data.vocab, d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82471464",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab tensorflow\n",
    "model.predict('it has', 20, data.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f74d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab jax\n",
    "model.predict('it has', 20, data.vocab, trainer.state.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5ba872",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Compared with LSTMs, GRUs achieve similar performance but tend to be lighter computationally.\n",
    "Generally, compared with simple RNNs, gated RNNS, just like LSTMs and GRUs,\n",
    "can better capture dependencies for sequences with large time step distances.\n",
    "GRUs contain basic RNNs as their extreme case whenever the reset gate is switched on. \n",
    "They can also skip subsequences by turning on the update gate.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Assume that we only want to use the input at time step $t'$ to predict the output at time step $t > t'$. What are the best values for the reset and update gates for each time step?\n",
    "1. Adjust the hyperparameters and analyze their influence on running time, perplexity, and the output sequence.\n",
    "1. Compare runtime, perplexity, and the output strings for `rnn.RNN` and `rnn.GRU` implementations with each other.\n",
    "1. What happens if you implement only parts of a GRU, e.g., with only a reset gate or only an update gate?\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/342)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/1056)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "[Discussions](https://discuss.d2l.ai/t/3860)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`jax`\n",
    "[Discussions](https://discuss.d2l.ai/t/18017)\n",
    ":end_tab:"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
