{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b75a55b",
   "metadata": {},
   "source": [
    "# Deep Recurrent Neural Networks\n",
    "\n",
    ":label:`sec_deep_rnn`\n",
    "\n",
    "Up until now, we have focused on defining networks \n",
    "consisting of a sequence input, \n",
    "a single hidden RNN layer,\n",
    "and an output layer. \n",
    "Despite having just one hidden layer \n",
    "between the input at any time step\n",
    "and the corresponding output,\n",
    "there is a sense in which these networks are deep.\n",
    "Inputs from the first time step can influence\n",
    "the outputs at the final time step $T$ \n",
    "(often 100s or 1000s of steps later).\n",
    "These inputs pass through $T$ applications\n",
    "of the recurrent layer before reaching \n",
    "the final output. \n",
    "However, we often also wish to retain the ability\n",
    "to express complex relationships \n",
    "between the inputs at a given time step\n",
    "and the outputs at that same time step.\n",
    "Thus we often construct RNNs that are deep\n",
    "not only in the time direction \n",
    "but also in the input-to-output direction.\n",
    "This is precisely the notion of depth\n",
    "that we have already encountered \n",
    "in our development of MLPs\n",
    "and deep CNNs.\n",
    "\n",
    "\n",
    "The standard method for building this sort of deep RNN \n",
    "is strikingly simple: we stack the RNNs on top of each other. \n",
    "Given a sequence of length $T$, the first RNN produces \n",
    "a sequence of outputs, also of length $T$.\n",
    "These, in turn, constitute the inputs to the next RNN layer. \n",
    "In this short section, we illustrate this design pattern\n",
    "and present a simple example for how to code up such stacked RNNs.\n",
    "Below, in :numref:`fig_deep_rnn`, we illustrate\n",
    "a deep RNN with $L$ hidden layers.\n",
    "Each hidden state operates on a sequential input\n",
    "and produces a sequential output.\n",
    "Moreover, any RNN cell (white box in :numref:`fig_deep_rnn`) at each time step\n",
    "depends on both the same layer's \n",
    "value at the previous time step\n",
    "and the previous layer's value \n",
    "at the same time step. \n",
    "\n",
    "![Architecture of a deep RNN.](../img/deep-rnn.svg)\n",
    ":label:`fig_deep_rnn`\n",
    "\n",
    "Formally, suppose that we have a minibatch input\n",
    "$\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}$ \n",
    "(number of examples $=n$; number of inputs in each example $=d$) at time step $t$.\n",
    "At the same time step, \n",
    "let the hidden state of the $l^\\textrm{th}$ hidden layer ($l=1,\\ldots,L$) be $\\mathbf{H}_t^{(l)} \\in \\mathbb{R}^{n \\times h}$ \n",
    "(number of hidden units $=h$)\n",
    "and the output layer variable be \n",
    "$\\mathbf{O}_t \\in \\mathbb{R}^{n \\times q}$ \n",
    "(number of outputs: $q$).\n",
    "Setting $\\mathbf{H}_t^{(0)} = \\mathbf{X}_t$,\n",
    "the hidden state of\n",
    "the $l^\\textrm{th}$ hidden layer\n",
    "that uses the activation function $\\phi_l$\n",
    "is calculated as follows:\n",
    "\n",
    "$$\\mathbf{H}_t^{(l)} = \\phi_l(\\mathbf{H}_t^{(l-1)} \\mathbf{W}_{\\textrm{xh}}^{(l)} + \\mathbf{H}_{t-1}^{(l)} \\mathbf{W}_{\\textrm{hh}}^{(l)}  + \\mathbf{b}_\\textrm{h}^{(l)}),$$\n",
    ":eqlabel:`eq_deep_rnn_H`\n",
    "\n",
    "where the weights $\\mathbf{W}_{\\textrm{xh}}^{(l)} \\in \\mathbb{R}^{h \\times h}$ and $\\mathbf{W}_{\\textrm{hh}}^{(l)} \\in \\mathbb{R}^{h \\times h}$, together with\n",
    "the bias $\\mathbf{b}_\\textrm{h}^{(l)} \\in \\mathbb{R}^{1 \\times h}$, \n",
    "are the model parameters of the $l^\\textrm{th}$ hidden layer.\n",
    "\n",
    "At the end, the calculation of the output layer \n",
    "is only based on the hidden state \n",
    "of the final $L^\\textrm{th}$ hidden layer:\n",
    "\n",
    "$$\\mathbf{O}_t = \\mathbf{H}_t^{(L)} \\mathbf{W}_{\\textrm{hq}} + \\mathbf{b}_\\textrm{q},$$\n",
    "\n",
    "where the weight $\\mathbf{W}_{\\textrm{hq}} \\in \\mathbb{R}^{h \\times q}$ \n",
    "and the bias $\\mathbf{b}_\\textrm{q} \\in \\mathbb{R}^{1 \\times q}$ \n",
    "are the model parameters of the output layer.\n",
    "\n",
    "Just as with MLPs, the number of hidden layers $L$ \n",
    "and the number of hidden units $h$ are hyperparameters\n",
    "that we can tune.\n",
    "Common RNN layer widths ($h$) are in the range $(64, 2056)$,\n",
    "and common depths ($L$) are in the range $(1, 8)$. \n",
    "In addition, we can easily get a deep-gated RNN\n",
    "by replacing the hidden state computation in :eqref:`eq_deep_rnn_H`\n",
    "with that from an LSTM or a GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479906df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext d2lbook.tab\n",
    "tab.interact_select('mxnet', 'pytorch', 'tensorflow', 'jax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef08be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab mxnet\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import np, npx\n",
    "from mxnet.gluon import rnn\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8222f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab pytorch\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdc5631",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab tensorflow\n",
    "from d2l import tensorflow as d2l\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4055b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab jax\n",
    "from d2l import jax as d2l\n",
    "from flax import linen as nn\n",
    "import jax\n",
    "from jax import numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213db81f",
   "metadata": {},
   "source": [
    "## Implementation from Scratch\n",
    "\n",
    "To implement a multilayer RNN from scratch,\n",
    "we can treat each layer as an `RNNScratch` instance\n",
    "with its own learnable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b729f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab mxnet, tensorflow\n",
    "class StackedRNNScratch(d2l.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, num_layers, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.rnns = [d2l.RNNScratch(num_inputs if i==0 else num_hiddens,\n",
    "                                    num_hiddens, sigma)\n",
    "                     for i in range(num_layers)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43e0280",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab pytorch\n",
    "class StackedRNNScratch(d2l.Module):\n",
    "    def __init__(self, num_inputs, num_hiddens, num_layers, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.rnns = nn.Sequential(*[d2l.RNNScratch(\n",
    "            num_inputs if i==0 else num_hiddens, num_hiddens, sigma)\n",
    "                                    for i in range(num_layers)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a9a1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab jax\n",
    "class StackedRNNScratch(d2l.Module):\n",
    "    num_inputs: int\n",
    "    num_hiddens: int\n",
    "    num_layers: int\n",
    "    sigma: float = 0.01\n",
    "\n",
    "    def setup(self):\n",
    "        self.rnns = [d2l.RNNScratch(self.num_inputs if i==0 else self.num_hiddens,\n",
    "                                    self.num_hiddens, self.sigma)\n",
    "                     for i in range(self.num_layers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e914be8",
   "metadata": {},
   "source": [
    "The multilayer forward computation\n",
    "simply performs forward computation\n",
    "layer by layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a504d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab all\n",
    "@d2l.add_to_class(StackedRNNScratch)\n",
    "def forward(self, inputs, Hs=None):\n",
    "    outputs = inputs\n",
    "    if Hs is None: Hs = [None] * self.num_layers\n",
    "    for i in range(self.num_layers):\n",
    "        outputs, Hs[i] = self.rnns[i](outputs, Hs[i])\n",
    "        outputs = d2l.stack(outputs, 0)\n",
    "    return outputs, Hs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ef0ea8",
   "metadata": {},
   "source": [
    "As an example, we train a deep GRU model on\n",
    "*The Time Machine* dataset (same as in :numref:`sec_rnn-scratch`).\n",
    "To keep things simple we set the number of layers to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9b394d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab all\n",
    "data = d2l.TimeMachine(batch_size=1024, num_steps=32)\n",
    "if tab.selected('mxnet', 'pytorch', 'jax'):\n",
    "    rnn_block = StackedRNNScratch(num_inputs=len(data.vocab),\n",
    "                                  num_hiddens=32, num_layers=2)\n",
    "    model = d2l.RNNLMScratch(rnn_block, vocab_size=len(data.vocab), lr=2)\n",
    "    trainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1, num_gpus=1)\n",
    "if tab.selected('tensorflow'):\n",
    "    with d2l.try_gpu():\n",
    "        rnn_block = StackedRNNScratch(num_inputs=len(data.vocab),\n",
    "                                  num_hiddens=32, num_layers=2)\n",
    "        model = d2l.RNNLMScratch(rnn_block, vocab_size=len(data.vocab), lr=2)\n",
    "    trainer = d2l.Trainer(max_epochs=100, gradient_clip_val=1)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15c658d",
   "metadata": {},
   "source": [
    "## Concise Implementation\n",
    "\n",
    ":begin_tab:`pytorch, mxnet, tensorflow`\n",
    "Fortunately many of the logistical details required\n",
    "to implement multiple layers of an RNN \n",
    "are readily available in high-level APIs.\n",
    "Our concise implementation will use such built-in functionalities.\n",
    "The code generalizes the one we used previously in :numref:`sec_gru`,\n",
    "letting us specify the number of layers explicitly \n",
    "rather than picking the default of only one layer.\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`jax`\n",
    "Flax takes a minimalistic approach while implementing\n",
    "RNNs. Defining the number of layers in an RNN or combining it with dropout\n",
    "is not available out of the box.\n",
    "Our concise implementation will use all built-in functionalities and\n",
    "add `num_layers` and `dropout` features on top.\n",
    "The code generalizes the one we used previously in :numref:`sec_gru`,\n",
    "allowing specification of the number of layers explicitly\n",
    "rather than picking the default of a single layer.\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f2e230",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab mxnet\n",
    "class GRU(d2l.RNN):  #@save\n",
    "    \"\"\"The multilayer GRU model.\"\"\"\n",
    "    def __init__(self, num_hiddens, num_layers, dropout=0):\n",
    "        d2l.Module.__init__(self)\n",
    "        self.save_hyperparameters()\n",
    "        self.rnn = rnn.GRU(num_hiddens, num_layers, dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d215b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab pytorch\n",
    "class GRU(d2l.RNN):  #@save\n",
    "    \"\"\"The multilayer GRU model.\"\"\"\n",
    "    def __init__(self, num_inputs, num_hiddens, num_layers, dropout=0):\n",
    "        d2l.Module.__init__(self)\n",
    "        self.save_hyperparameters()\n",
    "        self.rnn = nn.GRU(num_inputs, num_hiddens, num_layers,\n",
    "                          dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eda80a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab tensorflow\n",
    "class GRU(d2l.RNN):  #@save\n",
    "    \"\"\"The multilayer GRU model.\"\"\"\n",
    "    def __init__(self, num_hiddens, num_layers, dropout=0):\n",
    "        d2l.Module.__init__(self)\n",
    "        self.save_hyperparameters()\n",
    "        gru_cells = [tf.keras.layers.GRUCell(num_hiddens, dropout=dropout)\n",
    "                     for _ in range(num_layers)]\n",
    "        self.rnn = tf.keras.layers.RNN(gru_cells, return_sequences=True,\n",
    "                                       return_state=True, time_major=True)\n",
    "\n",
    "    def forward(self, X, state=None):\n",
    "        outputs, *state = self.rnn(X, state)\n",
    "        return outputs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4faa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab jax\n",
    "class GRU(d2l.RNN):  #@save\n",
    "    \"\"\"The multilayer GRU model.\"\"\"\n",
    "    num_hiddens: int\n",
    "    num_layers: int\n",
    "    dropout: float = 0\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, X, state=None, training=False):\n",
    "        outputs = X\n",
    "        new_state = []\n",
    "        if state is None:\n",
    "            batch_size = X.shape[1]\n",
    "            state = [nn.GRUCell.initialize_carry(jax.random.PRNGKey(0),\n",
    "                    (batch_size,), self.num_hiddens)] * self.num_layers\n",
    "\n",
    "        GRU = nn.scan(nn.GRUCell, variable_broadcast=\"params\",\n",
    "                      in_axes=0, out_axes=0, split_rngs={\"params\": False})\n",
    "\n",
    "        # Introduce a dropout layer after every GRU layer except last\n",
    "        for i in range(self.num_layers - 1):\n",
    "            layer_i_state, X = GRU()(state[i], outputs)\n",
    "            new_state.append(layer_i_state)\n",
    "            X = nn.Dropout(self.dropout, deterministic=not training)(X)\n",
    "\n",
    "        # Final GRU layer without dropout\n",
    "        out_state, X = GRU()(state[-1], X)\n",
    "        new_state.append(out_state)\n",
    "        return X, jnp.array(new_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e1b1d1",
   "metadata": {},
   "source": [
    "The architectural decisions such as choosing hyperparameters \n",
    "are very similar to those of :numref:`sec_gru`.\n",
    "We pick the same number of inputs and outputs \n",
    "as we have distinct tokens, i.e., `vocab_size`.\n",
    "The number of hidden units is still 32.\n",
    "The only difference is that we now \n",
    "(**select a nontrivial number of hidden layers \n",
    "by specifying the value of `num_layers`.**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99731b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab mxnet\n",
    "gru = GRU(num_hiddens=32, num_layers=2)\n",
    "model = d2l.RNNLM(gru, vocab_size=len(data.vocab), lr=2)\n",
    "\n",
    "# Running takes > 1h (pending fix from MXNet)\n",
    "# trainer.fit(model, data)\n",
    "# model.predict('it has', 20, data.vocab, d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758422c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab pytorch, tensorflow, jax\n",
    "if tab.selected('tensorflow', 'jax'):\n",
    "    gru = GRU(num_hiddens=32, num_layers=2)\n",
    "if tab.selected('pytorch'):\n",
    "    gru = GRU(num_inputs=len(data.vocab), num_hiddens=32, num_layers=2)\n",
    "if tab.selected('pytorch', 'jax'):\n",
    "    model = d2l.RNNLM(gru, vocab_size=len(data.vocab), lr=2)\n",
    "if tab.selected('tensorflow'):\n",
    "    with d2l.try_gpu():\n",
    "        model = d2l.RNNLM(gru, vocab_size=len(data.vocab), lr=2)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2be55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab pytorch\n",
    "model.predict('it has', 20, data.vocab, d2l.try_gpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adb1ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab tensorflow\n",
    "model.predict('it has', 20, data.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d517d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab jax\n",
    "model.predict('it has', 20, data.vocab, trainer.state.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62711993",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In deep RNNs, the hidden state information is passed \n",
    "to the next time step of the current layer \n",
    "and the current time step of the next layer.\n",
    "There exist many different flavors of deep RNNs, such as LSTMs, GRUs, or vanilla RNNs. \n",
    "Conveniently, these models are all available \n",
    "as parts of the high-level APIs of deep learning frameworks.\n",
    "Initialization of models requires care. \n",
    "Overall, deep RNNs require considerable amount of work \n",
    "(such as learning rate and clipping) \n",
    "to ensure proper convergence.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Replace the GRU by an LSTM and compare the accuracy and training speed.\n",
    "1. Increase the training data to include multiple books. How low can you go on the perplexity scale?\n",
    "1. Would you want to combine sources of different authors when modeling text? Why is this a good idea? What could go wrong?\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/340)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/1058)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "[Discussions](https://discuss.d2l.ai/t/3862)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`jax`\n",
    "[Discussions](https://discuss.d2l.ai/t/18018)\n",
    ":end_tab:"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
