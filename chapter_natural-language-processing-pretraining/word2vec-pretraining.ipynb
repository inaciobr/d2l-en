{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "286e38e4",
   "metadata": {},
   "source": [
    "# Pretraining word2vec\n",
    ":label:`sec_word2vec_pretraining`\n",
    "\n",
    "\n",
    "We go on to implement the skip-gram\n",
    "model defined in\n",
    ":numref:`sec_word2vec`.\n",
    "Then\n",
    "we will pretrain word2vec using negative sampling\n",
    "on the PTB dataset.\n",
    "First of all,\n",
    "let's obtain the data iterator\n",
    "and the vocabulary for this dataset\n",
    "by calling the `d2l.load_data_ptb`\n",
    "function, which was described in :numref:`sec_word2vec_data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13209d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "from d2l import mxnet as d2l\n",
    "import math\n",
    "from mxnet import autograd, gluon, np, npx\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()\n",
    "\n",
    "batch_size, max_window_size, num_noise_words = 512, 5, 5\n",
    "data_iter, vocab = d2l.load_data_ptb(batch_size, max_window_size,\n",
    "                                     num_noise_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ad8b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "from d2l import torch as d2l\n",
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "batch_size, max_window_size, num_noise_words = 512, 5, 5\n",
    "data_iter, vocab = d2l.load_data_ptb(batch_size, max_window_size,\n",
    "                                     num_noise_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14494a82",
   "metadata": {},
   "source": [
    "## The Skip-Gram Model\n",
    "\n",
    "We implement the skip-gram model\n",
    "by using embedding layers and batch matrix multiplications.\n",
    "First, let's review\n",
    "how embedding layers work.\n",
    "\n",
    "\n",
    "### Embedding Layer\n",
    "\n",
    "As described in :numref:`sec_seq2seq`,\n",
    "an embedding layer\n",
    "maps a token's index to its feature vector.\n",
    "The weight of this layer\n",
    "is a matrix whose number of rows equals to\n",
    "the dictionary size (`input_dim`) and\n",
    "number of columns equals to\n",
    "the vector dimension for each token (`output_dim`).\n",
    "After a word embedding model is trained,\n",
    "this weight is what we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefa7109",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "embed = nn.Embedding(input_dim=20, output_dim=4)\n",
    "embed.initialize()\n",
    "embed.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3057f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "embed = nn.Embedding(num_embeddings=20, embedding_dim=4)\n",
    "print(f'Parameter embedding_weight ({embed.weight.shape}, '\n",
    "      f'dtype={embed.weight.dtype})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a272351",
   "metadata": {},
   "source": [
    "The input of an embedding layer is the\n",
    "index of a token (word).\n",
    "For any token index $i$,\n",
    "its vector representation\n",
    "can be obtained from\n",
    "the $i^\\textrm{th}$ row of the weight matrix\n",
    "in the embedding layer.\n",
    "Since the vector dimension (`output_dim`)\n",
    "was set to 4,\n",
    "the embedding layer\n",
    "returns vectors with shape (2, 3, 4)\n",
    "for a minibatch of token indices with shape\n",
    "(2, 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc95754",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "x = d2l.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "embed(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85cd2d4",
   "metadata": {},
   "source": [
    "### Defining the Forward Propagation\n",
    "\n",
    "In the forward propagation,\n",
    "the input of the skip-gram model\n",
    "includes\n",
    "the center word indices `center`\n",
    "of shape (batch size, 1)\n",
    "and\n",
    "the concatenated context and noise word indices `contexts_and_negatives`\n",
    "of shape (batch size, `max_len`),\n",
    "where `max_len`\n",
    "is defined\n",
    "in :numref:`subsec_word2vec-minibatch-loading`.\n",
    "These two variables are first transformed from the\n",
    "token indices into vectors via the embedding layer,\n",
    "then their batch matrix multiplication\n",
    "(described in :numref:`subsec_batch_dot`)\n",
    "returns\n",
    "an output of shape (batch size, 1, `max_len`).\n",
    "Each element in the output is the dot product of\n",
    "a center word vector and a context or noise word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9225711",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "def skip_gram(center, contexts_and_negatives, embed_v, embed_u):\n",
    "    v = embed_v(center)\n",
    "    u = embed_u(contexts_and_negatives)\n",
    "    pred = npx.batch_dot(v, u.swapaxes(1, 2))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d903a785",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def skip_gram(center, contexts_and_negatives, embed_v, embed_u):\n",
    "    v = embed_v(center)\n",
    "    u = embed_u(contexts_and_negatives)\n",
    "    pred = torch.bmm(v, u.permute(0, 2, 1))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40e1c0f",
   "metadata": {},
   "source": [
    "Let's print the output shape of this `skip_gram` function for some example inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f433ee68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "skip_gram(np.ones((2, 1)), np.ones((2, 4)), embed, embed).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aac684",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "skip_gram(torch.ones((2, 1), dtype=torch.long),\n",
    "          torch.ones((2, 4), dtype=torch.long), embed, embed).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3eb9aed",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Before training the skip-gram model with negative sampling,\n",
    "let's first define its loss function.\n",
    "\n",
    "\n",
    "### Binary Cross-Entropy Loss\n",
    "\n",
    "According to the definition of the loss function\n",
    "for negative sampling in :numref:`subsec_negative-sampling`, \n",
    "we will use \n",
    "the binary cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51089757",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "loss = gluon.loss.SigmoidBCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f48b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "class SigmoidBCELoss(nn.Module):\n",
    "    # Binary cross-entropy loss with masking\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, inputs, target, mask=None):\n",
    "        out = nn.functional.binary_cross_entropy_with_logits(\n",
    "            inputs, target, weight=mask, reduction=\"none\")\n",
    "        return out.mean(dim=1)\n",
    "\n",
    "loss = SigmoidBCELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60c444e",
   "metadata": {},
   "source": [
    "Recall our descriptions\n",
    "of the mask variable\n",
    "and the label variable in\n",
    ":numref:`subsec_word2vec-minibatch-loading`.\n",
    "The following\n",
    "calculates the \n",
    "binary cross-entropy loss\n",
    "for the given variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd6939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "pred = d2l.tensor([[1.1, -2.2, 3.3, -4.4]] * 2)\n",
    "label = d2l.tensor([[1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0]])\n",
    "mask = d2l.tensor([[1, 1, 1, 1], [1, 1, 0, 0]])\n",
    "loss(pred, label, mask) * mask.shape[1] / mask.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7588342c",
   "metadata": {},
   "source": [
    "Below shows\n",
    "how the above results are calculated\n",
    "(in a less efficient way)\n",
    "using the\n",
    "sigmoid activation function\n",
    "in the binary cross-entropy loss.\n",
    "We can consider \n",
    "the two outputs as\n",
    "two normalized losses\n",
    "that are averaged over non-masked predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a303de2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "def sigmd(x):\n",
    "    return -math.log(1 / (1 + math.exp(-x)))\n",
    "\n",
    "print(f'{(sigmd(1.1) + sigmd(2.2) + sigmd(-3.3) + sigmd(4.4)) / 4:.4f}')\n",
    "print(f'{(sigmd(-1.1) + sigmd(-2.2)) / 2:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2d2b53",
   "metadata": {},
   "source": [
    "### Initializing Model Parameters\n",
    "\n",
    "We define two embedding layers\n",
    "for all the words in the vocabulary\n",
    "when they are used as center words\n",
    "and context words, respectively.\n",
    "The word vector dimension\n",
    "`embed_size` is set to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9f94ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "embed_size = 100\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Embedding(input_dim=len(vocab), output_dim=embed_size),\n",
    "        nn.Embedding(input_dim=len(vocab), output_dim=embed_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b25cb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "embed_size = 100\n",
    "net = nn.Sequential(nn.Embedding(num_embeddings=len(vocab),\n",
    "                                 embedding_dim=embed_size),\n",
    "                    nn.Embedding(num_embeddings=len(vocab),\n",
    "                                 embedding_dim=embed_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f40323",
   "metadata": {},
   "source": [
    "### Defining the Training Loop\n",
    "\n",
    "The training loop is defined below. Because of the existence of padding, the calculation of the loss function is slightly different compared to the previous training functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e13237",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "def train(net, data_iter, lr, num_epochs, device=d2l.try_gpu()):\n",
    "    net.initialize(ctx=device, force_reinit=True)\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam',\n",
    "                            {'learning_rate': lr})\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[1, num_epochs])\n",
    "    # Sum of normalized losses, no. of normalized losses\n",
    "    metric = d2l.Accumulator(2)\n",
    "    for epoch in range(num_epochs):\n",
    "        timer, num_batches = d2l.Timer(), len(data_iter)\n",
    "        for i, batch in enumerate(data_iter):\n",
    "            center, context_negative, mask, label = [\n",
    "                data.as_in_ctx(device) for data in batch]\n",
    "            with autograd.record():\n",
    "                pred = skip_gram(center, context_negative, net[0], net[1])\n",
    "                l = (loss(pred.reshape(label.shape), label, mask) *\n",
    "                     mask.shape[1] / mask.sum(axis=1))\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            metric.add(l.sum(), l.size)\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,\n",
    "                             (metric[0] / metric[1],))\n",
    "    print(f'loss {metric[0] / metric[1]:.3f}, '\n",
    "          f'{metric[1] / timer.stop():.1f} tokens/sec on {str(device)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a60ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def train(net, data_iter, lr, num_epochs, device=d2l.try_gpu()):\n",
    "    def init_weights(module):\n",
    "        if type(module) == nn.Embedding:\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "    net.apply(init_weights)\n",
    "    net = net.to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[1, num_epochs])\n",
    "    # Sum of normalized losses, no. of normalized losses\n",
    "    metric = d2l.Accumulator(2)\n",
    "    for epoch in range(num_epochs):\n",
    "        timer, num_batches = d2l.Timer(), len(data_iter)\n",
    "        for i, batch in enumerate(data_iter):\n",
    "            optimizer.zero_grad()\n",
    "            center, context_negative, mask, label = [\n",
    "                data.to(device) for data in batch]\n",
    "\n",
    "            pred = skip_gram(center, context_negative, net[0], net[1])\n",
    "            l = (loss(pred.reshape(label.shape).float(), label.float(), mask)\n",
    "                     / mask.sum(axis=1) * mask.shape[1])\n",
    "            l.sum().backward()\n",
    "            optimizer.step()\n",
    "            metric.add(l.sum(), l.numel())\n",
    "            if (i + 1) % (num_batches // 5) == 0 or i == num_batches - 1:\n",
    "                animator.add(epoch + (i + 1) / num_batches,\n",
    "                             (metric[0] / metric[1],))\n",
    "    print(f'loss {metric[0] / metric[1]:.3f}, '\n",
    "          f'{metric[1] / timer.stop():.1f} tokens/sec on {str(device)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09604ce7",
   "metadata": {},
   "source": [
    "Now we can train a skip-gram model using negative sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6987b21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "lr, num_epochs = 0.002, 5\n",
    "train(net, data_iter, lr, num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fdc59e1",
   "metadata": {},
   "source": [
    "## Applying Word Embeddings\n",
    ":label:`subsec_apply-word-embed`\n",
    "\n",
    "\n",
    "After training the word2vec model,\n",
    "we can use the cosine similarity\n",
    "of word vectors from the trained model\n",
    "to \n",
    "find words from the dictionary\n",
    "that are most semantically similar\n",
    "to an input word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4557881b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "def get_similar_tokens(query_token, k, embed):\n",
    "    W = embed.weight.data()\n",
    "    x = W[vocab[query_token]]\n",
    "    # Compute the cosine similarity. Add 1e-9 for numerical stability\n",
    "    cos = np.dot(W, x) / np.sqrt(np.sum(W * W, axis=1) * np.sum(x * x) + 1e-9)\n",
    "    topk = npx.topk(cos, k=k+1, ret_typ='indices').asnumpy().astype('int32')\n",
    "    for i in topk[1:]:  # Remove the input words\n",
    "        print(f'cosine sim={float(cos[i]):.3f}: {vocab.to_tokens(i)}')\n",
    "\n",
    "get_similar_tokens('chip', 3, net[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6054241b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def get_similar_tokens(query_token, k, embed):\n",
    "    W = embed.weight.data\n",
    "    x = W[vocab[query_token]]\n",
    "    # Compute the cosine similarity. Add 1e-9 for numerical stability\n",
    "    cos = torch.mv(W, x) / torch.sqrt(torch.sum(W * W, dim=1) *\n",
    "                                      torch.sum(x * x) + 1e-9)\n",
    "    topk = torch.topk(cos, k=k+1)[1].cpu().numpy().astype('int32')\n",
    "    for i in topk[1:]:  # Remove the input words\n",
    "        print(f'cosine sim={float(cos[i]):.3f}: {vocab.to_tokens(i)}')\n",
    "\n",
    "get_similar_tokens('chip', 3, net[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558fdc13",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* We can train a skip-gram model with negative sampling using embedding layers and the binary cross-entropy loss.\n",
    "* Applications of word embeddings include finding semantically similar words for a given word based on the cosine similarity of word vectors.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Using the trained model, find semantically similar words for other input words. Can you improve the results by tuning hyperparameters?\n",
    "1. When a training corpus is huge, we often sample context words and noise words for the center words in the current minibatch *when updating model parameters*. In other words, the same center word may have different context words or noise words in different training epochs. What are the benefits of this method? Try to implement this training method.\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/384)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/1335)\n",
    ":end_tab:"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
