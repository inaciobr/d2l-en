{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df82f47d",
   "metadata": {},
   "source": [
    "# Sentiment Analysis: Using Convolutional Neural Networks\n",
    ":label:`sec_sentiment_cnn` \n",
    "\n",
    "\n",
    "In :numref:`chap_cnn`,\n",
    "we investigated mechanisms\n",
    "for processing\n",
    "two-dimensional image data\n",
    "with two-dimensional CNNs,\n",
    "which were applied to\n",
    "local features such as adjacent pixels.\n",
    "Though originally\n",
    "designed for computer vision,\n",
    "CNNs are also widely used\n",
    "for natural language processing.\n",
    "Simply put,\n",
    "just think of any text sequence\n",
    "as a one-dimensional image.\n",
    "In this way,\n",
    "one-dimensional CNNs\n",
    "can process local features\n",
    "such as $n$-grams in text.\n",
    "\n",
    "In this section,\n",
    "we will use the *textCNN* model\n",
    "to demonstrate\n",
    "how to design a CNN architecture\n",
    "for representing single text :cite:`Kim.2014`.\n",
    "Compared with\n",
    ":numref:`fig_nlp-map-sa-rnn`\n",
    "that uses an RNN architecture with GloVe pretraining\n",
    "for sentiment analysis,\n",
    "the only difference in :numref:`fig_nlp-map-sa-cnn`\n",
    "lies in\n",
    "the choice of the architecture.\n",
    "\n",
    "\n",
    "![This section feeds pretrained GloVe to a CNN-based architecture for sentiment analysis.](../img/nlp-map-sa-cnn.svg)\n",
    ":label:`fig_nlp-map-sa-cnn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3777360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import gluon, init, np, npx\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()\n",
    "\n",
    "batch_size = 64\n",
    "train_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2593fc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "batch_size = 64\n",
    "train_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1ed1f8",
   "metadata": {},
   "source": [
    "## One-Dimensional Convolutions\n",
    "\n",
    "Before introducing the model,\n",
    "let's see how a one-dimensional convolution works.\n",
    "Bear in mind that it is just a special case\n",
    "of a two-dimensional convolution\n",
    "based on the cross-correlation operation.\n",
    "\n",
    "![One-dimensional cross-correlation operation. The shaded portions are the first output element as well as the input and kernel tensor elements used for the output computation: $0\\times1+1\\times2=2$.](../img/conv1d.svg)\n",
    ":label:`fig_conv1d`\n",
    "\n",
    "As shown in :numref:`fig_conv1d`,\n",
    "in the one-dimensional case,\n",
    "the convolution window\n",
    "slides from left to right\n",
    "across the input tensor.\n",
    "During sliding,\n",
    "the input subtensor (e.g., $0$ and $1$ in :numref:`fig_conv1d`) contained in the convolution window\n",
    "at a certain position\n",
    "and the kernel tensor (e.g., $1$ and $2$ in :numref:`fig_conv1d`) are multiplied elementwise.\n",
    "The sum of these multiplications\n",
    "gives the single scalar value (e.g., $0\\times1+1\\times2=2$ in :numref:`fig_conv1d`)\n",
    "at the corresponding position of the output tensor.\n",
    "\n",
    "We implement one-dimensional cross-correlation in the following `corr1d` function.\n",
    "Given an input tensor `X`\n",
    "and a kernel tensor `K`,\n",
    "it returns the output tensor `Y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025af488",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "def corr1d(X, K):\n",
    "    w = K.shape[0]\n",
    "    Y = d2l.zeros((X.shape[0] - w + 1))\n",
    "    for i in range(Y.shape[0]):\n",
    "        Y[i] = (X[i: i + w] * K).sum()\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3a9d8e",
   "metadata": {},
   "source": [
    "We can construct the input tensor `X` and the kernel tensor `K` from :numref:`fig_conv1d` to validate the output of the above one-dimensional cross-correlation implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cea2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "X, K = d2l.tensor([0, 1, 2, 3, 4, 5, 6]), d2l.tensor([1, 2])\n",
    "corr1d(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88c3c98",
   "metadata": {},
   "source": [
    "For any\n",
    "one-dimensional input with multiple channels,\n",
    "the convolution kernel\n",
    "needs to have the same number of input channels.\n",
    "Then for each channel,\n",
    "perform a cross-correlation operation on the one-dimensional tensor of the input and the one-dimensional tensor of the convolution kernel,\n",
    "summing the results over all the channels\n",
    "to produce the one-dimensional output tensor.\n",
    ":numref:`fig_conv1d_channel` shows a one-dimensional cross-correlation operation with 3 input channels.\n",
    "\n",
    "![One-dimensional cross-correlation operation with 3 input channels. The shaded portions are the first output element as well as the input and kernel tensor elements used for the output computation: $0\\times1+1\\times2+1\\times3+2\\times4+2\\times(-1)+3\\times(-3)=2$.](../img/conv1d-channel.svg)\n",
    ":label:`fig_conv1d_channel`\n",
    "\n",
    "\n",
    "We can implement the one-dimensional cross-correlation operation for multiple input channels\n",
    "and validate the results in :numref:`fig_conv1d_channel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453ceace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "def corr1d_multi_in(X, K):\n",
    "    # First, iterate through the 0th dimension (channel dimension) of `X` and\n",
    "    # `K`. Then, add them together\n",
    "    return sum(corr1d(x, k) for x, k in zip(X, K))\n",
    "\n",
    "X = d2l.tensor([[0, 1, 2, 3, 4, 5, 6],\n",
    "              [1, 2, 3, 4, 5, 6, 7],\n",
    "              [2, 3, 4, 5, 6, 7, 8]])\n",
    "K = d2l.tensor([[1, 2], [3, 4], [-1, -3]])\n",
    "corr1d_multi_in(X, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a9c264",
   "metadata": {},
   "source": [
    "Note that\n",
    "multi-input-channel one-dimensional cross-correlations\n",
    "are equivalent\n",
    "to\n",
    "single-input-channel\n",
    "two-dimensional cross-correlations.\n",
    "To illustrate,\n",
    "an equivalent form of\n",
    "the multi-input-channel one-dimensional cross-correlation\n",
    "in :numref:`fig_conv1d_channel`\n",
    "is\n",
    "the\n",
    "single-input-channel\n",
    "two-dimensional cross-correlation\n",
    "in :numref:`fig_conv1d_2d`,\n",
    "where the height of the convolution kernel\n",
    "has to be the same as that of the input tensor.\n",
    "\n",
    "\n",
    "![Two-dimensional cross-correlation operation with a single input channel. The shaded portions are the first output element as well as the input and kernel tensor elements used for the output computation: $2\\times(-1)+3\\times(-3)+1\\times3+2\\times4+0\\times1+1\\times2=2$.](../img/conv1d-2d.svg)\n",
    ":label:`fig_conv1d_2d`\n",
    "\n",
    "Both the outputs in :numref:`fig_conv1d` and :numref:`fig_conv1d_channel` have only one channel.\n",
    "Same as two-dimensional convolutions with multiple output channels described in :numref:`subsec_multi-output-channels`,\n",
    "we can also specify multiple output channels\n",
    "for one-dimensional convolutions.\n",
    "\n",
    "## Max-Over-Time Pooling\n",
    "\n",
    "Similarly, we can use pooling\n",
    "to extract the highest value\n",
    "from sequence representations\n",
    "as the most important feature\n",
    "across time steps.\n",
    "The *max-over-time pooling* used in textCNN\n",
    "works like\n",
    "the one-dimensional global max-pooling\n",
    ":cite:`Collobert.Weston.Bottou.ea.2011`.\n",
    "For a multi-channel input\n",
    "where each channel stores values\n",
    "at different time steps,\n",
    "the output at each channel\n",
    "is the maximum value\n",
    "for that channel.\n",
    "Note that\n",
    "the max-over-time pooling\n",
    "allows different numbers of time steps\n",
    "at different channels.\n",
    "\n",
    "## The textCNN Model\n",
    "\n",
    "Using the one-dimensional convolution\n",
    "and max-over-time pooling,\n",
    "the textCNN model\n",
    "takes individual pretrained token representations\n",
    "as input,\n",
    "then obtains and transforms sequence representations\n",
    "for the downstream application.\n",
    "\n",
    "For a single text sequence\n",
    "with $n$ tokens represented by\n",
    "$d$-dimensional vectors,\n",
    "the width, height, and number of channels\n",
    "of the input tensor\n",
    "are $n$, $1$, and $d$, respectively.\n",
    "The textCNN model transforms the input\n",
    "into the output as follows:\n",
    "\n",
    "1. Define multiple one-dimensional convolution kernels and perform convolution operations separately on the inputs. Convolution kernels with different widths may capture local features among different numbers of adjacent tokens.\n",
    "1. Perform max-over-time pooling on all the output channels, and then concatenate all the scalar pooling outputs as a vector.\n",
    "1. Transform the concatenated vector into the output categories using the fully connected layer. Dropout can be used for reducing overfitting.\n",
    "\n",
    "![The model architecture of textCNN.](../img/textcnn.svg)\n",
    ":label:`fig_conv1d_textcnn`\n",
    "\n",
    ":numref:`fig_conv1d_textcnn`\n",
    "illustrates the model architecture of textCNN\n",
    "with a concrete example.\n",
    "The input is a sentence with 11 tokens,\n",
    "where\n",
    "each token is represented by a 6-dimensional vectors.\n",
    "So we have a 6-channel input with width 11.\n",
    "Define\n",
    "two one-dimensional convolution kernels\n",
    "of widths 2 and 4,\n",
    "with 4 and 5 output channels, respectively.\n",
    "They produce\n",
    "4 output channels with width $11-2+1=10$\n",
    "and 5 output channels with width $11-4+1=8$.\n",
    "Despite different widths of these 9 channels,\n",
    "the max-over-time pooling\n",
    "gives a concatenated 9-dimensional vector,\n",
    "which is finally transformed\n",
    "into a 2-dimensional output vector\n",
    "for binary sentiment predictions.\n",
    "\n",
    "\n",
    "\n",
    "### Defining the Model\n",
    "\n",
    "We implement the textCNN model in the following class.\n",
    "Compared with the bidirectional RNN model in\n",
    ":numref:`sec_sentiment_rnn`,\n",
    "besides\n",
    "replacing recurrent layers with convolutional layers,\n",
    "we also use two embedding layers:\n",
    "one with trainable weights and the other\n",
    "with fixed weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cb861a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "class TextCNN(nn.Block):\n",
    "    def __init__(self, vocab_size, embed_size, kernel_sizes, num_channels,\n",
    "                 **kwargs):\n",
    "        super(TextCNN, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # The embedding layer not to be trained\n",
    "        self.constant_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.decoder = nn.Dense(2)\n",
    "        # The max-over-time pooling layer has no parameters, so this instance\n",
    "        # can be shared\n",
    "        self.pool = nn.GlobalMaxPool1D()\n",
    "        # Create multiple one-dimensional convolutional layers\n",
    "        self.convs = nn.Sequential()\n",
    "        for c, k in zip(num_channels, kernel_sizes):\n",
    "            self.convs.add(nn.Conv1D(c, k, activation='relu'))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Concatenate two embedding layer outputs with shape (batch size, no.\n",
    "        # of tokens, token vector dimension) along vectors\n",
    "        embeddings = np.concatenate((\n",
    "            self.embedding(inputs), self.constant_embedding(inputs)), axis=2)\n",
    "        # Per the input format of one-dimensional convolutional layers,\n",
    "        # rearrange the tensor so that the second dimension stores channels\n",
    "        embeddings = embeddings.transpose(0, 2, 1)\n",
    "        # For each one-dimensional convolutional layer, after max-over-time\n",
    "        # pooling, a tensor of shape (batch size, no. of channels, 1) is\n",
    "        # obtained. Remove the last dimension and concatenate along channels\n",
    "        encoding = np.concatenate([\n",
    "            np.squeeze(self.pool(conv(embeddings)), axis=-1)\n",
    "            for conv in self.convs], axis=1)\n",
    "        outputs = self.decoder(self.dropout(encoding))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31eefd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, kernel_sizes, num_channels,\n",
    "                 **kwargs):\n",
    "        super(TextCNN, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # The embedding layer not to be trained\n",
    "        self.constant_embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.decoder = nn.Linear(sum(num_channels), 2)\n",
    "        # The max-over-time pooling layer has no parameters, so this instance\n",
    "        # can be shared\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.relu = nn.ReLU()\n",
    "        # Create multiple one-dimensional convolutional layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        for c, k in zip(num_channels, kernel_sizes):\n",
    "            self.convs.append(nn.Conv1d(2 * embed_size, c, k))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Concatenate two embedding layer outputs with shape (batch size, no.\n",
    "        # of tokens, token vector dimension) along vectors\n",
    "        embeddings = torch.cat((\n",
    "            self.embedding(inputs), self.constant_embedding(inputs)), dim=2)\n",
    "        # Per the input format of one-dimensional convolutional layers,\n",
    "        # rearrange the tensor so that the second dimension stores channels\n",
    "        embeddings = embeddings.permute(0, 2, 1)\n",
    "        # For each one-dimensional convolutional layer, after max-over-time\n",
    "        # pooling, a tensor of shape (batch size, no. of channels, 1) is\n",
    "        # obtained. Remove the last dimension and concatenate along channels\n",
    "        encoding = torch.cat([\n",
    "            torch.squeeze(self.relu(self.pool(conv(embeddings))), dim=-1)\n",
    "            for conv in self.convs], dim=1)\n",
    "        outputs = self.decoder(self.dropout(encoding))\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6042a5e2",
   "metadata": {},
   "source": [
    "Let's create a textCNN instance.\n",
    "It has 3 convolutional layers with kernel widths of 3, 4, and 5, all with 100 output channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17093a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "embed_size, kernel_sizes, nums_channels = 100, [3, 4, 5], [100, 100, 100]\n",
    "devices = d2l.try_all_gpus()\n",
    "net = TextCNN(len(vocab), embed_size, kernel_sizes, nums_channels)\n",
    "net.initialize(init.Xavier(), ctx=devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f57de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "embed_size, kernel_sizes, nums_channels = 100, [3, 4, 5], [100, 100, 100]\n",
    "devices = d2l.try_all_gpus()\n",
    "net = TextCNN(len(vocab), embed_size, kernel_sizes, nums_channels)\n",
    "\n",
    "def init_weights(module):\n",
    "    if type(module) in (nn.Linear, nn.Conv1d):\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "\n",
    "net.apply(init_weights);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f020743c",
   "metadata": {},
   "source": [
    "### Loading Pretrained Word Vectors\n",
    "\n",
    "Same as :numref:`sec_sentiment_rnn`,\n",
    "we load pretrained 100-dimensional GloVe embeddings\n",
    "as the initialized token representations.\n",
    "These token representations (embedding weights)\n",
    "will be trained in `embedding`\n",
    "and fixed in `constant_embedding`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ac6339",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "glove_embedding = d2l.TokenEmbedding('glove.6b.100d')\n",
    "embeds = glove_embedding[vocab.idx_to_token]\n",
    "net.embedding.weight.set_data(embeds)\n",
    "net.constant_embedding.weight.set_data(embeds)\n",
    "net.constant_embedding.collect_params().setattr('grad_req', 'null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6051649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "glove_embedding = d2l.TokenEmbedding('glove.6b.100d')\n",
    "embeds = glove_embedding[vocab.idx_to_token]\n",
    "net.embedding.weight.data.copy_(embeds)\n",
    "net.constant_embedding.weight.data.copy_(embeds)\n",
    "net.constant_embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4989149",
   "metadata": {},
   "source": [
    "### Training and Evaluating the Model\n",
    "\n",
    "Now we can train the textCNN model for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd9159a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "lr, num_epochs = 0.001, 5\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14221d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "lr, num_epochs = 0.001, 5\n",
    "trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749fba08",
   "metadata": {},
   "source": [
    "Below we use the trained model to predict the sentiment for two simple sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d582c442",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "d2l.predict_sentiment(net, vocab, 'this movie is so great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f8aced",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "d2l.predict_sentiment(net, vocab, 'this movie is so bad')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a82900d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* One-dimensional CNNs can process local features such as $n$-grams in text.\n",
    "* Multi-input-channel one-dimensional cross-correlations are equivalent to single-input-channel two-dimensional cross-correlations.\n",
    "* The max-over-time pooling allows different numbers of time steps at different channels.\n",
    "* The textCNN model transforms individual token representations into downstream application outputs using one-dimensional convolutional layers and max-over-time pooling layers.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Tune hyperparameters and compare the two architectures for sentiment analysis in :numref:`sec_sentiment_rnn` and in this section, such as in classification accuracy and computational efficiency.\n",
    "1. Can you further improve the classification accuracy of the model by using the methods introduced in the exercises of :numref:`sec_sentiment_rnn`?\n",
    "1. Add positional encoding in the input representations. Does it improve the classification accuracy?\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/393)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/1425)\n",
    ":end_tab:"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
