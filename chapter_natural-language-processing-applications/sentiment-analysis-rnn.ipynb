{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "910be0d3",
   "metadata": {},
   "source": [
    "# Sentiment Analysis: Using Recurrent Neural Networks\n",
    ":label:`sec_sentiment_rnn` \n",
    "\n",
    "\n",
    "Like word similarity and analogy tasks,\n",
    "we can also apply pretrained word vectors\n",
    "to sentiment analysis.\n",
    "Since the IMDb review dataset\n",
    "in :numref:`sec_sentiment`\n",
    "is not very big,\n",
    "using text representations\n",
    "that were pretrained\n",
    "on large-scale corpora\n",
    "may reduce overfitting of the model.\n",
    "As a specific example\n",
    "illustrated in :numref:`fig_nlp-map-sa-rnn`,\n",
    "we will represent each token\n",
    "using the pretrained GloVe model,\n",
    "and feed these token representations\n",
    "into a multilayer bidirectional RNN\n",
    "to obtain the text sequence representation,\n",
    "which will\n",
    "be transformed into \n",
    "sentiment analysis outputs :cite:`Maas.Daly.Pham.ea.2011`.\n",
    "For the same downstream application,\n",
    "we will consider a different architectural\n",
    "choice later.\n",
    "\n",
    "![This section feeds pretrained GloVe to an RNN-based architecture for sentiment analysis.](../img/nlp-map-sa-rnn.svg)\n",
    ":label:`fig_nlp-map-sa-rnn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c699f43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import gluon, init, np, npx\n",
    "from mxnet.gluon import nn, rnn\n",
    "npx.set_np()\n",
    "\n",
    "batch_size = 64\n",
    "train_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95846dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "batch_size = 64\n",
    "train_iter, test_iter, vocab = d2l.load_data_imdb(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2ed447",
   "metadata": {},
   "source": [
    "## Representing Single Text with RNNs\n",
    "\n",
    "In text classifications tasks,\n",
    "such as sentiment analysis,\n",
    "a varying-length text sequence \n",
    "will be transformed into fixed-length categories.\n",
    "In the following `BiRNN` class,\n",
    "while each token of a text sequence\n",
    "gets its individual\n",
    "pretrained GloVe\n",
    "representation via the embedding layer\n",
    "(`self.embedding`),\n",
    "the entire sequence\n",
    "is encoded by a bidirectional RNN (`self.encoder`).\n",
    "More concretely,\n",
    "the hidden states (at the last layer)\n",
    "of the bidirectional LSTM\n",
    "at both the initial and final time steps\n",
    "are concatenated \n",
    "as the representation of the text sequence.\n",
    "This single text representation\n",
    "is then transformed into output categories\n",
    "by a fully connected layer (`self.decoder`)\n",
    "with two outputs (\"positive\" and \"negative\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa653766",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "class BiRNN(nn.Block):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens,\n",
    "                 num_layers, **kwargs):\n",
    "        super(BiRNN, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # Set `bidirectional` to True to get a bidirectional RNN\n",
    "        self.encoder = rnn.LSTM(num_hiddens, num_layers=num_layers,\n",
    "                                bidirectional=True, input_size=embed_size)\n",
    "        self.decoder = nn.Dense(2)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # The shape of `inputs` is (batch size, no. of time steps). Because\n",
    "        # LSTM requires its input's first dimension to be the temporal\n",
    "        # dimension, the input is transposed before obtaining token\n",
    "        # representations. The output shape is (no. of time steps, batch size,\n",
    "        # word vector dimension)\n",
    "        embeddings = self.embedding(inputs.T)\n",
    "        # Returns hidden states of the last hidden layer at different time\n",
    "        # steps. The shape of `outputs` is (no. of time steps, batch size,\n",
    "        # 2 * no. of hidden units)\n",
    "        outputs = self.encoder(embeddings)\n",
    "        # Concatenate the hidden states at the initial and final time steps as\n",
    "        # the input of the fully connected layer. Its shape is (batch size,\n",
    "        # 4 * no. of hidden units)\n",
    "        encoding = np.concatenate((outputs[0], outputs[-1]), axis=1)\n",
    "        outs = self.decoder(encoding)\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4147821",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "class BiRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_hiddens,\n",
    "                 num_layers, **kwargs):\n",
    "        super(BiRNN, self).__init__(**kwargs)\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # Set `bidirectional` to True to get a bidirectional RNN\n",
    "        self.encoder = nn.LSTM(embed_size, num_hiddens, num_layers=num_layers,\n",
    "                                bidirectional=True)\n",
    "        self.decoder = nn.Linear(4 * num_hiddens, 2)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # The shape of `inputs` is (batch size, no. of time steps). Because\n",
    "        # LSTM requires its input's first dimension to be the temporal\n",
    "        # dimension, the input is transposed before obtaining token\n",
    "        # representations. The output shape is (no. of time steps, batch size,\n",
    "        # word vector dimension)\n",
    "        embeddings = self.embedding(inputs.T)\n",
    "        self.encoder.flatten_parameters()\n",
    "        # Returns hidden states of the last hidden layer at different time\n",
    "        # steps. The shape of `outputs` is (no. of time steps, batch size,\n",
    "        # 2 * no. of hidden units)\n",
    "        outputs, _ = self.encoder(embeddings)\n",
    "        # Concatenate the hidden states at the initial and final time steps as\n",
    "        # the input of the fully connected layer. Its shape is (batch size,\n",
    "        # 4 * no. of hidden units)\n",
    "        encoding = torch.cat((outputs[0], outputs[-1]), dim=1) \n",
    "        outs = self.decoder(encoding)\n",
    "        return outs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce10c44",
   "metadata": {},
   "source": [
    "Let's construct a bidirectional RNN with two hidden layers to represent single text for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03888e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "embed_size, num_hiddens, num_layers, devices = 100, 100, 2, d2l.try_all_gpus()\n",
    "net = BiRNN(len(vocab), embed_size, num_hiddens, num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff51de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "net.initialize(init.Xavier(), ctx=devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b88c52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def init_weights(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(module.weight)\n",
    "    if type(module) == nn.LSTM:\n",
    "        for param in module._flat_weights_names:\n",
    "            if \"weight\" in param:\n",
    "                nn.init.xavier_uniform_(module._parameters[param])\n",
    "net.apply(init_weights);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166f8a5d",
   "metadata": {},
   "source": [
    "## Loading Pretrained Word Vectors\n",
    "\n",
    "Below we load the pretrained 100-dimensional (needs to be consistent with `embed_size`) GloVe embeddings for tokens in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8bd182",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "glove_embedding = d2l.TokenEmbedding('glove.6b.100d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c0f3ed",
   "metadata": {},
   "source": [
    "Print the shape of the vectors\n",
    "for all the tokens in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cc0dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "embeds = glove_embedding[vocab.idx_to_token]\n",
    "embeds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035e48bf",
   "metadata": {},
   "source": [
    "We use these pretrained\n",
    "word vectors\n",
    "to represent tokens in the reviews\n",
    "and will not update\n",
    "these vectors during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d27591",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "net.embedding.weight.set_data(embeds)\n",
    "net.embedding.collect_params().setattr('grad_req', 'null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1ad1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "net.embedding.weight.data.copy_(embeds)\n",
    "net.embedding.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa9fc04",
   "metadata": {},
   "source": [
    "## Training and Evaluating the Model\n",
    "\n",
    "Now we can train the bidirectional RNN for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ee4513",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "lr, num_epochs = 0.01, 5\n",
    "trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': lr})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0349499e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "lr, num_epochs = 0.01, 5\n",
    "trainer = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "loss = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "d2l.train_ch13(net, train_iter, test_iter, loss, trainer, num_epochs, devices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b10cde",
   "metadata": {},
   "source": [
    "We define the following function to predict the sentiment of a text sequence using the trained model `net`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7a9be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "#@save\n",
    "def predict_sentiment(net, vocab, sequence):\n",
    "    \"\"\"Predict the sentiment of a text sequence.\"\"\"\n",
    "    sequence = np.array(vocab[sequence.split()], ctx=d2l.try_gpu())\n",
    "    label = np.argmax(net(sequence.reshape(1, -1)), axis=1)\n",
    "    return 'positive' if label == 1 else 'negative'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797bfc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def predict_sentiment(net, vocab, sequence):\n",
    "    \"\"\"Predict the sentiment of a text sequence.\"\"\"\n",
    "    sequence = torch.tensor(vocab[sequence.split()], device=d2l.try_gpu())\n",
    "    label = torch.argmax(net(sequence.reshape(1, -1)), dim=1)\n",
    "    return 'positive' if label == 1 else 'negative'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbefc03b",
   "metadata": {},
   "source": [
    "Finally, let's use the trained model to predict the sentiment for two simple sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49225ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "predict_sentiment(net, vocab, 'this movie is so great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c9e8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "predict_sentiment(net, vocab, 'this movie is so bad')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8cb73f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* Pretrained word vectors can represent individual tokens in a text sequence.\n",
    "* Bidirectional RNNs can represent a text sequence, such as via the concatenation of its hidden states at the initial and final time steps. This single text representation can be transformed into categories using a fully connected layer.\n",
    "\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Increase the number of epochs. Can you improve the training and testing accuracies? How about tuning other hyperparameters?\n",
    "1. Use larger pretrained word vectors, such as 300-dimensional GloVe embeddings. Does it improve classification accuracy?\n",
    "1. Can we improve the classification accuracy by using the spaCy tokenization? You need to install spaCy (`pip install spacy`) and install the English package (`python -m spacy download en`). In the code, first, import spaCy (`import spacy`). Then, load the spaCy English package (`spacy_en = spacy.load('en')`). Finally, define the function `def tokenizer(text): return [tok.text for tok in spacy_en.tokenizer(text)]` and replace the original `tokenizer` function. Note the different forms of phrase tokens in GloVe and spaCy. For example, the phrase token \"new york\" takes the form of \"new-york\" in GloVe and the form of \"new york\" after the spaCy tokenization.\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/392)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/1424)\n",
    ":end_tab:"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
