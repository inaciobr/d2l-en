{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9976a132",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext d2lbook.tab\n",
    "tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3899392",
   "metadata": {},
   "source": [
    "# Weight Decay\n",
    ":label:`sec_weight_decay`\n",
    "\n",
    "Now that we have characterized the problem of overfitting,\n",
    "we can introduce our first *regularization* technique.\n",
    "Recall that we can always mitigate overfitting\n",
    "by collecting more training data.\n",
    "However, that can be costly, time consuming,\n",
    "or entirely out of our control,\n",
    "making it impossible in the short run.\n",
    "For now, we can assume that we already have\n",
    "as much high-quality data as our resources permit\n",
    "and focus the tools at our disposal\n",
    "when the dataset is taken as a given.\n",
    "\n",
    "Recall that in our polynomial regression example\n",
    "(:numref:`subsec_polynomial-curve-fitting`)\n",
    "we could limit our model's capacity\n",
    "by tweaking the degree\n",
    "of the fitted polynomial.\n",
    "Indeed, limiting the number of features\n",
    "is a popular technique for mitigating overfitting.\n",
    "However, simply tossing aside features\n",
    "can be too blunt an instrument.\n",
    "Sticking with the polynomial regression\n",
    "example, consider what might happen\n",
    "with high-dimensional input.\n",
    "The natural extensions of polynomials\n",
    "to multivariate data are called *monomials*,\n",
    "which are simply products of powers of variables.\n",
    "The degree of a monomial is the sum of the powers.\n",
    "For example, $x_1^2 x_2$, and $x_3 x_5^2$\n",
    "are both monomials of degree 3.\n",
    "\n",
    "Note that the number of terms with degree $d$\n",
    "blows up rapidly as $d$ grows larger.\n",
    "Given $k$ variables, the number of monomials\n",
    "of degree $d$ is ${k - 1 + d} \\choose {k - 1}$.\n",
    "Even small changes in degree, say from $2$ to $3$,\n",
    "dramatically increase the complexity of our model.\n",
    "Thus we often need a more fine-grained tool\n",
    "for adjusting function complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ed0c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab mxnet\n",
    "%matplotlib inline\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import autograd, gluon, init, np, npx\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aaa7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab pytorch\n",
    "%matplotlib inline\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6875c38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab tensorflow\n",
    "%matplotlib inline\n",
    "from d2l import tensorflow as d2l\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7f6e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab jax\n",
    "%matplotlib inline\n",
    "from d2l import jax as d2l\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef1003d",
   "metadata": {},
   "source": [
    "## Norms and Weight Decay\n",
    "\n",
    "(**Rather than directly manipulating the number of parameters,\n",
    "*weight decay*, operates by restricting the values \n",
    "that the parameters can take.**)\n",
    "More commonly called $\\ell_2$ regularization\n",
    "outside of deep learning circles\n",
    "when optimized by minibatch stochastic gradient descent,\n",
    "weight decay might be the most widely used technique\n",
    "for regularizing parametric machine learning models.\n",
    "The technique is motivated by the basic intuition\n",
    "that among all functions $f$,\n",
    "the function $f = 0$\n",
    "(assigning the value $0$ to all inputs)\n",
    "is in some sense the *simplest*,\n",
    "and that we can measure the complexity\n",
    "of a function by the distance of its parameters from zero.\n",
    "But how precisely should we measure\n",
    "the distance between a function and zero?\n",
    "There is no single right answer.\n",
    "In fact, entire branches of mathematics,\n",
    "including parts of functional analysis\n",
    "and the theory of Banach spaces,\n",
    "are devoted to addressing such issues.\n",
    "\n",
    "One simple interpretation might be\n",
    "to measure the complexity of a linear function\n",
    "$f(\\mathbf{x}) = \\mathbf{w}^\\top \\mathbf{x}$\n",
    "by some norm of its weight vector, e.g., $\\| \\mathbf{w} \\|^2$.\n",
    "Recall that we introduced the $\\ell_2$ norm and $\\ell_1$ norm,\n",
    "which are special cases of the more general $\\ell_p$ norm,\n",
    "in :numref:`subsec_lin-algebra-norms`.\n",
    "The most common method for ensuring a small weight vector\n",
    "is to add its norm as a penalty term\n",
    "to the problem of minimizing the loss.\n",
    "Thus we replace our original objective,\n",
    "*minimizing the prediction loss on the training labels*,\n",
    "with new objective,\n",
    "*minimizing the sum of the prediction loss and the penalty term*.\n",
    "Now, if our weight vector grows too large,\n",
    "our learning algorithm might focus\n",
    "on minimizing the weight norm $\\| \\mathbf{w} \\|^2$\n",
    "rather than minimizing the training error.\n",
    "That is exactly what we want.\n",
    "To illustrate things in code,\n",
    "we revive our previous example\n",
    "from :numref:`sec_linear_regression` for linear regression.\n",
    "There, our loss was given by\n",
    "\n",
    "$$L(\\mathbf{w}, b) = \\frac{1}{n}\\sum_{i=1}^n \\frac{1}{2}\\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)^2.$$\n",
    "\n",
    "Recall that $\\mathbf{x}^{(i)}$ are the features,\n",
    "$y^{(i)}$ is the label for any data example $i$, and $(\\mathbf{w}, b)$\n",
    "are the weight and bias parameters, respectively.\n",
    "To penalize the size of the weight vector,\n",
    "we must somehow add $\\| \\mathbf{w} \\|^2$ to the loss function,\n",
    "but how should the model trade off the\n",
    "standard loss for this new additive penalty?\n",
    "In practice, we characterize this trade-off\n",
    "via the *regularization constant* $\\lambda$,\n",
    "a nonnegative hyperparameter\n",
    "that we fit using validation data:\n",
    "\n",
    "$$L(\\mathbf{w}, b) + \\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2.$$\n",
    "\n",
    "\n",
    "For $\\lambda = 0$, we recover our original loss function.\n",
    "For $\\lambda > 0$, we restrict the size of $\\| \\mathbf{w} \\|$.\n",
    "We divide by $2$ by convention:\n",
    "when we take the derivative of a quadratic function,\n",
    "the $2$ and $1/2$ cancel out, ensuring that the expression\n",
    "for the update looks nice and simple.\n",
    "The astute reader might wonder why we work with the squared\n",
    "norm and not the standard norm (i.e., the Euclidean distance).\n",
    "We do this for computational convenience.\n",
    "By squaring the $\\ell_2$ norm, we remove the square root,\n",
    "leaving the sum of squares of\n",
    "each component of the weight vector.\n",
    "This makes the derivative of the penalty easy to compute: \n",
    "the sum of derivatives equals the derivative of the sum.\n",
    "\n",
    "\n",
    "Moreover, you might ask why we work with the $\\ell_2$ norm\n",
    "in the first place and not, say, the $\\ell_1$ norm.\n",
    "In fact, other choices are valid and\n",
    "popular throughout statistics.\n",
    "While $\\ell_2$-regularized linear models constitute\n",
    "the classic *ridge regression* algorithm,\n",
    "$\\ell_1$-regularized linear regression\n",
    "is a similarly fundamental method in statistics, \n",
    "popularly known as *lasso regression*.\n",
    "One reason to work with the $\\ell_2$ norm\n",
    "is that it places an outsize penalty\n",
    "on large components of the weight vector.\n",
    "This biases our learning algorithm\n",
    "towards models that distribute weight evenly\n",
    "across a larger number of features.\n",
    "In practice, this might make them more robust\n",
    "to measurement error in a single variable.\n",
    "By contrast, $\\ell_1$ penalties lead to models\n",
    "that concentrate weights on a small set of features\n",
    "by clearing the other weights to zero.\n",
    "This gives us an effective method for *feature selection*,\n",
    "which may be desirable for other reasons.\n",
    "For example, if our model only relies on a few features,\n",
    "then we may not need to collect, store, or transmit data\n",
    "for the other (dropped) features. \n",
    "\n",
    "Using the same notation in :eqref:`eq_linreg_batch_update`,\n",
    "minibatch stochastic gradient descent updates\n",
    "for $\\ell_2$-regularized regression as follows:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mathbf{w} & \\leftarrow \\left(1- \\eta\\lambda \\right) \\mathbf{w} - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\mathbf{x}^{(i)} \\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right).\n",
    "\\end{aligned}$$\n",
    "\n",
    "As before, we update $\\mathbf{w}$ based on the amount\n",
    "by which our estimate differs from the observation.\n",
    "However, we also shrink the size of $\\mathbf{w}$ towards zero.\n",
    "That is why the method is sometimes called \"weight decay\":\n",
    "given the penalty term alone,\n",
    "our optimization algorithm *decays*\n",
    "the weight at each step of training.\n",
    "In contrast to feature selection,\n",
    "weight decay offers us a mechanism for continuously adjusting the complexity of a function.\n",
    "Smaller values of $\\lambda$ correspond\n",
    "to less constrained $\\mathbf{w}$,\n",
    "whereas larger values of $\\lambda$\n",
    "constrain $\\mathbf{w}$ more considerably.\n",
    "Whether we include a corresponding bias penalty $b^2$ \n",
    "can vary across implementations, \n",
    "and may vary across layers of a neural network.\n",
    "Often, we do not regularize the bias term.\n",
    "Besides,\n",
    "although $\\ell_2$ regularization may not be equivalent to weight decay for other optimization algorithms,\n",
    "the idea of regularization through\n",
    "shrinking the size of weights\n",
    "still holds true.\n",
    "\n",
    "## High-Dimensional Linear Regression\n",
    "\n",
    "We can illustrate the benefits of weight decay \n",
    "through a simple synthetic example.\n",
    "\n",
    "First, we [**generate some data as before**]:\n",
    "\n",
    "(**$$y = 0.05 + \\sum_{i = 1}^d 0.01 x_i + \\epsilon \\textrm{ where }\n",
    "\\epsilon \\sim \\mathcal{N}(0, 0.01^2).$$**)\n",
    "\n",
    "In this synthetic dataset, our label is given \n",
    "by an underlying linear function of our inputs,\n",
    "corrupted by Gaussian noise \n",
    "with zero mean and standard deviation 0.01.\n",
    "For illustrative purposes, \n",
    "we can make the effects of overfitting pronounced,\n",
    "by increasing the dimensionality of our problem to $d = 200$\n",
    "and working with a small training set with only 20 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89995ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab all\n",
    "class Data(d2l.DataModule):\n",
    "    def __init__(self, num_train, num_val, num_inputs, batch_size):\n",
    "        self.save_hyperparameters()                \n",
    "        n = num_train + num_val \n",
    "        if tab.selected('mxnet') or tab.selected('pytorch'):\n",
    "            self.X = d2l.randn(n, num_inputs)\n",
    "            noise = d2l.randn(n, 1) * 0.01\n",
    "        if tab.selected('tensorflow'):\n",
    "            self.X = d2l.normal((n, num_inputs))\n",
    "            noise = d2l.normal((n, 1)) * 0.01\n",
    "        if tab.selected('jax'):\n",
    "            self.X = jax.random.normal(jax.random.PRNGKey(0), (n, num_inputs))\n",
    "            noise = jax.random.normal(jax.random.PRNGKey(0), (n, 1)) * 0.01\n",
    "        w, b = d2l.ones((num_inputs, 1)) * 0.01, 0.05\n",
    "        self.y = d2l.matmul(self.X, w) + b + noise\n",
    "\n",
    "    def get_dataloader(self, train):\n",
    "        i = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
    "        return self.get_tensorloader([self.X, self.y], train, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd411216",
   "metadata": {},
   "source": [
    "## Implementation from Scratch\n",
    "\n",
    "Now, let's try implementing weight decay from scratch.\n",
    "Since minibatch stochastic gradient descent\n",
    "is our optimizer,\n",
    "we just need to add the squared $\\ell_2$ penalty\n",
    "to the original loss function.\n",
    "\n",
    "### (**Defining $\\ell_2$ Norm Penalty**)\n",
    "\n",
    "Perhaps the most convenient way of implementing this penalty\n",
    "is to square all terms in place and sum them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fcb7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab all\n",
    "def l2_penalty(w):\n",
    "    return d2l.reduce_sum(w**2) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33870ed6",
   "metadata": {},
   "source": [
    "### Defining the Model\n",
    "\n",
    "In the final model,\n",
    "the linear regression and the squared loss have not changed since :numref:`sec_linear_scratch`,\n",
    "so we will just define a subclass of `d2l.LinearRegressionScratch`. The only change here is that our loss now includes the penalty term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bf2729",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab pytorch, mxnet, tensorflow\n",
    "class WeightDecayScratch(d2l.LinearRegressionScratch):\n",
    "    def __init__(self, num_inputs, lambd, lr, sigma=0.01):\n",
    "        super().__init__(num_inputs, lr, sigma)\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "    def loss(self, y_hat, y):\n",
    "        return (super().loss(y_hat, y) +\n",
    "                self.lambd * l2_penalty(self.w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67771244",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab jax\n",
    "class WeightDecayScratch(d2l.LinearRegressionScratch):\n",
    "    lambd: int = 0\n",
    "        \n",
    "    def loss(self, params, X, y, state):\n",
    "        return (super().loss(params, X, y, state) +\n",
    "                self.lambd * l2_penalty(params['w']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9aead41",
   "metadata": {},
   "source": [
    "The following code fits our model on the training set with 20 examples and evaluates it on the validation set with 100 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5f9f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab all\n",
    "data = Data(num_train=20, num_val=100, num_inputs=200, batch_size=5)\n",
    "trainer = d2l.Trainer(max_epochs=10)\n",
    "\n",
    "def train_scratch(lambd):    \n",
    "    model = WeightDecayScratch(num_inputs=200, lambd=lambd, lr=0.01)\n",
    "    model.board.yscale='log'\n",
    "    trainer.fit(model, data)\n",
    "    if tab.selected('pytorch', 'mxnet', 'tensorflow'):\n",
    "        print('L2 norm of w:', float(l2_penalty(model.w)))\n",
    "    if tab.selected('jax'):\n",
    "        print('L2 norm of w:',\n",
    "              float(l2_penalty(trainer.state.params['w'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910c3dac",
   "metadata": {},
   "source": [
    "### [**Training without Regularization**]\n",
    "\n",
    "We now run this code with `lambd = 0`,\n",
    "disabling weight decay.\n",
    "Note that we overfit badly,\n",
    "decreasing the training error but not the\n",
    "validation error---a textbook case of overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d4bd978",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab all\n",
    "train_scratch(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5cc77f",
   "metadata": {},
   "source": [
    "### [**Using Weight Decay**]\n",
    "\n",
    "Below, we run with substantial weight decay.\n",
    "Note that the training error increases\n",
    "but the validation error decreases.\n",
    "This is precisely the effect\n",
    "we expect from regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68453f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab all\n",
    "train_scratch(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53d6cda",
   "metadata": {},
   "source": [
    "## [**Concise Implementation**]\n",
    "\n",
    "Because weight decay is ubiquitous\n",
    "in neural network optimization,\n",
    "the deep learning framework makes it especially convenient,\n",
    "integrating weight decay into the optimization algorithm itself\n",
    "for easy use in combination with any loss function.\n",
    "Moreover, this integration serves a computational benefit,\n",
    "allowing implementation tricks to add weight decay to the algorithm,\n",
    "without any additional computational overhead.\n",
    "Since the weight decay portion of the update\n",
    "depends only on the current value of each parameter,\n",
    "the optimizer must touch each parameter once anyway.\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "Below, we specify\n",
    "the weight decay hyperparameter directly\n",
    "through `wd` when instantiating our `Trainer`.\n",
    "By default, Gluon decays both\n",
    "weights and biases simultaneously.\n",
    "Note that the hyperparameter `wd`\n",
    "will be multiplied by `wd_mult`\n",
    "when updating model parameters.\n",
    "Thus, if we set `wd_mult` to zero,\n",
    "the bias parameter $b$ will not decay.\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "Below, we specify\n",
    "the weight decay hyperparameter directly\n",
    "through `weight_decay` when instantiating our optimizer.\n",
    "By default, PyTorch decays both\n",
    "weights and biases simultaneously, but\n",
    "we can configure the optimizer to handle different parameters\n",
    "according to different policies.\n",
    "Here, we only set `weight_decay` for\n",
    "the weights (the `net.weight` parameters), hence the \n",
    "bias (the `net.bias` parameter) will not decay.\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "Below, we create an $\\ell_2$ regularizer with\n",
    "the weight decay hyperparameter `wd` and apply it to the layer's weights\n",
    "through the `kernel_regularizer` argument.\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7fcf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab mxnet\n",
    "class WeightDecay(d2l.LinearRegression):\n",
    "    def __init__(self, wd, lr):\n",
    "        super().__init__(lr)\n",
    "        self.save_hyperparameters()\n",
    "        self.wd = wd\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        self.collect_params('.*bias').setattr('wd_mult', 0)\n",
    "        return gluon.Trainer(self.collect_params(),\n",
    "                             'sgd', \n",
    "                             {'learning_rate': self.lr, 'wd': self.wd})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9890ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab pytorch\n",
    "class WeightDecay(d2l.LinearRegression):\n",
    "    def __init__(self, wd, lr):\n",
    "        super().__init__(lr)\n",
    "        self.save_hyperparameters()\n",
    "        self.wd = wd\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.SGD([\n",
    "            {'params': self.net.weight, 'weight_decay': self.wd},\n",
    "            {'params': self.net.bias}], lr=self.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705a358a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab tensorflow\n",
    "class WeightDecay(d2l.LinearRegression):\n",
    "    def __init__(self, wd, lr):\n",
    "        super().__init__(lr)\n",
    "        self.save_hyperparameters()\n",
    "        self.net = tf.keras.layers.Dense(\n",
    "            1, kernel_regularizer=tf.keras.regularizers.l2(wd),\n",
    "            kernel_initializer=tf.keras.initializers.RandomNormal(0, 0.01)\n",
    "        )\n",
    "        \n",
    "    def loss(self, y_hat, y):\n",
    "        return super().loss(y_hat, y) + self.net.losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23f7232",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab jax\n",
    "class WeightDecay(d2l.LinearRegression):\n",
    "    wd: int = 0\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # Weight Decay is not available directly within optax.sgd, but\n",
    "        # optax allows chaining several transformations together\n",
    "        return optax.chain(optax.additive_weight_decay(self.wd),\n",
    "                           optax.sgd(self.lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65da5eeb",
   "metadata": {},
   "source": [
    "[**The plot looks similar to that when\n",
    "we implemented weight decay from scratch**].\n",
    "However, this version runs faster\n",
    "and is easier to implement,\n",
    "benefits that will become more\n",
    "pronounced as you address larger problems\n",
    "and this work becomes more routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b885bad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab all\n",
    "model = WeightDecay(wd=3, lr=0.01)\n",
    "model.board.yscale='log'\n",
    "trainer.fit(model, data)\n",
    "\n",
    "if tab.selected('jax'):\n",
    "    print('L2 norm of w:', float(l2_penalty(model.get_w_b(trainer.state)[0])))\n",
    "if tab.selected('pytorch', 'mxnet', 'tensorflow'):\n",
    "    print('L2 norm of w:', float(l2_penalty(model.get_w_b()[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f788daeb",
   "metadata": {},
   "source": [
    "So far, we have touched upon one notion of\n",
    "what constitutes a simple linear function.\n",
    "However, even for simple nonlinear functions, the situation can be much more complex. To see this, the concept of [reproducing kernel Hilbert space (RKHS)](https://en.wikipedia.org/wiki/Reproducing_kernel_Hilbert_space)\n",
    "allows one to apply tools introduced\n",
    "for linear functions in a nonlinear context.\n",
    "Unfortunately, RKHS-based algorithms\n",
    "tend to scale poorly to large, high-dimensional data.\n",
    "In this book we will often adopt the common heuristic\n",
    "whereby weight decay is applied\n",
    "to all layers of a deep network.\n",
    "\n",
    "## Summary\n",
    "\n",
    "Regularization is a common method for dealing with overfitting. Classical regularization techniques add a penalty term to the loss function (when training) to reduce the complexity of the learned model.\n",
    "One particular choice for keeping the model simple is using an $\\ell_2$ penalty. This leads to weight decay in the update steps of the minibatch stochastic gradient descent algorithm.\n",
    "In practice, the weight decay functionality is provided in optimizers from deep learning frameworks.\n",
    "Different sets of parameters can have different update behaviors within the same training loop.\n",
    "\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Experiment with the value of $\\lambda$ in the estimation problem in this section. Plot training and validation accuracy as a function of $\\lambda$. What do you observe?\n",
    "1. Use a validation set to find the optimal value of $\\lambda$. Is it really the optimal value? Does this matter?\n",
    "1. What would the update equations look like if instead of $\\|\\mathbf{w}\\|^2$ we used $\\sum_i |w_i|$ as our penalty of choice ($\\ell_1$ regularization)?\n",
    "1. We know that $\\|\\mathbf{w}\\|^2 = \\mathbf{w}^\\top \\mathbf{w}$. Can you find a similar equation for matrices (see the Frobenius norm in :numref:`subsec_lin-algebra-norms`)?\n",
    "1. Review the relationship between training error and generalization error. In addition to weight decay, increased training, and the use of a model of suitable complexity, what other ways might help us deal with overfitting?\n",
    "1. In Bayesian statistics we use the product of prior and likelihood to arrive at a posterior via $P(w \\mid x) \\propto P(x \\mid w) P(w)$. How can you identify $P(w)$ with regularization?\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/98)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/99)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "[Discussions](https://discuss.d2l.ai/t/236)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`jax`\n",
    "[Discussions](https://discuss.d2l.ai/t/17979)\n",
    ":end_tab:"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
