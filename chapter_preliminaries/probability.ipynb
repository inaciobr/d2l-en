{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37059850",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext d2lbook.tab\n",
    "tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d19b608",
   "metadata": {},
   "source": [
    "# Probability and Statistics\n",
    ":label:`sec_prob`\n",
    "\n",
    "One way or another,\n",
    "machine learning is all about uncertainty.\n",
    "In supervised learning, we want to predict\n",
    "something unknown (the *target*)\n",
    "given something known (the *features*).\n",
    "Depending on our objective,\n",
    "we might attempt to predict\n",
    "the most likely value of the target.\n",
    "Or we might predict the value with the smallest\n",
    "expected distance from the target.\n",
    "And sometimes we wish not only\n",
    "to predict a specific value\n",
    "but to *quantify our uncertainty*.\n",
    "For example, given some features\n",
    "describing a patient,\n",
    "we might want to know *how likely* they are\n",
    "to suffer a heart attack in the next year.\n",
    "In unsupervised learning,\n",
    "we often care about uncertainty.\n",
    "To determine whether a set of measurements are anomalous,\n",
    "it helps to know how likely one is\n",
    "to observe values in a population of interest.\n",
    "Furthermore, in reinforcement learning,\n",
    "we wish to develop agents\n",
    "that act intelligently in various environments.\n",
    "This requires reasoning about\n",
    "how an environment might be expected to change\n",
    "and what rewards one might expect to encounter\n",
    "in response to each of the available actions.\n",
    "\n",
    "*Probability* is the mathematical field\n",
    "concerned with reasoning under uncertainty.\n",
    "Given a probabilistic model of some process,\n",
    "we can reason about the likelihood of various events.\n",
    "The use of probabilities to describe\n",
    "the frequencies of repeatable events\n",
    "(like coin tosses)\n",
    "is fairly uncontroversial.\n",
    "In fact, *frequentist* scholars adhere\n",
    "to an interpretation of probability\n",
    "that applies *only* to such repeatable events.\n",
    "By contrast *Bayesian* scholars\n",
    "use the language of probability more broadly\n",
    "to formalize reasoning under uncertainty.\n",
    "Bayesian probability is characterized\n",
    "by two unique features:\n",
    "(i) assigning degrees of belief\n",
    "to non-repeatable events,\n",
    "e.g., what is the *probability*\n",
    "that a dam will collapse?;\n",
    "and (ii) subjectivity. While Bayesian\n",
    "probability provides unambiguous rules\n",
    "for how one should update their beliefs\n",
    "in light of new evidence,\n",
    "it allows for different individuals\n",
    "to start off with different *prior* beliefs.\n",
    "*Statistics* helps us to reason backwards,\n",
    "starting off with collection and organization of data\n",
    "and backing out to what inferences\n",
    "we might draw about the process\n",
    "that generated the data.\n",
    "Whenever we analyze a dataset, hunting for patterns\n",
    "that we hope might characterize a broader population,\n",
    "we are employing statistical thinking.\n",
    "Many courses, majors, theses, careers, departments,\n",
    "companies, and institutions have been devoted\n",
    "to the study of probability and statistics.\n",
    "While this section only scratches the surface,\n",
    "we will provide the foundation\n",
    "that you need to begin building models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ce09b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab mxnet\n",
    "%matplotlib inline\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import np, npx\n",
    "from mxnet.numpy.random import multinomial\n",
    "import random\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94abeb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab pytorch\n",
    "%matplotlib inline\n",
    "from d2l import torch as d2l\n",
    "import random\n",
    "import torch\n",
    "from torch.distributions.multinomial import Multinomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc28236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab tensorflow\n",
    "%matplotlib inline\n",
    "from d2l import tensorflow as d2l\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow_probability import distributions as tfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a97c070",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab jax\n",
    "%matplotlib inline\n",
    "from d2l import jax as d2l\n",
    "import random\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28387dbc",
   "metadata": {},
   "source": [
    "## A Simple Example: Tossing Coins\n",
    "\n",
    "Imagine that we plan to toss a coin\n",
    "and want to quantify how likely\n",
    "we are to see heads (vs. tails).\n",
    "If the coin is *fair*,\n",
    "then both outcomes\n",
    "(heads and tails),\n",
    "are equally likely.\n",
    "Moreover if we plan to toss the coin $n$ times\n",
    "then the fraction of heads\n",
    "that we *expect* to see\n",
    "should exactly match\n",
    "the *expected* fraction of tails.\n",
    "One intuitive way to see this\n",
    "is by symmetry:\n",
    "for every possible outcome\n",
    "with $n_\\textrm{h}$ heads and $n_\\textrm{t} = (n - n_\\textrm{h})$ tails,\n",
    "there is an equally likely outcome\n",
    "with $n_\\textrm{t}$ heads and $n_\\textrm{h}$ tails.\n",
    "Note that this is only possible\n",
    "if on average we expect to see\n",
    "$1/2$ of tosses come up heads\n",
    "and $1/2$ come up tails.\n",
    "Of course, if you conduct this experiment\n",
    "many times with $n=1000000$ tosses each,\n",
    "you might never see a trial\n",
    "where $n_\\textrm{h} = n_\\textrm{t}$ exactly.\n",
    "\n",
    "\n",
    "Formally, the quantity $1/2$ is called a *probability*\n",
    "and here it captures the certainty with which\n",
    "any given toss will come up heads.\n",
    "Probabilities assign scores between $0$ and $1$\n",
    "to outcomes of interest, called *events*.\n",
    "Here the event of interest is $\\textrm{heads}$\n",
    "and we denote the corresponding probability $P(\\textrm{heads})$.\n",
    "A probability of $1$ indicates absolute certainty\n",
    "(imagine a trick coin where both sides were heads)\n",
    "and a probability of $0$ indicates impossibility\n",
    "(e.g., if both sides were tails).\n",
    "The frequencies $n_\\textrm{h}/n$ and $n_\\textrm{t}/n$ are not probabilities\n",
    "but rather *statistics*.\n",
    "Probabilities are *theoretical* quantities\n",
    "that underly the data generating process.\n",
    "Here, the probability $1/2$\n",
    "is a property of the coin itself.\n",
    "By contrast, statistics are *empirical* quantities\n",
    "that are computed as functions of the observed data.\n",
    "Our interests in probabilistic and statistical quantities\n",
    "are inextricably intertwined.\n",
    "We often design special statistics called *estimators*\n",
    "that, given a dataset, produce *estimates*\n",
    "of model parameters such as probabilities.\n",
    "Moreover, when those estimators satisfy\n",
    "a nice property called *consistency*,\n",
    "our estimates will converge\n",
    "to the corresponding probability.\n",
    "In turn, these inferred probabilities\n",
    "tell about the likely statistical properties\n",
    "of data from the same population\n",
    "that we might encounter in the future.\n",
    "\n",
    "Suppose that we stumbled upon a real coin\n",
    "for which we did not know\n",
    "the true $P(\\textrm{heads})$.\n",
    "To investigate this quantity\n",
    "with statistical methods,\n",
    "we need to (i) collect some data;\n",
    "and (ii) design an estimator.\n",
    "Data acquisition here is easy;\n",
    "we can toss the coin many times\n",
    "and record all the outcomes.\n",
    "Formally, drawing realizations\n",
    "from some underlying random process\n",
    "is called *sampling*.\n",
    "As you might have guessed,\n",
    "one natural estimator\n",
    "is the ratio of\n",
    "the number of observed *heads*\n",
    "to the total number of tosses.\n",
    "\n",
    "Now, suppose that the coin was in fact fair,\n",
    "i.e., $P(\\textrm{heads}) = 0.5$.\n",
    "To simulate tosses of a fair coin,\n",
    "we can invoke any random number generator.\n",
    "There are some easy ways to draw samples\n",
    "of an event with probability $0.5$.\n",
    "For example Python's `random.random`\n",
    "yields numbers in the interval $[0,1]$\n",
    "where the probability of lying\n",
    "in any sub-interval $[a, b] \\subset [0,1]$\n",
    "is equal to $b-a$.\n",
    "Thus we can get out `0` and `1` with probability `0.5` each\n",
    "by testing whether the returned float number is greater than `0.5`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8411434b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab all\n",
    "num_tosses = 100\n",
    "heads = sum([random.random() > 0.5 for _ in range(num_tosses)])\n",
    "tails = num_tosses - heads\n",
    "print(\"heads, tails: \", [heads, tails])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ba30c5",
   "metadata": {},
   "source": [
    "More generally, we can simulate multiple draws\n",
    "from any variable with a finite number\n",
    "of possible outcomes\n",
    "(like the toss of a coin or roll of a die)\n",
    "by calling the multinomial function,\n",
    "setting the first argument\n",
    "to the number of draws\n",
    "and the second as a list of probabilities\n",
    "associated with each of the possible outcomes.\n",
    "To simulate ten tosses of a fair coin,\n",
    "we assign probability vector `[0.5, 0.5]`,\n",
    "interpreting index 0 as heads\n",
    "and index 1 as tails.\n",
    "The function returns a vector\n",
    "with length equal to the number\n",
    "of possible outcomes (here, 2),\n",
    "where the first component tells us\n",
    "the number of occurrences of heads\n",
    "and the second component tells us\n",
    "the number of occurrences of tails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07642223",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab mxnet\n",
    "fair_probs = [0.5, 0.5]\n",
    "multinomial(100, fair_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c37a1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab pytorch\n",
    "fair_probs = torch.tensor([0.5, 0.5])\n",
    "Multinomial(100, fair_probs).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5e7d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab tensorflow\n",
    "fair_probs = tf.ones(2) / 2\n",
    "tfd.Multinomial(100, fair_probs).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8a8ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab jax\n",
    "fair_probs = [0.5, 0.5]\n",
    "# jax.random does not have multinomial distribution implemented\n",
    "np.random.multinomial(100, fair_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03653cf8",
   "metadata": {},
   "source": [
    "Each time you run this sampling process,\n",
    "you will receive a new random value\n",
    "that may differ from the previous outcome.\n",
    "Dividing by the number of tosses\n",
    "gives us the *frequency*\n",
    "of each outcome in our data.\n",
    "Note that these frequencies,\n",
    "just like the probabilities\n",
    "that they are intended\n",
    "to estimate, sum to $1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058ebf95",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab mxnet\n",
    "multinomial(100, fair_probs) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fda45b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab pytorch\n",
    "Multinomial(100, fair_probs).sample() / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89283d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab tensorflow\n",
    "tfd.Multinomial(100, fair_probs).sample() / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7fef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab jax\n",
    "np.random.multinomial(100, fair_probs) / 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9440f15c",
   "metadata": {},
   "source": [
    "Here, even though our simulated coin is fair\n",
    "(we ourselves set the probabilities `[0.5, 0.5]`),\n",
    "the counts of heads and tails may not be identical.\n",
    "That is because we only drew a relatively small number of samples.\n",
    "If we did not implement the simulation ourselves,\n",
    "and only saw the outcome,\n",
    "how would we know if the coin were slightly unfair\n",
    "or if the possible deviation from $1/2$ was\n",
    "just an artifact of the small sample size?\n",
    "Let's see what happens when we simulate 10,000 tosses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a84ac66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab mxnet\n",
    "counts = multinomial(10000, fair_probs).astype(np.float32)\n",
    "counts / 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511dbe8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab pytorch\n",
    "counts = Multinomial(10000, fair_probs).sample()\n",
    "counts / 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0bb843",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab tensorflow\n",
    "counts = tfd.Multinomial(10000, fair_probs).sample()\n",
    "counts / 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc22be46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab jax\n",
    "counts = np.random.multinomial(10000, fair_probs).astype(np.float32)\n",
    "counts / 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfc64df",
   "metadata": {},
   "source": [
    "In general, for averages of repeated events (like coin tosses),\n",
    "as the number of repetitions grows,\n",
    "our estimates are guaranteed to converge\n",
    "to the true underlying probabilities.\n",
    "The mathematical formulation of this phenomenon\n",
    "is called the *law of large numbers*\n",
    "and the *central limit theorem*\n",
    "tells us that in many situations,\n",
    "as the sample size $n$ grows,\n",
    "these errors should go down\n",
    "at a rate of $(1/\\sqrt{n})$.\n",
    "Let's get some more intuition by studying\n",
    "how our estimate evolves as we grow\n",
    "the number of tosses from 1 to 10,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c027d325",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab pytorch\n",
    "counts = Multinomial(1, fair_probs).sample((10000,))\n",
    "cum_counts = counts.cumsum(dim=0)\n",
    "estimates = cum_counts / cum_counts.sum(dim=1, keepdims=True)\n",
    "estimates = estimates.numpy()\n",
    "\n",
    "d2l.set_figsize((4.5, 3.5))\n",
    "d2l.plt.plot(estimates[:, 0], label=(\"P(coin=heads)\"))\n",
    "d2l.plt.plot(estimates[:, 1], label=(\"P(coin=tails)\"))\n",
    "d2l.plt.axhline(y=0.5, color='black', linestyle='dashed')\n",
    "d2l.plt.gca().set_xlabel('Samples')\n",
    "d2l.plt.gca().set_ylabel('Estimated probability')\n",
    "d2l.plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bf7e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab mxnet\n",
    "counts = multinomial(1, fair_probs, size=10000)\n",
    "cum_counts = counts.astype(np.float32).cumsum(axis=0)\n",
    "estimates = cum_counts / cum_counts.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00b2103",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab tensorflow\n",
    "counts = tfd.Multinomial(1, fair_probs).sample(10000)\n",
    "cum_counts = tf.cumsum(counts, axis=0)\n",
    "estimates = cum_counts / tf.reduce_sum(cum_counts, axis=1, keepdims=True)\n",
    "estimates = estimates.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55e85c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab jax\n",
    "counts = np.random.multinomial(1, fair_probs, size=10000).astype(np.float32)\n",
    "cum_counts = counts.cumsum(axis=0)\n",
    "estimates = cum_counts / cum_counts.sum(axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0aad694",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab mxnet, tensorflow, jax\n",
    "d2l.set_figsize((4.5, 3.5))\n",
    "d2l.plt.plot(estimates[:, 0], label=(\"P(coin=heads)\"))\n",
    "d2l.plt.plot(estimates[:, 1], label=(\"P(coin=tails)\"))\n",
    "d2l.plt.axhline(y=0.5, color='black', linestyle='dashed')\n",
    "d2l.plt.gca().set_xlabel('Samples')\n",
    "d2l.plt.gca().set_ylabel('Estimated probability')\n",
    "d2l.plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5990e31a",
   "metadata": {},
   "source": [
    "Each solid curve corresponds to one of the two values of the coin\n",
    "and gives our estimated probability that the coin turns up that value\n",
    "after each group of experiments.\n",
    "The dashed black line gives the true underlying probability.\n",
    "As we get more data by conducting more experiments,\n",
    "the curves converge towards the true probability.\n",
    "You might already begin to see the shape\n",
    "of some of the more advanced questions\n",
    "that preoccupy statisticians:\n",
    "How quickly does this convergence happen?\n",
    "If we had already tested many coins\n",
    "manufactured at the same plant,\n",
    "how might we incorporate this information?\n",
    "\n",
    "##  A More Formal Treatment\n",
    "\n",
    "We have already gotten pretty far: posing\n",
    "a probabilistic model,\n",
    "generating synthetic data,\n",
    "running a statistical estimator,\n",
    "empirically assessing convergence,\n",
    "and reporting error metrics (checking the deviation).\n",
    "However, to go much further,\n",
    "we will need to be more precise.\n",
    "\n",
    "\n",
    "When dealing with randomness,\n",
    "we denote the set of possible outcomes $\\mathcal{S}$\n",
    "and call it the *sample space* or *outcome space*.\n",
    "Here, each element is a distinct possible *outcome*.\n",
    "In the case of rolling a single coin,\n",
    "$\\mathcal{S} = \\{\\textrm{heads}, \\textrm{tails}\\}$.\n",
    "For a single die, $\\mathcal{S} = \\{1, 2, 3, 4, 5, 6\\}$.\n",
    "When flipping two coins, possible outcomes are\n",
    "$\\{(\\textrm{heads}, \\textrm{heads}), (\\textrm{heads}, \\textrm{tails}), (\\textrm{tails}, \\textrm{heads}),  (\\textrm{tails}, \\textrm{tails})\\}$.\n",
    "*Events* are subsets of the sample space.\n",
    "For instance, the event \"the first coin toss comes up heads\"\n",
    "corresponds to the set $\\{(\\textrm{heads}, \\textrm{heads}), (\\textrm{heads}, \\textrm{tails})\\}$.\n",
    "Whenever the outcome $z$ of a random experiment satisfies\n",
    "$z \\in \\mathcal{A}$, then event $\\mathcal{A}$ has occurred.\n",
    "For a single roll of a die, we could define the events\n",
    "\"seeing a $5$\" ($\\mathcal{A} = \\{5\\}$)\n",
    "and \"seeing an odd number\"  ($\\mathcal{B} = \\{1, 3, 5\\}$).\n",
    "In this case, if the die came up $5$,\n",
    "we would say that both $\\mathcal{A}$ and $\\mathcal{B}$ occurred.\n",
    "On the other hand, if $z = 3$,\n",
    "then $\\mathcal{A}$ did not occur\n",
    "but $\\mathcal{B}$ did.\n",
    "\n",
    "\n",
    "A *probability* function maps events\n",
    "onto real values ${P: \\mathcal{A} \\subseteq \\mathcal{S} \\rightarrow [0,1]}$.\n",
    "The probability, denoted $P(\\mathcal{A})$, of an event $\\mathcal{A}$\n",
    "in the given sample space $\\mathcal{S}$,\n",
    "has the following properties:\n",
    "\n",
    "* The probability of any event $\\mathcal{A}$ is a nonnegative real number, i.e., $P(\\mathcal{A}) \\geq 0$;\n",
    "* The probability of the entire sample space is $1$, i.e., $P(\\mathcal{S}) = 1$;\n",
    "* For any countable sequence of events $\\mathcal{A}_1, \\mathcal{A}_2, \\ldots$ that are *mutually exclusive* (i.e., $\\mathcal{A}_i \\cap \\mathcal{A}_j = \\emptyset$ for all $i \\neq j$), the probability that any of them happens is equal to the sum of their individual probabilities, i.e., $P(\\bigcup_{i=1}^{\\infty} \\mathcal{A}_i) = \\sum_{i=1}^{\\infty} P(\\mathcal{A}_i)$.\n",
    "\n",
    "These axioms of probability theory,\n",
    "proposed by :citet:`Kolmogorov.1933`,\n",
    "can be applied to rapidly derive a number of important consequences.\n",
    "For instance, it follows immediately\n",
    "that the probability of any event $\\mathcal{A}$\n",
    "*or* its complement $\\mathcal{A}'$ occurring is 1\n",
    "(because $\\mathcal{A} \\cup \\mathcal{A}' = \\mathcal{S}$).\n",
    "We can also prove that $P(\\emptyset) = 0$\n",
    "because $1 = P(\\mathcal{S} \\cup \\mathcal{S}') = P(\\mathcal{S} \\cup \\emptyset) = P(\\mathcal{S}) + P(\\emptyset) = 1 + P(\\emptyset)$.\n",
    "Consequently, the probability of any event $\\mathcal{A}$\n",
    "*and* its complement $\\mathcal{A}'$ occurring simultaneously\n",
    "is $P(\\mathcal{A} \\cap \\mathcal{A}') = 0$.\n",
    "Informally, this tells us that impossible events\n",
    "have zero probability of occurring.\n",
    "\n",
    "\n",
    "\n",
    "## Random Variables\n",
    "\n",
    "When we spoke about events like the roll of a die\n",
    "coming up odds or the first coin toss coming up heads,\n",
    "we were invoking the idea of a *random variable*.\n",
    "Formally, random variables are mappings\n",
    "from an underlying sample space\n",
    "to a set of (possibly many) values.\n",
    "You might wonder how a random variable\n",
    "is different from the sample space,\n",
    "since both are collections of outcomes.\n",
    "Importantly, random variables can be much coarser\n",
    "than the raw sample space.\n",
    "We can define a binary random variable like \"greater than 0.5\"\n",
    "even when the underlying sample space is infinite,\n",
    "e.g., points on the line segment between $0$ and $1$.\n",
    "Additionally, multiple random variables\n",
    "can share the same underlying sample space.\n",
    "For example \"whether my home alarm goes off\"\n",
    "and \"whether my house was burgled\"\n",
    "are both binary random variables\n",
    "that share an underlying sample space.\n",
    "Consequently, knowing the value taken by one random variable\n",
    "can tell us something about the likely value of another random variable.\n",
    "Knowing that the alarm went off,\n",
    "we might suspect that the house was likely burgled.\n",
    "\n",
    "\n",
    "Every value taken by a random variable corresponds\n",
    "to a subset of the underlying sample space.\n",
    "Thus the occurrence where the random variable $X$\n",
    "takes value $v$, denoted by $X=v$, is an *event*\n",
    "and $P(X=v)$ denotes its probability.\n",
    "Sometimes this notation can get clunky,\n",
    "and we can abuse notation when the context is clear.\n",
    "For example, we might use $P(X)$ to refer broadly\n",
    "to the *distribution* of $X$, i.e.,\n",
    "the function that tells us the probability\n",
    "that $X$ takes any given value.\n",
    "Other times we write expressions\n",
    "like $P(X,Y) = P(X) P(Y)$,\n",
    "as a shorthand to express a statement\n",
    "that is true for all of the values\n",
    "that the random variables $X$ and $Y$ can take, i.e.,\n",
    "for all $i,j$ it holds that $P(X=i \\textrm{ and } Y=j) = P(X=i)P(Y=j)$.\n",
    "Other times, we abuse notation by writing\n",
    "$P(v)$ when the random variable is clear from the context.\n",
    "Since an event in probability theory is a set of outcomes from the sample space,\n",
    "we can specify a range of values for a random variable to take.\n",
    "For example, $P(1 \\leq X \\leq 3)$ denotes the probability of the event $\\{1 \\leq X \\leq 3\\}$.\n",
    "\n",
    "\n",
    "Note that there is a subtle difference\n",
    "between *discrete* random variables,\n",
    "like flips of a coin or tosses of a die,\n",
    "and *continuous* ones,\n",
    "like the weight and the height of a person\n",
    "sampled at random from the population.\n",
    "In this case we seldom really care about\n",
    "someone's exact height.\n",
    "Moreover, if we took precise enough measurements,\n",
    "we would find that no two people on the planet\n",
    "have the exact same height.\n",
    "In fact, with fine enough measurements,\n",
    "you would never have the same height\n",
    "when you wake up and when you go to sleep.\n",
    "There is little point in asking about\n",
    "the exact probability that someone\n",
    "is 1.801392782910287192 meters tall.\n",
    "Instead, we typically care more about being able to say\n",
    "whether someone's height falls into a given interval,\n",
    "say between 1.79 and 1.81 meters.\n",
    "In these cases we work with probability *densities*.\n",
    "The height of exactly 1.80 meters\n",
    "has no probability, but nonzero density.\n",
    "To work out the probability assigned to an interval,\n",
    "we must take an *integral* of the density\n",
    "over that interval.\n",
    "\n",
    "## Multiple Random Variables\n",
    "\n",
    "You might have noticed that we could not even\n",
    "make it through the previous section without\n",
    "making statements involving interactions\n",
    "among multiple random variables\n",
    "(recall that $P(X,Y) = P(X) P(Y)$).\n",
    "Most of machine learning\n",
    "is concerned with such relationships.\n",
    "Here, the sample space would be\n",
    "the population of interest,\n",
    "say customers who transact with a business,\n",
    "photographs on the Internet,\n",
    "or proteins known to biologists.\n",
    "Each random variable would represent\n",
    "the (unknown) value of a different attribute.\n",
    "Whenever we sample an individual from the population,\n",
    "we observe a realization of each of the random variables.\n",
    "Because the values taken by random variables\n",
    "correspond to subsets of the sample space\n",
    "that could be overlapping, partially overlapping,\n",
    "or entirely disjoint,\n",
    "knowing the value taken by one random variable\n",
    "can cause us to update our beliefs\n",
    "about which values of another random variable are likely.\n",
    "If a patient walks into a hospital\n",
    "and we observe that they\n",
    "are having trouble breathing\n",
    "and have lost their sense of smell,\n",
    "then we believe that they are more likely\n",
    "to have COVID-19 than we might\n",
    "if they had no trouble breathing\n",
    "and a perfectly ordinary sense of smell.\n",
    "\n",
    "\n",
    "When working with multiple random variables,\n",
    "we can construct events corresponding\n",
    "to every combination of values\n",
    "that the variables can jointly take.\n",
    "The probability function that assigns\n",
    "probabilities to each of these combinations\n",
    "(e.g. $A=a$ and $B=b$)\n",
    "is called the *joint probability* function\n",
    "and simply returns the probability assigned\n",
    "to the intersection of the corresponding subsets\n",
    "of the sample space.\n",
    "The *joint probability* assigned to the event\n",
    "where random variables $A$ and $B$\n",
    "take values $a$ and $b$, respectively,\n",
    "is denoted $P(A = a, B = b)$,\n",
    "where the comma indicates \"and\".\n",
    "Note that for any values $a$ and $b$,\n",
    "it follows that\n",
    "\n",
    "$$P(A=a, B=b) \\leq P(A=a) \\textrm{ and } P(A=a, B=b) \\leq P(B = b),$$\n",
    "\n",
    "since for $A=a$ and $B=b$ to happen,\n",
    "$A=a$ has to happen *and* $B=b$ also has to happen.\n",
    "Interestingly, the joint probability\n",
    "tells us all that we can know about these\n",
    "random variables in a probabilistic sense,\n",
    "and can be used to derive many other\n",
    "useful quantities, including recovering the\n",
    "individual distributions $P(A)$ and $P(B)$.\n",
    "To recover $P(A=a)$ we simply sum up\n",
    "$P(A=a, B=v)$ over all values $v$\n",
    "that the random variable $B$ can take:\n",
    "$P(A=a) = \\sum_v P(A=a, B=v)$.\n",
    "\n",
    "\n",
    "The ratio $\\frac{P(A=a, B=b)}{P(A=a)} \\leq 1$\n",
    "turns out to be extremely important.\n",
    "It is called the *conditional probability*,\n",
    "and is denoted via the \"$\\mid$\" symbol:\n",
    "\n",
    "$$P(B=b \\mid A=a) = P(A=a,B=b)/P(A=a).$$\n",
    "\n",
    "It tells us the new probability\n",
    "associated with the event $B=b$,\n",
    "once we condition on the fact $A=a$ took place.\n",
    "We can think of this conditional probability\n",
    "as restricting attention only to the subset\n",
    "of the sample space associated with $A=a$\n",
    "and then renormalizing so that\n",
    "all probabilities sum to 1.\n",
    "Conditional probabilities\n",
    "are in fact just ordinary probabilities\n",
    "and thus respect all of the axioms,\n",
    "as long as we condition all terms\n",
    "on the same event and thus\n",
    "restrict attention to the same sample space.\n",
    "For instance, for disjoint events\n",
    "$\\mathcal{B}$ and $\\mathcal{B}'$, we have that\n",
    "$P(\\mathcal{B} \\cup \\mathcal{B}' \\mid A = a) = P(\\mathcal{B} \\mid A = a) + P(\\mathcal{B}' \\mid A = a)$.\n",
    "\n",
    "\n",
    "Using the definition of conditional probabilities,\n",
    "we can derive the famous result called *Bayes' theorem*.\n",
    "By construction, we have that $P(A, B) = P(B\\mid A) P(A)$\n",
    "and $P(A, B) = P(A\\mid B) P(B)$.\n",
    "Combining both equations yields\n",
    "$P(B\\mid A) P(A) = P(A\\mid B) P(B)$ and hence\n",
    "\n",
    "$$P(A \\mid B) = \\frac{P(B\\mid A) P(A)}{P(B)}.$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This simple equation has profound implications because\n",
    "it allows us to reverse the order of conditioning.\n",
    "If we know how to estimate $P(B\\mid A)$, $P(A)$, and $P(B)$,\n",
    "then we can estimate $P(A\\mid B)$.\n",
    "We often find it easier to estimate one term directly\n",
    "but not the other and Bayes' theorem can come to the rescue here.\n",
    "For instance, if we know the prevalence of symptoms for a given disease,\n",
    "and the overall prevalences of the disease and symptoms, respectively,\n",
    "we can determine how likely someone is\n",
    "to have the disease based on their symptoms.\n",
    "In some cases we might not have direct access to $P(B)$,\n",
    "such as the prevalence of symptoms.\n",
    "In this case a simplified version of Bayes' theorem comes in handy:\n",
    "\n",
    "$$P(A \\mid B) \\propto P(B \\mid A) P(A).$$\n",
    "\n",
    "Since we know that $P(A \\mid B)$ must be normalized to $1$, i.e., $\\sum_a P(A=a \\mid B) = 1$,\n",
    "we can use it to compute\n",
    "\n",
    "$$P(A \\mid B) = \\frac{P(B \\mid A) P(A)}{\\sum_a P(B \\mid A=a) P(A = a)}.$$\n",
    "\n",
    "In Bayesian statistics, we think of an observer\n",
    "as possessing some (subjective) prior beliefs\n",
    "about the plausibility of the available hypotheses\n",
    "encoded in the *prior* $P(H)$,\n",
    "and a *likelihood function* that says how likely\n",
    "one is to observe any value of the collected evidence\n",
    "for each of the hypotheses in the class $P(E \\mid H)$.\n",
    "Bayes' theorem is then interpreted as telling us\n",
    "how to update the initial *prior* $P(H)$\n",
    "in light of the available evidence $E$\n",
    "to produce *posterior* beliefs\n",
    "$P(H \\mid E) = \\frac{P(E \\mid H) P(H)}{P(E)}$.\n",
    "Informally, this can be stated as\n",
    "\"posterior equals prior times likelihood, divided by the evidence\".\n",
    "Now, because the evidence $P(E)$ is the same for all hypotheses,\n",
    "we can get away with simply normalizing over the hypotheses.\n",
    "\n",
    "Note that $\\sum_a P(A=a \\mid B) = 1$ also allows us to *marginalize* over random variables. That is, we can drop variables from a joint distribution such as $P(A, B)$. After all, we have that\n",
    "\n",
    "$$\\sum_a P(B \\mid A=a) P(A=a) = \\sum_a P(B, A=a) = P(B).$$\n",
    "\n",
    "Independence is another fundamentally important concept\n",
    "that forms the backbone of\n",
    "many important ideas in statistics.\n",
    "In short, two variables are *independent*\n",
    "if conditioning on the value of $A$ does not\n",
    "cause any change to the probability distribution\n",
    "associated with $B$ and vice versa.\n",
    "More formally, independence, denoted $A \\perp B$,\n",
    "requires that $P(A \\mid B) = P(A)$ and, consequently,\n",
    "that $P(A,B) = P(A \\mid B) P(B) = P(A) P(B)$.\n",
    "Independence is often an appropriate assumption.\n",
    "For example, if the random variable $A$\n",
    "represents the outcome from tossing one fair coin\n",
    "and the random variable $B$\n",
    "represents the outcome from tossing another,\n",
    "then knowing whether $A$ came up heads\n",
    "should not influence the probability\n",
    "of $B$ coming up heads.\n",
    "\n",
    "\n",
    "Independence is especially useful when it holds among the successive\n",
    "draws of our data from some underlying distribution\n",
    "(allowing us to make strong statistical conclusions)\n",
    "or when it holds among various variables in our data,\n",
    "allowing us to work with simpler models\n",
    "that encode this independence structure.\n",
    "On the other hand, estimating the dependencies\n",
    "among random variables is often the very aim of learning.\n",
    "We care to estimate the probability of disease given symptoms\n",
    "specifically because we believe\n",
    "that diseases and symptoms are *not* independent.\n",
    "\n",
    "\n",
    "Note that because conditional probabilities are proper probabilities,\n",
    "the concepts of independence and dependence also apply to them.\n",
    "Two random variables $A$ and $B$ are *conditionally independent*\n",
    "given a third variable $C$ if and only if $P(A, B \\mid C) = P(A \\mid C)P(B \\mid C)$.\n",
    "Interestingly, two variables can be independent in general\n",
    "but become dependent when conditioning on a third.\n",
    "This often occurs when the two random variables $A$ and $B$\n",
    "correspond to causes of some third variable $C$.\n",
    "For example, broken bones and lung cancer might be independent\n",
    "in the general population but if we condition on being in the hospital\n",
    "then we might find that broken bones are negatively correlated with lung cancer.\n",
    "That is because the broken bone *explains away* why some person is in the hospital\n",
    "and thus lowers the probability that they are hospitalized because of having lung cancer.\n",
    "\n",
    "\n",
    "And conversely, two dependent random variables\n",
    "can become independent upon conditioning on a third.\n",
    "This often happens when two otherwise unrelated events\n",
    "have a common cause.\n",
    "Shoe size and reading level are highly correlated\n",
    "among elementary school students,\n",
    "but this correlation disappears if we condition on age.\n",
    "\n",
    "\n",
    "\n",
    "## An Example\n",
    ":label:`subsec_probability_hiv_app`\n",
    "\n",
    "Let's put our skills to the test.\n",
    "Assume that a doctor administers an HIV test to a patient.\n",
    "This test is fairly accurate and fails only with 1% probability\n",
    "if the patient is healthy but reported as diseased,\n",
    "i.e., healthy patients test positive in 1% of cases.\n",
    "Moreover, it never fails to detect HIV if the patient actually has it.\n",
    "We use $D_1 \\in \\{0, 1\\}$ to indicate the diagnosis\n",
    "($0$ if negative and $1$ if positive)\n",
    "and $H \\in \\{0, 1\\}$ to denote the HIV status.\n",
    "\n",
    "| Conditional probability | $H=1$ | $H=0$ |\n",
    "|:------------------------|------:|------:|\n",
    "| $P(D_1 = 1 \\mid H)$        |     1 |  0.01 |\n",
    "| $P(D_1 = 0 \\mid H)$        |     0 |  0.99 |\n",
    "\n",
    "Note that the column sums are all 1 (but the row sums do not),\n",
    "since they are conditional probabilities.\n",
    "Let's compute the probability of the patient having HIV\n",
    "if the test comes back positive, i.e., $P(H = 1 \\mid D_1 = 1)$.\n",
    "Intuitively this is going to depend on how common the disease is,\n",
    "since it affects the number of false alarms.\n",
    "Assume that the population is fairly free of the disease, e.g., $P(H=1) = 0.0015$.\n",
    "To apply Bayes' theorem, we need to apply marginalization\n",
    "to determine\n",
    "\n",
    "$$\\begin{aligned}\n",
    "P(D_1 = 1)\n",
    "=& P(D_1=1, H=0) + P(D_1=1, H=1)  \\\\\n",
    "=& P(D_1=1 \\mid H=0) P(H=0) + P(D_1=1 \\mid H=1) P(H=1) \\\\\n",
    "=& 0.011485.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This leads us to\n",
    "\n",
    "$$P(H = 1 \\mid D_1 = 1) = \\frac{P(D_1=1 \\mid H=1) P(H=1)}{P(D_1=1)} = 0.1306.$$\n",
    "\n",
    "In other words, there is only a 13.06% chance\n",
    "that the patient actually has HIV,\n",
    "despite the test being pretty accurate.\n",
    "As we can see, probability can be counterintuitive.\n",
    "What should a patient do upon receiving such terrifying news?\n",
    "Likely, the patient would ask the physician\n",
    "to administer another test to get clarity.\n",
    "The second test has different characteristics\n",
    "and it is not as good as the first one.\n",
    "\n",
    "| Conditional probability | $H=1$ | $H=0$ |\n",
    "|:------------------------|------:|------:|\n",
    "| $P(D_2 = 1 \\mid H)$          |  0.98 |  0.03 |\n",
    "| $P(D_2 = 0 \\mid H)$          |  0.02 |  0.97 |\n",
    "\n",
    "Unfortunately, the second test comes back positive, too.\n",
    "Let's calculate the requisite probabilities to invoke Bayes' theorem\n",
    "by assuming conditional independence:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "P(D_1 = 1, D_2 = 1 \\mid H = 0)\n",
    "& = P(D_1 = 1 \\mid H = 0) P(D_2 = 1 \\mid H = 0)\n",
    "=& 0.0003, \\\\\n",
    "P(D_1 = 1, D_2 = 1 \\mid H = 1)\n",
    "& = P(D_1 = 1 \\mid H = 1) P(D_2 = 1 \\mid H = 1)\n",
    "=& 0.98.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Now we can apply marginalization to obtain the probability\n",
    "that both tests come back positive:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "&P(D_1 = 1, D_2 = 1)\\\\\n",
    "&= P(D_1 = 1, D_2 = 1, H = 0) + P(D_1 = 1, D_2 = 1, H = 1)  \\\\\n",
    "&= P(D_1 = 1, D_2 = 1 \\mid H = 0)P(H=0) + P(D_1 = 1, D_2 = 1 \\mid H = 1)P(H=1)\\\\\n",
    "&= 0.00176955.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Finally, the probability of the patient having HIV given that both tests are positive is\n",
    "\n",
    "$$P(H = 1 \\mid D_1 = 1, D_2 = 1)\n",
    "= \\frac{P(D_1 = 1, D_2 = 1 \\mid H=1) P(H=1)}{P(D_1 = 1, D_2 = 1)}\n",
    "= 0.8307.$$\n",
    "\n",
    "That is, the second test allowed us to gain much higher confidence that not all is well.\n",
    "Despite the second test being considerably less accurate than the first one,\n",
    "it still significantly improved our estimate.\n",
    "The assumption of both tests being conditionally independent of each other\n",
    "was crucial for our ability to generate a more accurate estimate.\n",
    "Take the extreme case where we run the same test twice.\n",
    "In this situation we would expect the same outcome both times,\n",
    "hence no additional insight is gained from running the same test again.\n",
    "The astute reader might have noticed that the diagnosis behaved\n",
    "like a classifier hiding in plain sight\n",
    "where our ability to decide whether a patient is healthy\n",
    "increases as we obtain more features (test outcomes).\n",
    "\n",
    "\n",
    "## Expectations\n",
    "\n",
    "Often, making decisions requires not just looking\n",
    "at the probabilities assigned to individual events\n",
    "but composing them together into useful aggregates\n",
    "that can provide us with guidance.\n",
    "For example, when random variables take continuous scalar values,\n",
    "we often care about knowing what value to expect *on average*.\n",
    "This quantity is formally called an *expectation*.\n",
    "If we are making investments,\n",
    "the first quantity of interest\n",
    "might be the return we can expect,\n",
    "averaging over all the possible outcomes\n",
    "(and weighting by the appropriate probabilities).\n",
    "For instance, say that with 50% probability,\n",
    "an investment might fail altogether,\n",
    "with 40% probability it might provide a 2$\\times$ return,\n",
    "and with 10% probability it might provide a 10$\\times$ return 10$\\times$.\n",
    "To calculate the expected return,\n",
    "we sum over all returns, multiplying each\n",
    "by the probability that they will occur.\n",
    "This yields the expectation\n",
    "$0.5 \\cdot 0 + 0.4 \\cdot 2 + 0.1 \\cdot 10 = 1.8$.\n",
    "Hence the expected return is 1.8$\\times$.\n",
    "\n",
    "\n",
    "In general, the *expectation* (or average)\n",
    "of the random variable $X$ is defined as\n",
    "\n",
    "$$E[X] = E_{x \\sim P}[x] = \\sum_{x} x P(X = x).$$\n",
    "\n",
    "Likewise, for densities we obtain $E[X] = \\int x \\;dp(x)$.\n",
    "Sometimes we are interested in the expected value\n",
    "of some function of $x$.\n",
    "We can calculate these expectations as\n",
    "\n",
    "$$E_{x \\sim P}[f(x)] = \\sum_x f(x) P(x) \\textrm{ and } E_{x \\sim P}[f(x)] = \\int f(x) p(x) \\;dx$$\n",
    "\n",
    "for discrete probabilities and densities, respectively.\n",
    "Returning to the investment example from above,\n",
    "$f$ might be the *utility* (happiness)\n",
    "associated with the return.\n",
    "Behavior economists have long noted\n",
    "that people associate greater disutility\n",
    "with losing money than the utility gained\n",
    "from earning one dollar relative to their baseline.\n",
    "Moreover, the value of money tends to be sub-linear.\n",
    "Possessing 100k dollars versus zero dollars\n",
    "can make the difference between paying the rent,\n",
    "eating well, and enjoying quality healthcare\n",
    "versus suffering through homelessness.\n",
    "On the other hand, the gains due to possessing\n",
    "200k versus 100k are less dramatic.\n",
    "Reasoning like this motivates the cliché\n",
    "that \"the utility of money is logarithmic\".\n",
    "\n",
    "\n",
    "If  the utility associated with a total loss were $-1$,\n",
    "and the utilities associated with returns of $1$, $2$, and $10$\n",
    "were $1$, $2$ and $4$, respectively,\n",
    "then the expected happiness of investing\n",
    "would be $0.5 \\cdot (-1) + 0.4 \\cdot 2 + 0.1 \\cdot 4 = 0.7$\n",
    "(an expected loss of utility of 30%).\n",
    "If indeed this were your utility function,\n",
    "you might be best off keeping the money in the bank.\n",
    "\n",
    "For financial decisions,\n",
    "we might also want to measure\n",
    "how *risky* an investment is.\n",
    "Here, we care not just about the expected value\n",
    "but how much the actual values tend to *vary*\n",
    "relative to this value.\n",
    "Note that we cannot just take\n",
    "the expectation of the difference\n",
    "between the actual and expected values.\n",
    "This is because the expectation of a difference\n",
    "is the difference of the expectations,\n",
    "i.e., $E[X - E[X]] = E[X] - E[E[X]] = 0$.\n",
    "However, we can look at the expectation\n",
    "of any non-negative function of this difference.\n",
    "The *variance* of a random variable is calculated by looking\n",
    "at the expected value of the *squared* differences:\n",
    "\n",
    "$$\\textrm{Var}[X] = E\\left[(X - E[X])^2\\right] = E[X^2] - E[X]^2.$$\n",
    "\n",
    "Here the equality follows by expanding\n",
    "$(X - E[X])^2 = X^2 - 2 X E[X] + E[X]^2$\n",
    "and taking expectations for each term.\n",
    "The square root of the variance is another\n",
    "useful quantity called the *standard deviation*.\n",
    "While this and the variance\n",
    "convey the same information (either can be calculated from the other),\n",
    "the standard deviation has the nice property\n",
    "that it is expressed in the same units\n",
    "as the original quantity represented\n",
    "by the random variable.\n",
    "\n",
    "Lastly, the variance of a function\n",
    "of a random variable\n",
    "is defined analogously as\n",
    "\n",
    "$$\\textrm{Var}_{x \\sim P}[f(x)] = E_{x \\sim P}[f^2(x)] - E_{x \\sim P}[f(x)]^2.$$\n",
    "\n",
    "Returning to our investment example,\n",
    "we can now compute the variance of the investment.\n",
    "It is given by $0.5 \\cdot 0 + 0.4 \\cdot 2^2 + 0.1 \\cdot 10^2 - 1.8^2 = 8.36$.\n",
    "For all intents and purposes this is a risky investment.\n",
    "Note that by mathematical convention mean and variance\n",
    "are often referenced as $\\mu$ and $\\sigma^2$.\n",
    "This is particularly the case whenever we use it\n",
    "to parametrize a Gaussian distribution.\n",
    "\n",
    "In the same way as we introduced expectations\n",
    "and variance for *scalar* random variables,\n",
    "we can do so for vector-valued ones.\n",
    "Expectations are easy, since we can apply them elementwise.\n",
    "For instance, $\\boldsymbol{\\mu} \\stackrel{\\textrm{def}}{=} E_{\\mathbf{x} \\sim P}[\\mathbf{x}]$\n",
    "has coordinates $\\mu_i = E_{\\mathbf{x} \\sim P}[x_i]$.\n",
    "*Covariances* are more complicated.\n",
    "We define them by taking expectations of the *outer product*\n",
    "of the difference between random variables and their mean:\n",
    "\n",
    "$$\\boldsymbol{\\Sigma} \\stackrel{\\textrm{def}}{=} \\textrm{Cov}_{\\mathbf{x} \\sim P}[\\mathbf{x}] = E_{\\mathbf{x} \\sim P}\\left[(\\mathbf{x} - \\boldsymbol{\\mu}) (\\mathbf{x} - \\boldsymbol{\\mu})^\\top\\right].$$\n",
    "\n",
    "This matrix $\\boldsymbol{\\Sigma}$ is referred to as the covariance matrix.\n",
    "An easy way to see its effect is to consider some vector $\\mathbf{v}$\n",
    "of the same size as $\\mathbf{x}$.\n",
    "It follows that\n",
    "\n",
    "$$\\mathbf{v}^\\top \\boldsymbol{\\Sigma} \\mathbf{v} = E_{\\mathbf{x} \\sim P}\\left[\\mathbf{v}^\\top(\\mathbf{x} - \\boldsymbol{\\mu}) (\\mathbf{x} - \\boldsymbol{\\mu})^\\top \\mathbf{v}\\right] = \\textrm{Var}_{x \\sim P}[\\mathbf{v}^\\top \\mathbf{x}].$$\n",
    "\n",
    "As such, $\\boldsymbol{\\Sigma}$ allows us to compute the variance\n",
    "for any linear function of $\\mathbf{x}$\n",
    "by a simple matrix multiplication.\n",
    "The off-diagonal elements tell us how correlated the coordinates are:\n",
    "a value of 0 means no correlation,\n",
    "where a larger positive value\n",
    "means that they are more strongly correlated.\n",
    "\n",
    "\n",
    "\n",
    "## Discussion\n",
    "\n",
    "In machine learning, there are many things to be uncertain about!\n",
    "We can be uncertain about the value of a label given an input.\n",
    "We can be uncertain about the estimated value of a parameter.\n",
    "We can even be uncertain about whether data arriving at deployment\n",
    "is even from the same distribution as the training data.\n",
    "\n",
    "By *aleatoric uncertainty*, we mean uncertainty\n",
    "that is intrinsic to the problem,\n",
    "and due to genuine randomness\n",
    "unaccounted for by the observed variables.\n",
    "By *epistemic uncertainty*, we mean uncertainty\n",
    "over a model's parameters, the sort of uncertainty\n",
    "that we can hope to reduce by collecting more data.\n",
    "We might have epistemic uncertainty\n",
    "concerning the probability\n",
    "that a coin turns up heads,\n",
    "but even once we know this probability,\n",
    "we are left with aleatoric uncertainty\n",
    "about the outcome of any future toss.\n",
    "No matter how long we watch someone tossing a fair coin,\n",
    "we will never be more or less than 50% certain\n",
    "that the next toss will come up heads.\n",
    "These terms come from mechanical modeling,\n",
    "(see e.g., :citet:`Der-Kiureghian.Ditlevsen.2009` for a review on this aspect of [uncertainty quantification](https://en.wikipedia.org/wiki/Uncertainty_quantification)).\n",
    "It is worth noting, however, that these terms constitute a slight abuse of language.\n",
    "The term *epistemic* refers to anything concerning *knowledge*\n",
    "and thus, in the philosophical sense, all uncertainty is epistemic.\n",
    "\n",
    "\n",
    "We saw that sampling data from some unknown probability distribution\n",
    "can provide us with information that can be used to estimate\n",
    "the parameters of the data generating distribution.\n",
    "That said, the rate at which this is possible can be quite slow.\n",
    "In our coin tossing example (and many others)\n",
    "we can do no better than to design estimators\n",
    "that converge at a rate of $1/\\sqrt{n}$,\n",
    "where $n$ is the sample size (e.g., the number of tosses).\n",
    "This means that by going from 10 to 1000 observations (usually a very achievable task)\n",
    "we see a tenfold reduction of uncertainty,\n",
    "whereas the next 1000 observations help comparatively little,\n",
    "offering only a 1.41 times reduction.\n",
    "This is a persistent feature of machine learning:\n",
    "while there are often easy gains, it takes a very large amount of data,\n",
    "and often with it an enormous amount of computation, to make further gains.\n",
    "For an empirical review of this fact for large scale language models see :citet:`Revels.Lubin.Papamarkou.2016`.\n",
    "\n",
    "We also sharpened our language and tools for statistical modeling.\n",
    "In the process of that we learned about conditional probabilities\n",
    "and about one of the most important equations in statistics---Bayes' theorem.\n",
    "It is an effective tool for decoupling information conveyed by data\n",
    "through a likelihood term $P(B \\mid A)$ that addresses\n",
    "how well observations $B$ match a choice of parameters $A$,\n",
    "and a prior probability $P(A)$ which governs how plausible\n",
    "a particular choice of $A$ was in the first place.\n",
    "In particular, we saw how this rule can be applied\n",
    "to assign probabilities to diagnoses,\n",
    "based on the efficacy of the test *and*\n",
    "the prevalence of the disease itself (i.e., our prior).\n",
    "\n",
    "Lastly, we introduced a first set of nontrivial questions\n",
    "about the effect of a specific probability distribution,\n",
    "namely expectations and variances.\n",
    "While there are many more than just linear and quadratic\n",
    "expectations for a probability distribution,\n",
    "these two already provide a good deal of knowledge\n",
    "about the possible behavior of the distribution.\n",
    "For instance, [Chebyshev's inequality](https://en.wikipedia.org/wiki/Chebyshev%27s_inequality)\n",
    "states that $P(|X - \\mu| \\geq k \\sigma) \\leq 1/k^2$,\n",
    "where $\\mu$ is the expectation, $\\sigma^2$ is the variance of the distribution,\n",
    "and $k > 1$ is a confidence parameter of our choosing.\n",
    "It tells us that draws from a distribution lie\n",
    "with at least 50% probability\n",
    "within a $[-\\sqrt{2} \\sigma, \\sqrt{2} \\sigma]$\n",
    "interval centered on the expectation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Give an example where observing more data can reduce the amount of uncertainty about the outcome to an arbitrarily low level.\n",
    "1. Give an example where observing more data will only reduce the amount of uncertainty up to a point and then no further. Explain why this is the case and where you expect this point to occur.\n",
    "1. We empirically demonstrated convergence to the mean for the toss of a coin. Calculate the variance of the estimate of the probability that we see a head after drawing $n$ samples.\n",
    "    1. How does the variance scale with the number of observations?\n",
    "    1. Use Chebyshev's inequality to bound the deviation from the expectation.\n",
    "    1. How does it relate to the central limit theorem?\n",
    "1. Assume that we draw $m$ samples $x_i$ from a probability distribution with zero mean and unit variance. Compute the averages $z_m \\stackrel{\\textrm{def}}{=} m^{-1} \\sum_{i=1}^m x_i$. Can we apply Chebyshev's inequality for every $z_m$ independently? Why not?\n",
    "1. Given two events with probability $P(\\mathcal{A})$ and $P(\\mathcal{B})$, compute upper and lower bounds on $P(\\mathcal{A} \\cup \\mathcal{B})$ and $P(\\mathcal{A} \\cap \\mathcal{B})$. Hint: graph the situation using a [Venn diagram](https://en.wikipedia.org/wiki/Venn_diagram).\n",
    "1. Assume that we have a sequence of random variables, say $A$, $B$, and $C$, where $B$ only depends on $A$, and $C$ only depends on $B$, can you simplify the joint probability $P(A, B, C)$? Hint: this is a [Markov chain](https://en.wikipedia.org/wiki/Markov_chain).\n",
    "1. In :numref:`subsec_probability_hiv_app`, assume that the outcomes of the two tests are not independent. In particular assume that either test on its own has a false positive rate of 10% and a false negative rate of 1%. That is, assume that $P(D =1 \\mid H=0) = 0.1$ and that $P(D = 0 \\mid H=1) = 0.01$. Moreover, assume that for $H = 1$ (infected) the test outcomes are conditionally independent, i.e., that $P(D_1, D_2 \\mid H=1) = P(D_1 \\mid H=1) P(D_2 \\mid H=1)$ but that for healthy patients the outcomes are coupled via $P(D_1 = D_2 = 1 \\mid H=0) = 0.02$.\n",
    "    1. Work out the joint probability table for $D_1$ and $D_2$, given $H=0$ based on the information you have so far.\n",
    "    1. Derive the probability that the patient is diseased ($H=1$) after one test returns positive. You can assume the same baseline probability $P(H=1) = 0.0015$ as before.\n",
    "    1. Derive the probability that the patient is diseased ($H=1$) after both tests return positive.\n",
    "1. Assume that you are an asset manager for an investment bank and you have a choice of stocks $s_i$ to invest in. Your portfolio needs to add up to $1$ with weights $\\alpha_i$ for each stock. The stocks have an average return $\\boldsymbol{\\mu} = E_{\\mathbf{s} \\sim P}[\\mathbf{s}]$ and covariance $\\boldsymbol{\\Sigma} = \\textrm{Cov}_{\\mathbf{s} \\sim P}[\\mathbf{s}]$.\n",
    "    1. Compute the expected return for a given portfolio $\\boldsymbol{\\alpha}$.\n",
    "    1. If you wanted to maximize the return of the portfolio, how should you choose your investment?\n",
    "    1. Compute the *variance* of the portfolio.\n",
    "    1. Formulate an optimization problem of maximizing the return while keeping the variance constrained to an upper bound. This is the Nobel-Prize winning [Markovitz portfolio](https://en.wikipedia.org/wiki/Markowitz_model) :cite:`Mangram.2013`. To solve it you will need a quadratic programming solver, something way beyond the scope of this book.\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/36)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/37)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "[Discussions](https://discuss.d2l.ai/t/198)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`jax`\n",
    "[Discussions](https://discuss.d2l.ai/t/17971)\n",
    ":end_tab:"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
