{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49d302fb",
   "metadata": {},
   "source": [
    "# Pretraining fastText\n",
    ":label:`sec_word2vec_gluon`\n",
    "\n",
    "In this section, we will\n",
    "train a skip-gram model defined in\n",
    ":numref:`sec_word2vec`.\n",
    "\n",
    "First, import the\n",
    "packages and modules required for the experiment, and load the PTB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c3063fd",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "from collections import defaultdict\n",
    "from d2l import mxnet as d2l\n",
    "from functools import partial\n",
    "from mxnet import autograd, gluon, init, np, npx, cpu\n",
    "from mxnet.gluon import nn\n",
    "import random\n",
    "\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e68212db",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "def compute_subword(token):\n",
    "    if token[0] != '<' and token[-1] != '>':\n",
    "        token = '<' + token + '>'\n",
    "        subwords = {token}\n",
    "        for i in range(len(token)-3):\n",
    "            for j in range(i + 3, len(token)+1):\n",
    "                if j - i <= 6:\n",
    "                    subwords.add(token[i:j])\n",
    "        return subwords\n",
    "    else:\n",
    "        return [token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07bc1681",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "def get_subword_map(vocab):\n",
    "    tokenid_to_subword, subword_to_idx = defaultdict(list), defaultdict(int)\n",
    "    for token, tokenid in vocab.token_to_idx.items():\n",
    "        subwords = compute_subword(token)\n",
    "        for subword in subwords:\n",
    "            if subword not in subword_to_idx:\n",
    "                subword_to_idx[subword] = len(subword_to_idx)\n",
    "            tokenid_to_subword[tokenid].append(subword_to_idx[subword])\n",
    "    return tokenid_to_subword, subword_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abdbc8b8",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "def token_transform(tokens, vocab, subword_map):\n",
    "    if not isinstance(tokens, (list, tuple)):\n",
    "        return d2l.truncate_pad(subword_map[tokens],\n",
    "                                 64, vocab['<pad>'])\n",
    "    return [token_transform(token, vocab, subword_map) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c21cec4b",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "def batchify(data, vocab, subword_map):\n",
    "    max_len = max(len(c) + len(n) for _, c, n in data)\n",
    "    centers, contexts_negatives, masks, labels = [], [], [], []\n",
    "    for center, context, negative in data:\n",
    "        cur_len = len(context) + len(negative)\n",
    "        centers += [token_transform([center], vocab, subword_map)]\n",
    "        contexts_negatives += [token_transform(context + negative + \\\n",
    "                               [1] * (max_len - cur_len), vocab, subword_map)]\n",
    "        masks += [[1] * cur_len + [0] * (max_len - cur_len)]\n",
    "        labels += [[1] * len(context) + [0] * (max_len - len(context))]\n",
    "    return (np.array(centers), np.array(contexts_negatives),\n",
    "            np.array(masks), np.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19427e52",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "def load_data_ptb(batch_size, max_window_size, num_noise_words):\n",
    "    num_workers = d2l.get_dataloader_workers()\n",
    "    sentences = d2l.read_ptb()\n",
    "    vocab = d2l.Vocab(sentences, min_freq=10, reserved_tokens=['<pad>'])\n",
    "    subsampled = d2l.subsampling(sentences, vocab)\n",
    "    corpus = [vocab[line] for line in subsampled]\n",
    "    all_centers, all_contexts = d2l.get_centers_and_contexts(\n",
    "        corpus, max_window_size)\n",
    "    all_negatives = d2l.get_negatives(all_contexts, corpus, num_noise_words)\n",
    "    dataset = gluon.data.ArrayDataset(\n",
    "        all_centers, all_contexts, all_negatives)\n",
    "    subword_map, subword_to_idx = get_subword_map(vocab)\n",
    "    data_iter = gluon.data.DataLoader(dataset, batch_size, shuffle=True,\n",
    "                                      batchify_fn=partial(batchify, vocab=vocab, subword_map=subword_map),\n",
    "                                      num_workers=num_workers)\n",
    "    return data_iter, vocab, subword_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "706f5f40",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "batch_size, max_window_size, num_noise_words = 512, 5, 5\n",
    "data_iter, vocab, subword_to_idx = load_data_ptb(batch_size, max_window_size, num_noise_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84f36c19",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "names = ['centers', 'contexts_negatives', 'masks', 'labels']\n",
    "for batch in data_iter:\n",
    "    for name, data in zip(names, batch):\n",
    "        print(name, 'shape:', data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91476ab4",
   "metadata": {},
   "source": [
    "## The Skip-Gram Model\n",
    "\n",
    "We will implement the skip-gram model by using embedding\n",
    "layers and minibatch multiplication. These methods are also often used to\n",
    "implement other natural language processing applications.\n",
    "\n",
    "### Embedding Layer\n",
    "The layer in which the obtained word is embedded is called the embedding layer,\n",
    "which can be obtained by creating an `nn.Embedding` instance in Gluon. The\n",
    "weight of the embedding layer is a matrix whose number of rows is the dictionary\n",
    "size (`input_dim`) and whose number of columns is the dimension of each word\n",
    "vector (`output_dim`). We set the dictionary size to $20$ and the word vector\n",
    "dimension to $4$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a15a89a1",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "embed = nn.Embedding(input_dim=20, output_dim=4)\n",
    "embed.initialize()\n",
    "embed.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578641cf",
   "metadata": {},
   "source": [
    "### Skip-gram Model Forward Calculation\n",
    "\n",
    "In forward calculation, the input of\n",
    "the skip-gram model contains the central target word index `center` and the\n",
    "concatenated context and noise word index `contexts_and_negatives`. In which,\n",
    "the `center` variable has the shape (batch size, 1), while the\n",
    "`contexts_and_negatives` variable has the shape (batch size, `max_len`). These\n",
    "two variables are first transformed from word indexes to word vectors by the\n",
    "word embedding layer, and then the output of shape (batch size, 1, `max_len`) is\n",
    "obtained by minibatch multiplication. Each element in the output is the inner\n",
    "product of the central target word vector and the context word vector or noise\n",
    "word vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f26eca5d",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    }
   },
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "def skip_gram(center, contexts_and_negatives, embed_v, embed_u, padding):\n",
    "    v_embedding = embed_v(center)\n",
    "    v_mask = (center!=padding).astype('float32')\n",
    "    v = (v_embedding * np.expand_dims(v_mask, axis=-1)).sum(-1)/(np.expand_dims(v_mask.sum(-1), axis=-1)+1e-5)\n",
    "    u_embedding = embed_u(contexts_and_negatives)\n",
    "    u_mask = (contexts_and_negatives!=padding).astype('float32')\n",
    "    u = (u_embedding * np.expand_dims(u_mask, axis=-1)).sum(-1)/(np.expand_dims(u_mask.sum(-1), axis=-1)+1e-5)\n",
    "    pred = npx.batch_dot(v, u.swapaxes(1, 2))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9688a1ff",
   "metadata": {},
   "source": [
    "Verify that the output shape should be (batch size, 1, `max_len`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d8c7758",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "skip_gram(np.ones((2, 1, 64)), np.ones((2, 6, 64)), embed, embed, vocab['<pad>']).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18fc321",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Before training the word embedding model, we need to define the\n",
    "loss function of the model.\n",
    "\n",
    "### Binary Cross Entropy Loss Function\n",
    "\n",
    "According\n",
    "to the definition of the loss function in negative sampling, we can directly use\n",
    "Gluon's binary cross-entropy loss function `SigmoidBinaryCrossEntropyLoss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1370522",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "loss = gluon.loss.SigmoidBinaryCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430ea745",
   "metadata": {},
   "source": [
    "It is worth mentioning that we can use the mask variable to specify the partial\n",
    "predicted value and label that participate in loss function calculation in the\n",
    "minibatch: when the mask is 1, the predicted value and label of the\n",
    "corresponding position will participate in the calculation of the loss function;\n",
    "When the mask is 0, the predicted value and label of the corresponding position\n",
    "do not participate in the calculation of the loss function. As we mentioned\n",
    "earlier, mask variables can be used to avoid the effect of padding on loss\n",
    "function calculations.\n",
    "\n",
    "Given two identical examples, different masks lead to\n",
    "different loss values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "848bc0f5",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    }
   },
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "pred = np.array([[.5]*4]*2)\n",
    "label = np.array([[1, 0, 1, 0]]*2)\n",
    "mask = np.array([[1, 1, 1, 1], [1, 1, 0, 0]])\n",
    "loss(pred, label, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd7480e",
   "metadata": {},
   "source": [
    "We can normalize the loss in each example due to various lengths in each\n",
    "example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4255d9bb",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "15"
    }
   },
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "loss(pred, label, mask) / mask.sum(axis=1) * mask.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d32d630",
   "metadata": {},
   "source": [
    "### Initializing Model Parameters\n",
    "\n",
    "We construct the embedding layers of the\n",
    "central and context words, respectively, and set the hyperparameter word vector\n",
    "dimension `embed_size` to 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5a7388d",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "16"
    }
   },
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "embed_size = 100\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Embedding(input_dim=len(subword_to_idx), output_dim=embed_size),\n",
    "        nn.Embedding(input_dim=len(subword_to_idx), output_dim=embed_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ba4a7a",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "The training function is defined below. Because of the existence\n",
    "of padding, the calculation of the loss function is slightly different compared\n",
    "to the previous training functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a9a87c64",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "17"
    }
   },
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "def train(net, data_iter, lr, num_epochs, ctx=d2l.try_gpu()):\n",
    "    net.initialize(ctx=ctx, force_reinit=True)\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam',\n",
    "                            {'learning_rate': lr})\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[0, num_epochs])\n",
    "    for epoch in range(num_epochs):\n",
    "        timer = d2l.Timer()\n",
    "        metric = d2l.Accumulator(2)  # loss_sum, num_tokens\n",
    "        for i, batch in enumerate(data_iter):\n",
    "            center, context_negative, mask, label = [\n",
    "                data.as_in_ctx(ctx) for data in batch]\n",
    "            with autograd.record():\n",
    "                pred = skip_gram(center, context_negative, net[0], net[1], vocab['<pad>'])\n",
    "                l = (loss(pred.reshape(label.shape), label, mask)\n",
    "                     / mask.sum(axis=1) * mask.shape[1])\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            metric.add(l.sum(), l.size)\n",
    "            if (i+1) % 50 == 0:\n",
    "                animator.add(epoch+(i+1)/len(data_iter),\n",
    "                             (metric[0]/metric[1],))\n",
    "            npx.waitall()\n",
    "    print('loss %.3f, %d tokens/sec on %s ' % (\n",
    "        metric[0]/metric[1], metric[1]/timer.stop(), ctx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4c5583",
   "metadata": {},
   "source": [
    "Now, we can train a skip-gram model using negative sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90f441cf",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "20"
    }
   },
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "lr, num_epochs = 0.01, 5\n",
    "train(net, data_iter, lr, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8709440",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "def get_similar_tokens(query_token, k, embed, vocab, subword_to_idx):\n",
    "    W = embed.weight.data()\n",
    "    x = W[vocab[query_token]]\n",
    "    all_v = []\n",
    "    for token in vocab.idx_to_token:\n",
    "        subword = compute_subword(token)\n",
    "        w_v = W[[subword_to_idx[s] for s in subword]].sum(0)\n",
    "        all_v.append(np.expand_dims(w_v, 0))\n",
    "    all_v = np.concatenate(all_v, 0)\n",
    "    # Compute the cosine similarity. Add 1e-9 for numerical stability\n",
    "    cos = np.dot(all_v, x) / np.sqrt(np.sum(all_v * all_v, axis=1) * np.sum(x * x) + 1e-9)\n",
    "    topk = npx.topk(cos, k=k+1, ret_typ='indices').asnumpy().astype('int32')\n",
    "    for i in topk[1:]:  # Remove the input words\n",
    "        print('cosine sim=%.3f: %s' % (cos[i], (vocab.idx_to_token[i])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717d5c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "get_similar_tokens('chip', 3, net[0], vocab, subword_to_idx)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
