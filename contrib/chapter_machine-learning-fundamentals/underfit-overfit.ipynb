{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fc844e8",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext d2lbook.tab\n",
    "tab.interact_select(['mxnet', 'pytorch', 'tensorflow'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb164d32",
   "metadata": {},
   "source": [
    "# Underfitting and Overfitting \n",
    ":label:`sec_polynomial`\n",
    "\n",
    "In this section we test out some of the concepts that we saw previously. To keep matters simple, we use polynomial regression as our toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86e589d0",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "%%tab mxnet\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import gluon, np, npx\n",
    "from mxnet.gluon import nn\n",
    "import math\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "54afea49",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "%%tab pytorch\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54eb2a89",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "%%tab tensorflow\n",
    "from d2l import tensorflow as d2l\n",
    "import tensorflow as tf\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9855576c",
   "metadata": {},
   "source": [
    "### Generating the Dataset\n",
    "\n",
    "First we need data. Given $x$, we will [**use the following cubic polynomial to generate the labels**] on training and test data:\n",
    "\n",
    "(**$$y = 5 + 1.2x - 3.4\\frac{x^2}{2!} + 5.6 \\frac{x^3}{3!} + \\epsilon \\text{ where }\n",
    "\\epsilon \\sim \\mathcal{N}(0, 0.1^2).$$**)\n",
    "\n",
    "The noise term $\\epsilon$ obeys a normal distribution\n",
    "with a mean of 0 and a standard deviation of 0.1.\n",
    "For optimization, we typically want to avoid\n",
    "very large values of gradients or losses.\n",
    "This is why the *features*\n",
    "are rescaled from $x^i$ to $\\frac{x^i}{i!}$.\n",
    "It allows us to avoid very large values for large exponents $i$.\n",
    "We will synthesize 100 samples each for the training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35b33d24",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [],
   "source": [
    "%%tab all\n",
    "class Data(d2l.DataModule):\n",
    "    def __init__(self, num_train, num_val, num_inputs, batch_size):\n",
    "        self.save_hyperparameters()        \n",
    "        p, n = max(3, self.num_inputs), num_train + num_val\n",
    "        w = d2l.tensor([1.2, -3.4, 5.6] + [0]*(p-3))\n",
    "        if tab.selected('mxnet') or tab.selected('pytorch'):\n",
    "            x = d2l.randn(n, 1)\n",
    "            noise = d2l.randn(n, 1) * 0.1\n",
    "        if tab.selected('tensorflow'):\n",
    "            x = d2l.normal((n, 1))\n",
    "            noise = d2l.normal((n, 1)) * 0.1\n",
    "        X = d2l.concat([x ** (i+1) / math.gamma(i+2) for i in range(p)], 1)\n",
    "        self.y = d2l.matmul(X, d2l.reshape(w, (-1, 1))) + noise\n",
    "        self.X = X[:,:num_inputs]\n",
    "        \n",
    "    def get_dataloader(self, train):\n",
    "        i = slice(0, self.num_train) if train else slice(self.num_train, None)\n",
    "        return self.get_tensorloader([self.X, self.y], train, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b6fbf2",
   "metadata": {},
   "source": [
    "Again, monomials stored in `poly_features`\n",
    "are rescaled by the gamma function,\n",
    "where $\\Gamma(n)=(n-1)!$.\n",
    "[**Take a look at the first 2 samples**] from the generated dataset.\n",
    "The value 1 is technically a feature,\n",
    "namely the constant feature corresponding to the bias.\n",
    "\n",
    "### [**Third-Order Polynomial Function Fitting (Normal)**]\n",
    "\n",
    "We will begin by first using a third-order polynomial function, which is the same order as that of the data generation function.\n",
    "The results show that this model's training and test losses can be both effectively reduced.\n",
    "The learned model parameters are also close\n",
    "to the true values $w = [1.2, -3.4, 5.6], b=5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a25b471",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [],
   "source": [
    "%%tab all\n",
    "def train(p):\n",
    "    if tab.selected('mxnet') or tab.selected('tensorflow'):\n",
    "        model = d2l.LinearRegression(lr=0.01)\n",
    "    if tab.selected('pytorch'):\n",
    "        model = d2l.LinearRegression(p, lr=0.01)\n",
    "    model.board.ylim = [1, 1e2]\n",
    "    data = Data(200, 200, p, 20)\n",
    "    trainer = d2l.Trainer(max_epochs=10)\n",
    "    trainer.fit(model, data)\n",
    "    print(model.get_w_b())\n",
    "    \n",
    "train(p=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af0e0cb",
   "metadata": {},
   "source": [
    "### [**Linear Function Fitting (Underfitting)**]\n",
    "\n",
    "Let's take another look at linear function fitting.\n",
    "After the decline in early epochs,\n",
    "it becomes difficult to further decrease\n",
    "this model's training loss.\n",
    "After the last epoch iteration has been completed,\n",
    "the training loss is still high.\n",
    "When used to fit nonlinear patterns\n",
    "(like the third-order polynomial function here)\n",
    "linear models are liable to underfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56094856",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "%%tab all\n",
    "train(p=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad3ecba",
   "metadata": {},
   "source": [
    "### [**Higher-Order Polynomial Function Fitting  (Overfitting)**]\n",
    "\n",
    "Now let's try to train the model\n",
    "using a polynomial of too high degree.\n",
    "Here, there is insufficient data to learn that\n",
    "the higher-degree coefficients should have values close to zero.\n",
    "As a result, our overly-complex model\n",
    "is so susceptible that it is being influenced\n",
    "by noise in the training data.\n",
    "Though the training loss can be effectively reduced,\n",
    "the test loss is still much higher.\n",
    "It shows that\n",
    "the complex model overfits the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf39b1f4",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "%%tab all\n",
    "train(p=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f9e59c",
   "metadata": {},
   "source": [
    "In the subsequent sections, we will continue\n",
    "to discuss overfitting problems\n",
    "and methods for dealing with them,\n",
    "such as weight decay and dropout.\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "* Since the generalization error cannot be estimated based on the training error, simply minimizing the training error will not necessarily mean a reduction in the generalization error. Machine learning models need to be careful to safeguard against overfitting so as to minimize the generalization error.\n",
    "* A validation set can be used for model selection, provided that it is not used too liberally.\n",
    "* Underfitting means that a model is not able to reduce the training error. When training error is much lower than validation error, there is overfitting.\n",
    "* We should choose an appropriately complex model and avoid using insufficient training samples.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Can you solve the polynomial regression problem exactly? Hint: use linear algebra.\n",
    "1. Consider model selection for polynomials:\n",
    "    1. Plot the training loss vs. model complexity (degree of the polynomial). What do you observe? What degree of polynomial do you need to reduce the training loss to 0?\n",
    "    1. Plot the test loss in this case.\n",
    "    1. Generate the same plot as a function of the amount of data.\n",
    "1. What happens if you drop the normalization ($1/i!$) of the polynomial features $x^i$? Can you fix this in some other way?\n",
    "1. Can you ever expect to see zero generalization error?\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/96)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/97)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "[Discussions](https://discuss.d2l.ai/t/234)\n",
    ":end_tab:"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
