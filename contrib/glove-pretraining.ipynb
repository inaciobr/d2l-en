{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ca4b9ea",
   "metadata": {},
   "source": [
    "# Pretraining GloVe\n",
    ":label:`sec_GloVe_gluon`\n",
    "\n",
    "In this section, we will train a\n",
    "GloVe model defined in\n",
    ":numref:`sec_glove`.\n",
    "\n",
    "First, import the\n",
    "packages and\n",
    "modules required for the experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6b2416a",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "from collections import defaultdict\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import autograd, gluon, init, np, npx, cpu\n",
    "from mxnet.gluon import nn\n",
    "import random\n",
    "\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0a60fb",
   "metadata": {},
   "source": [
    "## Preprocessing Dataset\n",
    "We will train GloVe model on PTB dataset.\n",
    "\n",
    "First, we\n",
    "read the PTB dataset, build a vocabulary with words and map each token into an\n",
    "index to construct the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67f71c80",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "sentences = d2l.read_ptb()\n",
    "vocab = d2l.Vocab(sentences, min_freq=10)\n",
    "corpus = [vocab[line] for line in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05475697",
   "metadata": {},
   "source": [
    "### Construct Cooccurrence Counts\n",
    "Let the word-word cooccurrence counts be\n",
    "denoted by $X$, whose entries $x_{ij}$ tabulate the number of times word $j$\n",
    "occurs in the context of word $i$.\n",
    "\n",
    "Next, we define following function to\n",
    "extracts all the central target words and their context words. It use a\n",
    "decreasing weighting function, so that word pairs that are $d$ words apart\n",
    "contribute $1/d$ to the total count. This is one way to account for the fact\n",
    "that very distant word pairs are expected to contain less relevant information\n",
    "about the words’ relationship to one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f3b16e3",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "def get_coocurrence_counts(corpus, window_size):\n",
    "    centers, contexts = [], []\n",
    "    cooccurence_counts = defaultdict(float)\n",
    "    for line in corpus:\n",
    "        # Each sentence needs at least 2 words to form a\n",
    "        # \"central target word - context word\" pair\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        centers += line\n",
    "        for i in range(len(line)):  # Context window centered at i\n",
    "            left_indices = list(range(max(0, i - window_size), i))\n",
    "            right_indices = list(range(i + 1,\n",
    "                                       min(len(line), i + 1 + window_size)))\n",
    "            left_context = [line[idx] for idx in left_indices]\n",
    "            right_context = [line[idx] for idx in right_indices]\n",
    "            for distance, word in enumerate(left_context[::-1]):\n",
    "                cooccurence_counts[line[i], word] += 1 / (distance + 1)\n",
    "            for distance, word in enumerate(right_context):\n",
    "                cooccurence_counts[line[i], word] += 1 / (distance + 1)\n",
    "    cooccurence_counts = [(word[0], word[1], count)\n",
    "                          for word, count in cooccurence_counts.items()]\n",
    "    return cooccurence_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adab8b8d",
   "metadata": {},
   "source": [
    "We create an artificial dataset containing two sentences of 5 and 2 words,\n",
    "respectively. Assume the maximum context window is 4. Then, we print the\n",
    "cooccurrence counts of all the central target words and context words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fc2657a",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "tiny_dataset = [list(range(5)), list(range(5, 7))]\n",
    "print('dataset', tiny_dataset)\n",
    "for center, context, coocurrence in get_coocurrence_counts(tiny_dataset, 4):\n",
    "        print('center: %s, context: %s, coocurrence: %.2f' %\n",
    "          (center, context, coocurrence))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629e5c41",
   "metadata": {},
   "source": [
    "We set the maximum context window size to 5. The following extracts all the\n",
    "central target words and their context words in the dataset, and calculate their\n",
    "cooccurrence counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58a84438",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "coocurrence_matrix = get_coocurrence_counts(corpus, 5)\n",
    "'# center-context pairs: %d' % len(coocurrence_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1904319e",
   "metadata": {},
   "source": [
    "### Putting All Things Together\n",
    "\n",
    "Last, We define the load_data_ptb_glove\n",
    "function that read the PTB dataset and return the data loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dce201a0",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "16"
    }
   },
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "def load_data_ptb_glove(batch_size, window_size):\n",
    "    num_workers = d2l.get_dataloader_workers()\n",
    "    sentences = d2l.read_ptb()\n",
    "    vocab = d2l.Vocab(sentences, min_freq=5)\n",
    "    corpus = [vocab[line] for line in sentences]\n",
    "    coocurrence_matrix = get_coocurrence_counts(corpus, window_size)\n",
    "    dataset = gluon.data.ArrayDataset(coocurrence_matrix)\n",
    "    data_iter = gluon.data.DataLoader(dataset, batch_size, shuffle=True,\n",
    "                                      num_workers=num_workers)\n",
    "    return data_iter, vocab\n",
    "\n",
    "batch_size, window_size = 1024, 10\n",
    "data_iter, vocab = load_data_ptb_glove(batch_size, window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0667463f",
   "metadata": {},
   "source": [
    "Let’s print the first minibatch of the data iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a20ab465",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "17"
    }
   },
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "names = ['center', 'context', 'Cooccurence']\n",
    "for batch in data_iter:\n",
    "    for name, data in zip(names, batch):\n",
    "        print(name, 'shape:', data.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49708aad",
   "metadata": {},
   "source": [
    "## The GloVe Model\n",
    "\n",
    "In section 15.1, we introduced the goal of GloVe is to\n",
    "minimize the loss function.\n",
    "\n",
    "$$\\sum_{i\\in\\mathcal{V}} \\sum_{j\\in\\mathcal{V}}\n",
    "h(x_{ij}) \\left(\\mathbf{u}_j^\\top \\mathbf{v}_i + b_i + c_j -\n",
    "\\log\\,x_{ij}\\right)^2.$$\n",
    "\n",
    "We will implement the GloVe model by implementing each\n",
    "part of the loss function.\n",
    "\n",
    "### Weight function\n",
    "\n",
    "GloVe introduced a weighting\n",
    "function $h(x_{ij})$ into the loss function.\n",
    "\n",
    "$$h(x_{ij})=\\begin{cases}\n",
    "(\\frac{x}{x_{max}})^\\alpha & x_{ij}<x_{max}\\\\\n",
    "1 & otherwise\n",
    "\\end{cases}$$\n",
    "\n",
    "\n",
    "We\n",
    "implement the weighting function $h(x_{ij})$. Since $x_{ij}<x_{max}$is\n",
    "equivalent to $(\\frac{x}{x_{max}})^\\alpha < 1$, we can give the following\n",
    "implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5742dc35",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "18"
    }
   },
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "def compute_weight(x, x_max = 30, alpha = 0.75):\n",
    "    w = (x / x_max) ** alpha\n",
    "    return np.minimum(w, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da91f0b",
   "metadata": {},
   "source": [
    "The following prints the weight of the cooccurrence counts of all the central\n",
    "target words and context words when the $x_{max}$ set to 2 and $\\alpha$ to 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "62fcf58c",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "19"
    }
   },
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "for center, context, coocurrence in get_coocurrence_counts(tiny_dataset, 4)[:5]:\n",
    "    print('center: %s, context: %s, coocurrence: %.2f, weight: %.2f' %\n",
    "          (center, context, coocurrence, compute_weight(coocurrence, x_max = 2, alpha = 0.75)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b3c433",
   "metadata": {},
   "source": [
    "### Bias Term\n",
    "\n",
    "GloVe has two scalar model parameters for each word $w_i$ : the\n",
    "bias terms $b_i$ (for central target words) and  $c_i$ (for context words).\n",
    "Bias term can be realized by embedding layer. The weight of the embedding layer\n",
    "is a matrix whose number of rows is the dictionary size (input_dim) and whose\n",
    "number of columns is one.\n",
    "\n",
    "We set the dictionary size to  20."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f236c2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "embed_bias = nn.Embedding(input_dim=20, output_dim=1)\n",
    "embed_bias.initialize()\n",
    "embed_bias.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0dc601",
   "metadata": {},
   "source": [
    "The input of the embedding layer is the index of the word. When we enter the\n",
    "index $i$ of a word, the embedding layer returns the $i$ th row of the weight\n",
    "value as its bias term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e2519c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "x = np.array([1, 2, 3])\n",
    "embed_bias(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4135d49c",
   "metadata": {},
   "source": [
    "### GloVe Model Forward Calculation\n",
    "\n",
    "In forward calculation, the input of\n",
    "the\n",
    "GloVe model contains the central\n",
    "target word index `center` and the context word\n",
    "index\n",
    "`context`. In which,\n",
    "the `center` variable has the shape (batch\n",
    "size, 1),\n",
    "while the\n",
    "`context` variable has the shape (batch size,\n",
    "1). These\n",
    "two variables\n",
    "are first transformed from word indexes to word\n",
    "vectors by the\n",
    "word embedding\n",
    "layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd166725",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "20"
    }
   },
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "def GloVe(center, context, coocurrence, embed_v, embed_u,\n",
    "          bias_v, bias_u, x_max, alpha):\n",
    "    # Shape of v: (batch_size, embed_size)\n",
    "    v = embed_v(center)\n",
    "    # Shape of u: (batch_size, embed_size)\n",
    "    u = embed_u(context)\n",
    "    # Shape of b: (batch_size, )\n",
    "    b = bias_v(center).squeeze()\n",
    "    # Shape of c: (batch_size, )\n",
    "    c = bias_u(context).squeeze()\n",
    "    # Shape of embed_products: (batch_size,)\n",
    "    embed_products = npx.batch_dot(np.expand_dims(v, 1),\n",
    "                                   np.expand_dims(u, 2)).squeeze()\n",
    "    # Shape of distance_expr: (batch_size,)\n",
    "    distance_expr = np.power(embed_products + b +\n",
    "                     c - np.log(coocurrence), 2)\n",
    "    # Shape of weight: (batch_size,)\n",
    "    weight = compute_weight(coocurrence)\n",
    "    return weight * distance_expr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f58278f",
   "metadata": {},
   "source": [
    "Verify that the output shape should be (batch size, )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5bc1a18",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "21"
    }
   },
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "embed_word = nn.Embedding(input_dim=20, output_dim=4)\n",
    "embed_word.initialize()\n",
    "GloVe(np.ones((2)), np.ones((2)), np.ones((2)), embed_word, embed_word,\n",
    "      embed_bias, embed_bias, x_max = 2, alpha = 0.75).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b52337c",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Before training the word embedding model, we need to define the\n",
    "loss function of\n",
    "the model.\n",
    "\n",
    "### Initializing Model Parameters\n",
    "We construct the\n",
    "embedding layers of words and\n",
    "additional biases,\n",
    "and set the hyperparameter word\n",
    "vector\n",
    "dimension `embed_size`\n",
    "to\n",
    "100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf5290f5",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "22"
    }
   },
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "embed_size = 100\n",
    "net = nn.Sequential()\n",
    "net.add(nn.Embedding(input_dim=len(vocab), output_dim=embed_size),\n",
    "        nn.Embedding(input_dim=len(vocab), output_dim=embed_size),\n",
    "        nn.Embedding(input_dim=len(vocab), output_dim=1),\n",
    "        nn.Embedding(input_dim=len(vocab), output_dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42765ad4",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "The training function is defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0d71e18",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "23"
    }
   },
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "def train(net, data_iter, lr, num_epochs, x_max, alpha, ctx=d2l.try_gpu()):\n",
    "    net.initialize(ctx=ctx, force_reinit=True)\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'AdaGrad',\n",
    "                            {'learning_rate': lr})\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[0, num_epochs])\n",
    "    for epoch in range(num_epochs):\n",
    "        timer = d2l.Timer()\n",
    "        metric = d2l.Accumulator(2)  # loss_sum, num_tokens\n",
    "        for i, batch in enumerate(data_iter):\n",
    "            center, context, coocurrence = [\n",
    "                data.as_in_context(ctx) for data in batch]\n",
    "            with autograd.record():\n",
    "                l = GloVe(center, context, coocurrence.astype('float32'),\n",
    "                          net[0], net[1], net[2], net[3], x_max, alpha)\n",
    "            l.backward()\n",
    "            trainer.step(batch_size)\n",
    "            metric.add(l.sum(), l.size)\n",
    "            if (i+1) % 50 == 0:\n",
    "                animator.add(epoch+(i+1)/len(data_iter),\n",
    "                             (metric[0]/metric[1],))\n",
    "    print('loss %.3f, %d tokens/sec on %s ' % (\n",
    "        metric[0]/metric[1], metric[1]/timer.stop(), ctx))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae96424f",
   "metadata": {},
   "source": [
    "Now, we can train a GloVe model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6f1c5d6",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "lr, num_epochs = 0.1, 5\n",
    "x_max, alpha = 100, 0.75\n",
    "train(net, data_iter, lr, num_epochs, x_max, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bcc9f56",
   "metadata": {},
   "source": [
    "## Applying the GloVe Model\n",
    "\n",
    "GloVe model generates two sets of word vectors,\n",
    "`embed_v` and `embed_u` . `embed_v` and `embed_u` are\n",
    "equivalent and differ only\n",
    "as a result of their random initializations; the two sets of vectors should\n",
    "perform equivalently.Generally, we choose to use the sum `embed_v`+`embed_u` as\n",
    "our word vectors.\n",
    "\n",
    "\n",
    "After training the GloVe model, we can still represent\n",
    "similarity in meaning between words based on the cosine similarity of two word\n",
    "vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "032235e4",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "13"
    }
   },
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "def get_similar_tokens(query_token, k, embed_v, embed_u):\n",
    "    W = embed_v.weight.data() + embed_u.weight.data()\n",
    "    x = W[vocab[query_token]]\n",
    "    # Compute the cosine similarity. Add 1e-9 for numerical stability\n",
    "    cos = np.dot(W, x) / np.sqrt(np.sum(W * W, axis=1) * np.sum(x * x) + 1e-9)\n",
    "    topk = npx.topk(cos, k=k+1, ret_typ='indices').asnumpy().astype('int32')\n",
    "    for i in topk[1:]:  # Remove the input words\n",
    "        print('cosine sim=%.3f: %s' % (cos[i], (vocab.idx_to_token[i])))\n",
    "\n",
    "get_similar_tokens('chip', 3, net[0], net[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c6966c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* We can pretrain a GloVe model.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "\n",
    "\n",
    "## Discussions"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
