{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b915034",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext d2lbook.tab\n",
    "tab.interact_select([\"pytorch\"])\n",
    "#required_libs(\"syne-tune[gpsearchers]==0.3.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce449307",
   "metadata": {},
   "source": [
    "# Asynchronous Successive Halving\n",
    "\n",
    ":label:`sec_sh_async`\n",
    "\n",
    "As we have seen in :numref:`sec_rs_async`, we can accelerate HPO by\n",
    "distributing the evaluation of hyperparameter configurations across either\n",
    "multiple instances or multiples CPUs / GPUs on a single instance. However,\n",
    "compared to random search, it is not straightforward to run\n",
    "successive halving (SH) asynchronously in a distributed setting. Before we can\n",
    "decide which configuration to run next, we first have to collect all\n",
    "observations at the current rung level. This requires to\n",
    "synchronize workers at each rung level. For example, for the lowest rung level\n",
    "$r_{\\mathrm{min}}$, we first have to evaluate all $N = \\eta^K$ configurations, before we\n",
    "can promote the $\\frac{1}{\\eta}$ of them to the next rung level.\n",
    "\n",
    "In any distributed system, synchronization typically implies idle time for workers.\n",
    "First, we often observe high variations in training time across hyperparameter\n",
    "configurations. For example, assuming the number of filters per layer is a\n",
    "hyperparameter, then networks with less filters finish training faster than\n",
    "networks with more filters, which implies idle worker time due to stragglers.\n",
    "Moreover, the number of slots in a rung level is not always a multiple of the number\n",
    "of workers, in which case some workers may even sit idle for a full batch.\n",
    "\n",
    "Figure :numref:`synchronous_sh` shows the scheduling of synchronous SH with $\\eta=2$\n",
    "for four different trials with two workers. We start with evaluating Trial-0 and\n",
    "Trial-1 for one epoch and immediately continue with the next two trials once they\n",
    "are finished. We first have to wait until Trial-2 finishes, which takes\n",
    "substantially more time than the other trials, before we can promote the best two\n",
    "trials, i.e., Trial-0 and Trial-3 to the next rung level. This causes idle time for\n",
    "Worker-1. Then, we continue with Rung 1. Also, here Trial-3 takes longer than Trial-0,\n",
    "which leads to an additional ideling time of Worker-0. Once, we reach Rung-2, only\n",
    "the best trial, Trial-0, remains which occupies only one worker. To avoid that\n",
    "Worker-1 idles during that time, most implementaitons of SH continue already with\n",
    "the next round, and start evaluating new trials (e.g Trial-4) on the first rung.\n",
    "\n",
    "![Synchronous successive halving with two workers.](../img/sync_sh.svg)\n",
    ":label:`synchronous_sh`\n",
    "\n",
    "Asynchronous successive halving (ASHA) :cite:`li-arxiv18` adapts SH to the asynchronous\n",
    "parallel scenario. The main idea of ASHA is to promote configurations to the next rung\n",
    "level as soon as we collected at least $\\eta$ observations on the current rung level.\n",
    "This decision rule may lead to suboptimal promotions: configurations can be promoted to the\n",
    "next rung level, which in hindsight do not compare favourably against most others\n",
    "at the same rung level. On the other hand, we get rid of all synchronization points\n",
    "this way. In practice, such suboptimal initial promotions have only a modest impact on\n",
    "performance, not only because the ranking of hyperparameter configurations is often\n",
    "fairly consistent across rung levels, but also because rungs grow over time and\n",
    "reflect the distribution of metric values at this level better and better. If a\n",
    "worker is free, but no configuration can be promoted, we start a new configuration\n",
    "with $r = r_{\\mathrm{min}}$, i.e the first rung level.\n",
    "\n",
    ":numref:`asha` shows the scheduling of the same configurations for ASHA. Once Trial-1\n",
    "finishes, we collect the results of two trials (i.e Trial-0 and Trial-1) and\n",
    "immediately promote the better of them (Trial-0) to the next rung level. After Trial-0\n",
    "finishes on rung 1, there are too few trials there in order to support a further\n",
    "promotion. Hence, we continue with rung 0 and evaluate Trial-3. Once Trial-3 finishes,\n",
    "Trial-2 is still pending. At this point we have 3 trials evaluated on rung 0 and one\n",
    "trial evaluated already on rung 1. Since Trial-3 performs worse than Trial-0 at rung 0,\n",
    "and $\\eta=2$, we cannot promote any new trial yet, and Worker-1 starts Trial-4 from\n",
    "scratch instead. However, once Trial-2 finishes and\n",
    "scores worse than Trial-3, the latter is promoted towards rung 1. Afterwards, we\n",
    "collected 2 evaluations on rung 1, which means we can now promote Trial-0 towards\n",
    "rung 2. At the same time, Worker-1 continues with evaluating new trials (i.e.,\n",
    "Trial-5) on rung 0.\n",
    "\n",
    "\n",
    "![Asynchronous successive halving (ASHA) with two workers.](../img/asha.svg)\n",
    ":label:`asha`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0eb7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "import matplotlib.pyplot as plt\n",
    "from syne_tune.config_space import loguniform, randint\n",
    "from syne_tune.backend.python_backend import PythonBackend\n",
    "from syne_tune.optimizer.baselines import ASHA\n",
    "from syne_tune import Tuner, StoppingCriterion\n",
    "from syne_tune.experiments import load_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae44d58",
   "metadata": {},
   "source": [
    "## Objective Function\n",
    "\n",
    "We will use *Syne Tune* with the same objective function as in\n",
    ":numref:`sec_rs_async`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "45bfd9ac",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "54"
    }
   },
   "outputs": [],
   "source": [
    "def hpo_objective_lenet_synetune(learning_rate, batch_size, max_epochs):\n",
    "    from d2l import torch as d2l\n",
    "    from syne_tune import Reporter\n",
    "\n",
    "    model = d2l.LeNet(lr=learning_rate, num_classes=10)\n",
    "    trainer = d2l.HPOTrainer(max_epochs=1, num_gpus=1)\n",
    "    data = d2l.FashionMNIST(batch_size=batch_size)\n",
    "    model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
    "    report = Reporter()\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        if epoch == 1:\n",
    "            # Initialize the state of Trainer\n",
    "            trainer.fit(model=model, data=data)\n",
    "        else:\n",
    "            trainer.fit_epoch()\n",
    "        validation_error = d2l.numpy(trainer.validation_error().cpu())\n",
    "        report(epoch=epoch, validation_error=float(validation_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bb8b62",
   "metadata": {},
   "source": [
    "We will also use the same configuration space as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "94bedc9d",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "55"
    }
   },
   "outputs": [],
   "source": [
    "min_number_of_epochs = 2\n",
    "max_number_of_epochs = 10\n",
    "eta = 2\n",
    "\n",
    "config_space = {\n",
    "    \"learning_rate\": loguniform(1e-2, 1),\n",
    "    \"batch_size\": randint(32, 256),\n",
    "    \"max_epochs\": max_number_of_epochs,\n",
    "}\n",
    "initial_config = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"batch_size\": 128,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd48d574",
   "metadata": {},
   "source": [
    "## Asynchronous Scheduler\n",
    "\n",
    "First, we define the number of workers that evaluate trials concurrently. We\n",
    "also need to specify how long we want to run random search, by defining an\n",
    "upper limit on the total wall-clock time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "13f0995c",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "56"
    }
   },
   "outputs": [],
   "source": [
    "n_workers = 2  # Needs to be <= the number of available GPUs\n",
    "max_wallclock_time = 12 * 60  # 12 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8fdee2",
   "metadata": {},
   "source": [
    "The code for running ASHA is a simple variation of what we did for asynchronous\n",
    "random search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "06b5fe93",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "56"
    }
   },
   "outputs": [],
   "source": [
    "mode = \"min\"\n",
    "metric = \"validation_error\"\n",
    "resource_attr = \"epoch\"\n",
    "\n",
    "scheduler = ASHA(\n",
    "    config_space,\n",
    "    metric=metric,\n",
    "    mode=mode,\n",
    "    points_to_evaluate=[initial_config],\n",
    "    max_resource_attr=\"max_epochs\",\n",
    "    resource_attr=resource_attr,\n",
    "    grace_period=min_number_of_epochs,\n",
    "    reduction_factor=eta,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82355309",
   "metadata": {},
   "source": [
    "Here, `metric` and `resource_attr` specify the key names used with the `report`\n",
    "callback, and `max_resource_attr` denotes which input to the objective function\n",
    "corresponds to $r_{\\mathrm{max}}$. Moreover, `grace_period` provides $r_{\\mathrm{min}}$, and\n",
    "`reduction_factor` is $\\eta$. We can run Syne Tune as before (this will\n",
    "take about 12 minutes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4be66cfb",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "57"
    }
   },
   "outputs": [],
   "source": [
    "trial_backend = PythonBackend(\n",
    "    tune_function=hpo_objective_lenet_synetune,\n",
    "    config_space=config_space,\n",
    ")\n",
    "\n",
    "stop_criterion = StoppingCriterion(max_wallclock_time=max_wallclock_time)\n",
    "tuner = Tuner(\n",
    "    trial_backend=trial_backend,\n",
    "    scheduler=scheduler,\n",
    "    stop_criterion=stop_criterion,\n",
    "    n_workers=n_workers,\n",
    "    print_update_interval=int(max_wallclock_time * 0.6),\n",
    ")\n",
    "tuner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a038c3f",
   "metadata": {},
   "source": [
    "Note that we are running a variant of ASHA where underperforming trials are\n",
    "stopped early. This is different to our implementation in\n",
    ":numref:`sec_mf_hpo_sh`, where each training job is started with a fixed\n",
    "`max_epochs`. In the latter case, a well-performing trial which reaches the\n",
    "full 10 epochs, first needs to train 1, then 2, then 4, then 8 epochs, each\n",
    "time starting from scratch. This type of pause-and-resume scheduling can be\n",
    "implemented efficiently by checkpointing the training state after each epoch,\n",
    "but we avoid this extra complexity here. After the experiment has finished,\n",
    "we can retrieve and plot results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "85055a34",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "59"
    }
   },
   "outputs": [],
   "source": [
    "d2l.set_figsize()\n",
    "e = load_experiment(tuner.name)\n",
    "e.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e569c0",
   "metadata": {},
   "source": [
    "## Visualize the Optimization Process\n",
    "\n",
    "Once more, we visualize the learning curves of every trial (each color in the plot represents a trial). Compare this to\n",
    "asynchronous random search in :numref:`sec_rs_async`. As we have seen for\n",
    "successive halving in :numref:`sec_mf_hpo`, most of the trials are stopped\n",
    "at 1 or 2 epochs ($r_{\\mathrm{min}}$ or $\\eta * r_{\\mathrm{min}}$). However, trials do not stop\n",
    "at the same point, because they require different amount of time per epoch. If\n",
    "we ran standard successive halving instead of ASHA, we would need to synchronize\n",
    "our workers, before we can promote configurations to the next rung level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d25d2854",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "60"
    }
   },
   "outputs": [],
   "source": [
    "d2l.set_figsize([6, 2.5])\n",
    "results = e.results\n",
    "for trial_id in results.trial_id.unique():\n",
    "    df = results[results[\"trial_id\"] == trial_id]\n",
    "    d2l.plt.plot(\n",
    "        df[\"st_tuner_time\"],\n",
    "        df[\"validation_error\"],\n",
    "        marker=\"o\"\n",
    "    )\n",
    "d2l.plt.xlabel(\"wall-clock time\")\n",
    "d2l.plt.ylabel(\"objective function\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6194afa5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Compared to random search, successive halving is not quite as trivial to run in\n",
    "an asynchronous distributed setting. To avoid synchronisation points, we promote\n",
    "configurations as quickly as possible to the next rung level, even if this means\n",
    "promoting some wrong ones. In practice, this usually does not hurt much, and the\n",
    "gains of asynchronous versus synchronous scheduling are usually much higher\n",
    "than the loss of the suboptimal decision making.\n",
    "\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/12101)\n",
    ":end_tab:"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
