{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddd52a2d",
   "metadata": {},
   "source": [
    "# Neural Style Transfer\n",
    "\n",
    "If you are a photography enthusiast,\n",
    "you may be familiar with the filter.\n",
    "It can change the color style of photos\n",
    "so that landscape photos become sharper\n",
    "or portrait photos have whitened skins.\n",
    "However,\n",
    "one filter usually only changes\n",
    "one aspect of the photo.\n",
    "To apply an ideal style\n",
    "to a photo,\n",
    "you probably need to\n",
    "try many different filter combinations.\n",
    "This process is\n",
    "as complex as tuning the hyperparameters of a model.\n",
    "\n",
    "\n",
    "\n",
    "In this section, we will\n",
    "leverage layerwise representations of a CNN\n",
    "to automatically apply the style of one image\n",
    "to another image, i.e., *style transfer* :cite:`Gatys.Ecker.Bethge.2016`.\n",
    "This task needs two input images:\n",
    "one is the *content image* and\n",
    "the other is the *style image*.\n",
    "We will use neural networks\n",
    "to modify the content image\n",
    "to make it close to the style image in style.\n",
    "For example,\n",
    "the content image in :numref:`fig_style_transfer` is a landscape photo taken by us\n",
    "in Mount Rainier National Park in the suburbs of Seattle, while the style image is an oil painting\n",
    "with the theme of autumn oak trees.\n",
    "In the output synthesized image,\n",
    "the oil brush strokes of the style image\n",
    "are applied, leading to more vivid colors,\n",
    "while preserving the main shape of the objects\n",
    "in the content image.\n",
    "\n",
    "![Given content and style images, style transfer outputs a synthesized image.](../img/style-transfer.svg)\n",
    ":label:`fig_style_transfer`\n",
    "\n",
    "## Method\n",
    "\n",
    ":numref:`fig_style_transfer_model` illustrates\n",
    "the CNN-based style transfer method with a simplified example.\n",
    "First, we initialize the synthesized image,\n",
    "for example, into the content image.\n",
    "This synthesized image is the only variable that needs to be updated during the style transfer process,\n",
    "i.e., the model parameters to be updated during training.\n",
    "Then we choose a pretrained CNN\n",
    "to extract image features and freeze its\n",
    "model parameters during training.\n",
    "This deep CNN uses multiple layers\n",
    "to extract\n",
    "hierarchical features for images.\n",
    "We can choose the output of some of these layers as content features or style features.\n",
    "Take :numref:`fig_style_transfer_model` as an example.\n",
    "The pretrained neural network here has 3 convolutional layers,\n",
    "where the second layer outputs the content features,\n",
    "and the first and third layers output the style features.\n",
    "\n",
    "![CNN-based style transfer process. Solid lines show the direction of forward propagation and dotted lines show backward propagation. ](../img/neural-style.svg)\n",
    ":label:`fig_style_transfer_model`\n",
    "\n",
    "Next, we calculate the loss function of style transfer through forward propagation (direction of solid arrows), and update the model parameters (the synthesized image for output) through backpropagation (direction of dashed arrows).\n",
    "The loss function commonly used in style transfer consists of three parts:\n",
    "(i) *content loss* makes the synthesized image and the content image close in content features;\n",
    "(ii) *style loss* makes the synthesized image and style image close in style features;\n",
    "and (iii) *total variation loss* helps to reduce the noise in the synthesized image.\n",
    "Finally, when the model training is over, we output the model parameters of the style transfer to generate\n",
    "the final synthesized image.\n",
    "\n",
    "\n",
    "\n",
    "In the following,\n",
    "we will explain the technical details of style transfer via a concrete experiment.\n",
    "\n",
    "\n",
    "## [**Reading the Content and Style Images**]\n",
    "\n",
    "First, we read the content and style images.\n",
    "From their printed coordinate axes,\n",
    "we can tell that these images have different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb95938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "%matplotlib inline\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import autograd, gluon, image, init, np, npx\n",
    "from mxnet.gluon import nn\n",
    "\n",
    "npx.set_np()\n",
    "\n",
    "d2l.set_figsize()\n",
    "content_img = image.imread('../img/rainier.jpg')\n",
    "d2l.plt.imshow(content_img.asnumpy());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17652bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "%matplotlib inline\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "\n",
    "d2l.set_figsize()\n",
    "content_img = d2l.Image.open('../img/rainier.jpg')\n",
    "d2l.plt.imshow(content_img);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8851cabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "style_img = image.imread('../img/autumn-oak.jpg')\n",
    "d2l.plt.imshow(style_img.asnumpy());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac58a57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "style_img = d2l.Image.open('../img/autumn-oak.jpg')\n",
    "d2l.plt.imshow(style_img);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9039f4ad",
   "metadata": {},
   "source": [
    "## [**Preprocessing and Postprocessing**]\n",
    "\n",
    "Below, we define two functions for preprocessing and postprocessing images.\n",
    "The `preprocess` function standardizes\n",
    "each of the three RGB channels of the input image and transforms the results into the CNN input format.\n",
    "The `postprocess` function restores the pixel values in the output image to their original values before standardization.\n",
    "Since the image printing function requires that each pixel has a floating point value from 0 to 1,\n",
    "we replace any value smaller than 0 or greater than 1 with 0 or 1, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fd90b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "rgb_mean = np.array([0.485, 0.456, 0.406])\n",
    "rgb_std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "def preprocess(img, image_shape):\n",
    "    img = image.imresize(img, *image_shape)\n",
    "    img = (img.astype('float32') / 255 - rgb_mean) / rgb_std\n",
    "    return np.expand_dims(img.transpose(2, 0, 1), axis=0)\n",
    "\n",
    "def postprocess(img):\n",
    "    img = img[0].as_in_ctx(rgb_std.ctx)\n",
    "    return (img.transpose(1, 2, 0) * rgb_std + rgb_mean).clip(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499b761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "rgb_mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "rgb_std = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "def preprocess(img, image_shape):\n",
    "    transforms = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Resize(image_shape),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=rgb_mean, std=rgb_std)])\n",
    "    return transforms(img).unsqueeze(0)\n",
    "\n",
    "def postprocess(img):\n",
    "    img = img[0].to(rgb_std.device)\n",
    "    img = torch.clamp(img.permute(1, 2, 0) * rgb_std + rgb_mean, 0, 1)\n",
    "    return torchvision.transforms.ToPILImage()(img.permute(2, 0, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82142fd6",
   "metadata": {},
   "source": [
    "## [**Extracting Features**]\n",
    "\n",
    "We use the VGG-19 model pretrained on the ImageNet dataset to extract image features :cite:`Gatys.Ecker.Bethge.2016`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cf3933",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "pretrained_net = gluon.model_zoo.vision.vgg19(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a258ce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "pretrained_net = torchvision.models.vgg19(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85eb65fd",
   "metadata": {},
   "source": [
    "In order to extract the content features and style features of the image, we can select the output of certain layers in the VGG network.\n",
    "Generally speaking, the closer to the input layer, the easier to extract details of the image, and vice versa, the easier to extract the global information of the image. In order to avoid excessively\n",
    "retaining the details of the content image in the synthesized image,\n",
    "we choose a VGG layer that is closer to the output as the *content layer* to output the content features of the image.\n",
    "We also select the output of different VGG layers for extracting local and global style features.\n",
    "These layers are also called *style layers*.\n",
    "As mentioned in :numref:`sec_vgg`,\n",
    "the VGG network uses 5 convolutional blocks.\n",
    "In the experiment, we choose the last convolutional layer of the fourth convolutional block as the content layer, and the first convolutional layer of each convolutional block as the style layer.\n",
    "The indices of these layers can be obtained by printing the `pretrained_net` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18cc8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "style_layers, content_layers = [0, 5, 10, 19, 28], [25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6861a4bf",
   "metadata": {},
   "source": [
    "When extracting features using VGG layers,\n",
    "we only need to use all those\n",
    "from the input layer to the content layer or style layer that is closest to the output layer.\n",
    "Let's construct a new network instance `net`, which only retains all the VGG layers to be\n",
    "used for feature extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbe9ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "net = nn.Sequential()\n",
    "for i in range(max(content_layers + style_layers) + 1):\n",
    "    net.add(pretrained_net.features[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6831f4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "net = nn.Sequential(*[pretrained_net.features[i] for i in\n",
    "                      range(max(content_layers + style_layers) + 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b832366",
   "metadata": {},
   "source": [
    "Given the input `X`, if we simply invoke\n",
    "the forward propagation `net(X)`, we can only get the output of the last layer.\n",
    "Since we also need the outputs of intermediate layers,\n",
    "we need to perform layer-by-layer computation and keep\n",
    "the content and style layer outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6803c549",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "def extract_features(X, content_layers, style_layers):\n",
    "    contents = []\n",
    "    styles = []\n",
    "    for i in range(len(net)):\n",
    "        X = net[i](X)\n",
    "        if i in style_layers:\n",
    "            styles.append(X)\n",
    "        if i in content_layers:\n",
    "            contents.append(X)\n",
    "    return contents, styles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697ac00b",
   "metadata": {},
   "source": [
    "Two functions are defined below:\n",
    "the `get_contents` function extracts content features from the content image,\n",
    "and the `get_styles` function extracts style features from the style image.\n",
    "Since there is no need to update the model parameters of the pretrained VGG during training,\n",
    "we can extract the content and the style features\n",
    "even before the training starts.\n",
    "Since the synthesized image\n",
    "is a set of model parameters to be updated\n",
    "for style transfer,\n",
    "we can only extract the content and style features of the synthesized image by calling the `extract_features` function during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa7cbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "def get_contents(image_shape, device):\n",
    "    content_X = preprocess(content_img, image_shape).copyto(device)\n",
    "    contents_Y, _ = extract_features(content_X, content_layers, style_layers)\n",
    "    return content_X, contents_Y\n",
    "\n",
    "def get_styles(image_shape, device):\n",
    "    style_X = preprocess(style_img, image_shape).copyto(device)\n",
    "    _, styles_Y = extract_features(style_X, content_layers, style_layers)\n",
    "    return style_X, styles_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c185f400",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def get_contents(image_shape, device):\n",
    "    content_X = preprocess(content_img, image_shape).to(device)\n",
    "    contents_Y, _ = extract_features(content_X, content_layers, style_layers)\n",
    "    return content_X, contents_Y\n",
    "\n",
    "def get_styles(image_shape, device):\n",
    "    style_X = preprocess(style_img, image_shape).to(device)\n",
    "    _, styles_Y = extract_features(style_X, content_layers, style_layers)\n",
    "    return style_X, styles_Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f89bbcd",
   "metadata": {},
   "source": [
    "## [**Defining the Loss Function**]\n",
    "\n",
    "Now we will describe the loss function for style transfer. The loss function consists of\n",
    "the content loss, style loss, and total variation loss.\n",
    "\n",
    "### Content Loss\n",
    "\n",
    "Similar to the loss function in linear regression,\n",
    "the content loss measures the difference\n",
    "in content features\n",
    "between the synthesized image and the content image via\n",
    "the squared loss function.\n",
    "The two inputs of the squared loss function\n",
    "are both\n",
    "outputs of the content layer computed by the `extract_features` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed7bd4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "def content_loss(Y_hat, Y):\n",
    "    return np.square(Y_hat - Y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc8143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def content_loss(Y_hat, Y):\n",
    "    # We detach the target content from the tree used to dynamically compute\n",
    "    # the gradient: this is a stated value, not a variable. Otherwise the loss\n",
    "    # will throw an error.\n",
    "    return torch.square(Y_hat - Y.detach()).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a18c24",
   "metadata": {},
   "source": [
    "### Style Loss\n",
    "\n",
    "Style loss, similar to content loss,\n",
    "also uses the squared loss function to measure the difference in style between the synthesized image and the style image.\n",
    "To express the style output of any style layer,\n",
    "we first use the `extract_features` function to\n",
    "compute the style layer output.\n",
    "Suppose that the output has\n",
    "1 example, $c$ channels,\n",
    "height $h$, and width $w$,\n",
    "we can transform this output into\n",
    "matrix $\\mathbf{X}$ with $c$ rows and $hw$ columns.\n",
    "This matrix can be thought of as\n",
    "the concatenation of\n",
    "$c$ vectors $\\mathbf{x}_1, \\ldots, \\mathbf{x}_c$,\n",
    "each of which has a length of $hw$.\n",
    "Here, vector $\\mathbf{x}_i$ represents the style feature of channel $i$.\n",
    "\n",
    "In the *Gram matrix* of these vectors $\\mathbf{X}\\mathbf{X}^\\top \\in \\mathbb{R}^{c \\times c}$, element $x_{ij}$ in row $i$ and column $j$ is the dot product of vectors $\\mathbf{x}_i$ and $\\mathbf{x}_j$.\n",
    "It represents the correlation of the style features of channels $i$ and $j$.\n",
    "We use this Gram matrix to represent the style output of any style layer.\n",
    "Note that when the value of $hw$ is larger,\n",
    "it likely leads to larger values in the Gram matrix.\n",
    "Note also that the height and width of the Gram matrix are both the number of channels $c$.\n",
    "To allow style loss not to be affected\n",
    "by these values,\n",
    "the `gram` function below divides\n",
    "the Gram matrix by the number of its elements, i.e., $chw$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787e4359",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "def gram(X):\n",
    "    num_channels, n = X.shape[1], d2l.size(X) // X.shape[1]\n",
    "    X = d2l.reshape(X, (num_channels, n))\n",
    "    return d2l.matmul(X, X.T) / (num_channels * n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d05ffc6",
   "metadata": {},
   "source": [
    "Obviously,\n",
    "the two Gram matrix inputs of the squared loss function for style loss are based on\n",
    "the style layer outputs for\n",
    "the synthesized image and the style image.\n",
    "It is assumed here that the Gram matrix `gram_Y` based on the style image has been precomputed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4607f9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "def style_loss(Y_hat, gram_Y):\n",
    "    return np.square(gram(Y_hat) - gram_Y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e538d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def style_loss(Y_hat, gram_Y):\n",
    "    return torch.square(gram(Y_hat) - gram_Y.detach()).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2866c72c",
   "metadata": {},
   "source": [
    "### Total Variation Loss\n",
    "\n",
    "Sometimes, the learned synthesized image\n",
    "has a lot of high-frequency noise,\n",
    "i.e., particularly bright or dark pixels.\n",
    "One common noise reduction method is\n",
    "*total variation denoising*.\n",
    "Denote by $x_{i, j}$ the pixel value at coordinate $(i, j)$.\n",
    "Reducing total variation loss\n",
    "\n",
    "$$\\sum_{i, j} \\left|x_{i, j} - x_{i+1, j}\\right| + \\left|x_{i, j} - x_{i, j+1}\\right|$$\n",
    "\n",
    "makes values of neighboring pixels on the synthesized image closer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63fea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "def tv_loss(Y_hat):\n",
    "    return 0.5 * (d2l.abs(Y_hat[:, :, 1:, :] - Y_hat[:, :, :-1, :]).mean() +\n",
    "                  d2l.abs(Y_hat[:, :, :, 1:] - Y_hat[:, :, :, :-1]).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c8eafa",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "\n",
    "[**The loss function of style transfer is the weighted sum of content loss, style loss, and total variation loss**].\n",
    "By adjusting these weight hyperparameters,\n",
    "we can balance among\n",
    "content retention,\n",
    "style transfer,\n",
    "and noise reduction on the synthesized image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b76566",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "content_weight, style_weight, tv_weight = 1, 1e4, 10\n",
    "\n",
    "def compute_loss(X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram):\n",
    "    # Calculate the content, style, and total variance losses respectively\n",
    "    contents_l = [content_loss(Y_hat, Y) * content_weight for Y_hat, Y in zip(\n",
    "        contents_Y_hat, contents_Y)]\n",
    "    styles_l = [style_loss(Y_hat, Y) * style_weight for Y_hat, Y in zip(\n",
    "        styles_Y_hat, styles_Y_gram)]\n",
    "    tv_l = tv_loss(X) * tv_weight\n",
    "    # Add up all the losses\n",
    "    l = sum(styles_l + contents_l + [tv_l])\n",
    "    return contents_l, styles_l, tv_l, l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abc16c8",
   "metadata": {},
   "source": [
    "## [**Initializing the Synthesized Image**]\n",
    "\n",
    "In style transfer,\n",
    "the synthesized image is the only variable that needs to be updated during training.\n",
    "Thus, we can define a simple model, `SynthesizedImage`, and treat the synthesized image as the model parameters.\n",
    "In this model, forward propagation just returns the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c58f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "class SynthesizedImage(nn.Block):\n",
    "    def __init__(self, img_shape, **kwargs):\n",
    "        super(SynthesizedImage, self).__init__(**kwargs)\n",
    "        self.weight = self.params.get('weight', shape=img_shape)\n",
    "\n",
    "    def forward(self):\n",
    "        return self.weight.data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d789673a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "class SynthesizedImage(nn.Module):\n",
    "    def __init__(self, img_shape, **kwargs):\n",
    "        super(SynthesizedImage, self).__init__(**kwargs)\n",
    "        self.weight = nn.Parameter(torch.rand(*img_shape))\n",
    "\n",
    "    def forward(self):\n",
    "        return self.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632962c1",
   "metadata": {},
   "source": [
    "Next, we define the `get_inits` function.\n",
    "This function creates a synthesized image model instance and initializes it to the image `X`.\n",
    "Gram matrices for the style image at various style layers, `styles_Y_gram`, are computed prior to training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897e000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "def get_inits(X, device, lr, styles_Y):\n",
    "    gen_img = SynthesizedImage(X.shape)\n",
    "    gen_img.initialize(init.Constant(X), ctx=device, force_reinit=True)\n",
    "    trainer = gluon.Trainer(gen_img.collect_params(), 'adam',\n",
    "                            {'learning_rate': lr})\n",
    "    styles_Y_gram = [gram(Y) for Y in styles_Y]\n",
    "    return gen_img(), styles_Y_gram, trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c522c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def get_inits(X, device, lr, styles_Y):\n",
    "    gen_img = SynthesizedImage(X.shape).to(device)\n",
    "    gen_img.weight.data.copy_(X.data)\n",
    "    trainer = torch.optim.Adam(gen_img.parameters(), lr=lr)\n",
    "    styles_Y_gram = [gram(Y) for Y in styles_Y]\n",
    "    return gen_img(), styles_Y_gram, trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1239e93f",
   "metadata": {},
   "source": [
    "## [**Training**]\n",
    "\n",
    "\n",
    "When training the model for style transfer,\n",
    "we continuously extract\n",
    "content features and style features of the synthesized image, and calculate the loss function.\n",
    "Below defines the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecb6fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "def train(X, contents_Y, styles_Y, device, lr, num_epochs, lr_decay_epoch):\n",
    "    X, styles_Y_gram, trainer = get_inits(X, device, lr, styles_Y)\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[10, num_epochs], ylim=[0, 20],\n",
    "                            legend=['content', 'style', 'TV'],\n",
    "                            ncols=2, figsize=(7, 2.5))\n",
    "    for epoch in range(num_epochs):\n",
    "        with autograd.record():\n",
    "            contents_Y_hat, styles_Y_hat = extract_features(\n",
    "                X, content_layers, style_layers)\n",
    "            contents_l, styles_l, tv_l, l = compute_loss(\n",
    "                X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram)\n",
    "        l.backward()\n",
    "        trainer.step(1)\n",
    "        if (epoch + 1) % lr_decay_epoch == 0:\n",
    "            trainer.set_learning_rate(trainer.learning_rate * 0.8)\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            animator.axes[1].imshow(postprocess(X).asnumpy())\n",
    "            animator.add(epoch + 1, [float(sum(contents_l)),\n",
    "                                     float(sum(styles_l)), float(tv_l)])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e9a360",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "def train(X, contents_Y, styles_Y, device, lr, num_epochs, lr_decay_epoch):\n",
    "    X, styles_Y_gram, trainer = get_inits(X, device, lr, styles_Y)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(trainer, lr_decay_epoch, 0.8)\n",
    "    animator = d2l.Animator(xlabel='epoch', ylabel='loss',\n",
    "                            xlim=[10, num_epochs],\n",
    "                            legend=['content', 'style', 'TV'],\n",
    "                            ncols=2, figsize=(7, 2.5))\n",
    "    for epoch in range(num_epochs):\n",
    "        trainer.zero_grad()\n",
    "        contents_Y_hat, styles_Y_hat = extract_features(\n",
    "            X, content_layers, style_layers)\n",
    "        contents_l, styles_l, tv_l, l = compute_loss(\n",
    "            X, contents_Y_hat, styles_Y_hat, contents_Y, styles_Y_gram)\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "        scheduler.step()\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            animator.axes[1].imshow(postprocess(X))\n",
    "            animator.add(epoch + 1, [float(sum(contents_l)),\n",
    "                                     float(sum(styles_l)), float(tv_l)])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca814f9f",
   "metadata": {},
   "source": [
    "Now we [**start to train the model**].\n",
    "We rescale the height and width of the content and style images to 300 by 450 pixels.\n",
    "We use the content image to initialize the synthesized image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b71e8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "device, image_shape = d2l.try_gpu(), (450, 300)\n",
    "net.collect_params().reset_ctx(device)\n",
    "content_X, contents_Y = get_contents(image_shape, device)\n",
    "_, styles_Y = get_styles(image_shape, device)\n",
    "output = train(content_X, contents_Y, styles_Y, device, 0.9, 500, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852ce8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "device, image_shape = d2l.try_gpu(), (300, 450)  # PIL Image (h, w)\n",
    "net = net.to(device)\n",
    "content_X, contents_Y = get_contents(image_shape, device)\n",
    "_, styles_Y = get_styles(image_shape, device)\n",
    "output = train(content_X, contents_Y, styles_Y, device, 0.3, 500, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bf9538",
   "metadata": {},
   "source": [
    "We can see that the synthesized image\n",
    "retains the scenery and objects of the content image,\n",
    "and transfers the color of the style image\n",
    "at the same time.\n",
    "For example,\n",
    "the synthesized image has blocks of color like\n",
    "those in the style image.\n",
    "Some of these blocks even have the subtle texture of brush strokes.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "* The loss function commonly used in style transfer consists of three parts: (i) content loss makes the synthesized image and the content image close in content features; (ii) style loss makes the synthesized image and style image close in style features; and (iii) total variation loss helps to reduce the noise in the synthesized image.\n",
    "* We can use a pretrained CNN to extract image features and minimize the loss function to continuously update the synthesized image as model parameters during training.\n",
    "* We use Gram matrices to represent the style outputs from the style layers.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. How does the output change when you select different content and style layers?\n",
    "1. Adjust the weight hyperparameters in the loss function. Does the output retain more content or have less noise?\n",
    "1. Use different content and style images. Can you create more interesting synthesized images?\n",
    "1. Can we apply style transfer for text? Hint: you may refer to the survey paper by :citet:`10.1145/3544903.3544906`.\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/378)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/1476)\n",
    ":end_tab:"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
