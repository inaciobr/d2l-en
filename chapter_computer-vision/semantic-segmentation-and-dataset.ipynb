{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a7fc874",
   "metadata": {},
   "source": [
    "# Semantic Segmentation and the Dataset\n",
    ":label:`sec_semantic_segmentation`\n",
    "\n",
    "When discussing object detection tasks\n",
    "in :numref:`sec_bbox`--:numref:`sec_rcnn`,\n",
    "rectangular bounding boxes\n",
    "are used to label and predict objects in images.\n",
    "This section will discuss the problem of *semantic segmentation*,\n",
    "which focuses on how to divide an image into regions belonging to different semantic classes.\n",
    "Different from object detection,\n",
    "semantic segmentation\n",
    "recognizes and understands\n",
    "what are in images in pixel level:\n",
    "its labeling and prediction of semantic regions are\n",
    "in pixel level.\n",
    ":numref:`fig_segmentation` shows the labels\n",
    "of the dog, cat, and background of the image in semantic segmentation.\n",
    "Compared with in object detection,\n",
    "the pixel-level borders labeled\n",
    "in semantic segmentation are obviously more fine-grained.\n",
    "\n",
    "\n",
    "![Labels of the dog, cat, and background of the image in semantic segmentation.](../img/segmentation.svg)\n",
    ":label:`fig_segmentation`\n",
    "\n",
    "\n",
    "## Image Segmentation and Instance Segmentation\n",
    "\n",
    "There are also two important tasks\n",
    "in the field of computer vision that are similar to semantic segmentation,\n",
    "namely image segmentation and instance segmentation.\n",
    "We will briefly\n",
    "distinguish them from semantic segmentation as follows.\n",
    "\n",
    "* *Image segmentation* divides an image into several constituent regions. The methods for this type of problem usually make use of the correlation between pixels in the image. It does not need label information about image pixels during training, and it cannot guarantee that the segmented regions will have the semantics that we hope to obtain during prediction. Taking the image in :numref:`fig_segmentation` as input, image segmentation may divide the dog into two regions: one covers the mouth and eyes which are mainly black, and the other covers the rest of the body which is mainly yellow.\n",
    "* *Instance segmentation* is also called *simultaneous detection and segmentation*. It studies how to recognize the pixel-level regions of each object instance in an image. Different from semantic segmentation, instance segmentation needs to distinguish not only semantics, but also different object instances. For example, if there are two dogs in the image, instance segmentation needs to distinguish which of the two dogs a pixel belongs to.\n",
    "\n",
    "\n",
    "\n",
    "## The Pascal VOC2012 Semantic Segmentation Dataset\n",
    "\n",
    "[**On of the most important semantic segmentation dataset\n",
    "is [Pascal VOC2012](http://host.robots.ox.ac.uk/pascal/VOC/voc2012/).**]\n",
    "In the following,\n",
    "we will take a look at this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd56c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "%matplotlib inline\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import gluon, image, np, npx\n",
    "import os\n",
    "\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6070574c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "%matplotlib inline\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "import torchvision\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ac1431",
   "metadata": {},
   "source": [
    "The tar file of the dataset is about 2 GB,\n",
    "so it may take a while to download the file.\n",
    "The extracted dataset is located at `../data/VOCdevkit/VOC2012`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293f9c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "d2l.DATA_HUB['voc2012'] = (d2l.DATA_URL + 'VOCtrainval_11-May-2012.tar',\n",
    "                           '4e443f8a2eca6b1dac8a6c57641b67dd40621a49')\n",
    "\n",
    "voc_dir = d2l.download_extract('voc2012', 'VOCdevkit/VOC2012')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3368224",
   "metadata": {},
   "source": [
    "After entering the path `../data/VOCdevkit/VOC2012`,\n",
    "we can see the different components of the dataset.\n",
    "The `ImageSets/Segmentation` path contains text files\n",
    "that specify training and test samples,\n",
    "while the `JPEGImages` and `SegmentationClass` paths\n",
    "store the input image and label for each example, respectively.\n",
    "The label here is also in the image format,\n",
    "with the same size\n",
    "as its labeled input image.\n",
    "Besides,\n",
    "pixels with the same color in any label image belong to the same semantic class.\n",
    "The following defines the `read_voc_images` function to [**read all the input images and labels into the memory**]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1af141c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "#@save\n",
    "def read_voc_images(voc_dir, is_train=True):\n",
    "    \"\"\"Read all VOC feature and label images.\"\"\"\n",
    "    txt_fname = os.path.join(voc_dir, 'ImageSets', 'Segmentation',\n",
    "                             'train.txt' if is_train else 'val.txt')\n",
    "    with open(txt_fname, 'r') as f:\n",
    "        images = f.read().split()\n",
    "    features, labels = [], []\n",
    "    for i, fname in enumerate(images):\n",
    "        features.append(image.imread(os.path.join(\n",
    "            voc_dir, 'JPEGImages', f'{fname}.jpg')))\n",
    "        labels.append(image.imread(os.path.join(\n",
    "            voc_dir, 'SegmentationClass', f'{fname}.png')))\n",
    "    return features, labels\n",
    "\n",
    "train_features, train_labels = read_voc_images(voc_dir, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7be3e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def read_voc_images(voc_dir, is_train=True):\n",
    "    \"\"\"Read all VOC feature and label images.\"\"\"\n",
    "    txt_fname = os.path.join(voc_dir, 'ImageSets', 'Segmentation',\n",
    "                             'train.txt' if is_train else 'val.txt')\n",
    "    mode = torchvision.io.image.ImageReadMode.RGB\n",
    "    with open(txt_fname, 'r') as f:\n",
    "        images = f.read().split()\n",
    "    features, labels = [], []\n",
    "    for i, fname in enumerate(images):\n",
    "        features.append(torchvision.io.read_image(os.path.join(\n",
    "            voc_dir, 'JPEGImages', f'{fname}.jpg')))\n",
    "        labels.append(torchvision.io.read_image(os.path.join(\n",
    "            voc_dir, 'SegmentationClass' ,f'{fname}.png'), mode))\n",
    "    return features, labels\n",
    "\n",
    "train_features, train_labels = read_voc_images(voc_dir, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6c1d71",
   "metadata": {},
   "source": [
    "We [**draw the first five input images and their labels**].\n",
    "In the label images, white and black represent borders and  background, respectively, while the other colors correspond to different classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ddb687",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "n = 5\n",
    "imgs = train_features[:n] + train_labels[:n]\n",
    "d2l.show_images(imgs, 2, n);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447d88d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "n = 5\n",
    "imgs = train_features[:n] + train_labels[:n]\n",
    "imgs = [img.permute(1,2,0) for img in imgs]\n",
    "d2l.show_images(imgs, 2, n);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8317ca76",
   "metadata": {},
   "source": [
    "Next, we [**enumerate\n",
    "the RGB color values and class names**]\n",
    "for all the labels in this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acdc0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "#@save\n",
    "VOC_COLORMAP = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0],\n",
    "                [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128],\n",
    "                [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],\n",
    "                [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128],\n",
    "                [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],\n",
    "                [0, 64, 128]]\n",
    "\n",
    "#@save\n",
    "VOC_CLASSES = ['background', 'aeroplane', 'bicycle', 'bird', 'boat',\n",
    "               'bottle', 'bus', 'car', 'cat', 'chair', 'cow',\n",
    "               'diningtable', 'dog', 'horse', 'motorbike', 'person',\n",
    "               'potted plant', 'sheep', 'sofa', 'train', 'tv/monitor']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f48ce6",
   "metadata": {},
   "source": [
    "With the two constants defined above,\n",
    "we can conveniently\n",
    "[**find the class index for each pixel in a label**].\n",
    "We define the `voc_colormap2label` function\n",
    "to build the mapping from the above RGB color values\n",
    "to class indices,\n",
    "and the `voc_label_indices` function\n",
    "to map any RGB values to their class indices in this Pascal VOC2012 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ff7e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "#@save\n",
    "def voc_colormap2label():\n",
    "    \"\"\"Build the mapping from RGB to class indices for VOC labels.\"\"\"\n",
    "    colormap2label = np.zeros(256 ** 3)\n",
    "    for i, colormap in enumerate(VOC_COLORMAP):\n",
    "        colormap2label[\n",
    "            (colormap[0] * 256 + colormap[1]) * 256 + colormap[2]] = i\n",
    "    return colormap2label\n",
    "\n",
    "#@save\n",
    "def voc_label_indices(colormap, colormap2label):\n",
    "    \"\"\"Map any RGB values in VOC labels to their class indices.\"\"\"\n",
    "    colormap = colormap.astype(np.int32)\n",
    "    idx = ((colormap[:, :, 0] * 256 + colormap[:, :, 1]) * 256\n",
    "           + colormap[:, :, 2])\n",
    "    return colormap2label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2071330e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def voc_colormap2label():\n",
    "    \"\"\"Build the mapping from RGB to class indices for VOC labels.\"\"\"\n",
    "    colormap2label = torch.zeros(256 ** 3, dtype=torch.long)\n",
    "    for i, colormap in enumerate(VOC_COLORMAP):\n",
    "        colormap2label[\n",
    "            (colormap[0] * 256 + colormap[1]) * 256 + colormap[2]] = i\n",
    "    return colormap2label\n",
    "\n",
    "#@save\n",
    "def voc_label_indices(colormap, colormap2label):\n",
    "    \"\"\"Map any RGB values in VOC labels to their class indices.\"\"\"\n",
    "    colormap = colormap.permute(1, 2, 0).numpy().astype('int32')\n",
    "    idx = ((colormap[:, :, 0] * 256 + colormap[:, :, 1]) * 256\n",
    "           + colormap[:, :, 2])\n",
    "    return colormap2label[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b312c7",
   "metadata": {},
   "source": [
    "[**For example**], in the first example image,\n",
    "the class index for the front part of the airplane is 1,\n",
    "while the background index is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abac5365",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "y = voc_label_indices(train_labels[0], voc_colormap2label())\n",
    "y[105:115, 130:140], VOC_CLASSES[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838f6b1c",
   "metadata": {},
   "source": [
    "### Data Preprocessing\n",
    "\n",
    "In previous experiments\n",
    "such as in :numref:`sec_alexnet`--:numref:`sec_googlenet`,\n",
    "images are rescaled\n",
    "to fit the model's required input shape.\n",
    "However, in semantic segmentation,\n",
    "doing so\n",
    "requires rescaling the predicted pixel classes\n",
    "back to the original shape of the input image.\n",
    "Such rescaling may be inaccurate,\n",
    "especially for segmented regions with different classes. To avoid this issue,\n",
    "we crop the image to a *fixed* shape instead of rescaling. Specifically, [**using random cropping from image augmentation, we crop the same area of\n",
    "the input image and the label**]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e678f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "#@save\n",
    "def voc_rand_crop(feature, label, height, width):\n",
    "    \"\"\"Randomly crop both feature and label images.\"\"\"\n",
    "    feature, rect = image.random_crop(feature, (width, height))\n",
    "    label = image.fixed_crop(label, *rect)\n",
    "    return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33607f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def voc_rand_crop(feature, label, height, width):\n",
    "    \"\"\"Randomly crop both feature and label images.\"\"\"\n",
    "    rect = torchvision.transforms.RandomCrop.get_params(\n",
    "        feature, (height, width))\n",
    "    feature = torchvision.transforms.functional.crop(feature, *rect)\n",
    "    label = torchvision.transforms.functional.crop(label, *rect)\n",
    "    return feature, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e161584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "imgs = []\n",
    "for _ in range(n):\n",
    "    imgs += voc_rand_crop(train_features[0], train_labels[0], 200, 300)\n",
    "d2l.show_images(imgs[::2] + imgs[1::2], 2, n);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ac660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "imgs = []\n",
    "for _ in range(n):\n",
    "    imgs += voc_rand_crop(train_features[0], train_labels[0], 200, 300)\n",
    "\n",
    "imgs = [img.permute(1, 2, 0) for img in imgs]\n",
    "d2l.show_images(imgs[::2] + imgs[1::2], 2, n);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1e406a",
   "metadata": {},
   "source": [
    "### [**Custom Semantic Segmentation Dataset Class**]\n",
    "\n",
    "We define a custom semantic segmentation dataset class `VOCSegDataset` by inheriting the `Dataset` class provided by high-level APIs.\n",
    "By implementing the `__getitem__` function,\n",
    "we can arbitrarily access the input image indexed as `idx` in the dataset and the class index of each pixel in this image.\n",
    "Since some images in the dataset\n",
    "have a smaller size\n",
    "than the output size of random cropping,\n",
    "these examples are filtered out\n",
    "by a custom `filter` function.\n",
    "In addition, we also\n",
    "define the `normalize_image` function to\n",
    "standardize the values of the three RGB channels of input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf420fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "#@save\n",
    "class VOCSegDataset(gluon.data.Dataset):\n",
    "    \"\"\"A customized dataset to load the VOC dataset.\"\"\"\n",
    "    def __init__(self, is_train, crop_size, voc_dir):\n",
    "        self.rgb_mean = np.array([0.485, 0.456, 0.406])\n",
    "        self.rgb_std = np.array([0.229, 0.224, 0.225])\n",
    "        self.crop_size = crop_size\n",
    "        features, labels = read_voc_images(voc_dir, is_train=is_train)\n",
    "        self.features = [self.normalize_image(feature)\n",
    "                         for feature in self.filter(features)]\n",
    "        self.labels = self.filter(labels)\n",
    "        self.colormap2label = voc_colormap2label()\n",
    "        print('read ' + str(len(self.features)) + ' examples')\n",
    "\n",
    "    def normalize_image(self, img):\n",
    "        return (img.astype('float32') / 255 - self.rgb_mean) / self.rgb_std\n",
    "\n",
    "    def filter(self, imgs):\n",
    "        return [img for img in imgs if (\n",
    "            img.shape[0] >= self.crop_size[0] and\n",
    "            img.shape[1] >= self.crop_size[1])]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature, label = voc_rand_crop(self.features[idx], self.labels[idx],\n",
    "                                       *self.crop_size)\n",
    "        return (feature.transpose(2, 0, 1),\n",
    "                voc_label_indices(label, self.colormap2label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7a3b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "class VOCSegDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"A customized dataset to load the VOC dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, is_train, crop_size, voc_dir):\n",
    "        self.transform = torchvision.transforms.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        self.crop_size = crop_size\n",
    "        features, labels = read_voc_images(voc_dir, is_train=is_train)\n",
    "        self.features = [self.normalize_image(feature)\n",
    "                         for feature in self.filter(features)]\n",
    "        self.labels = self.filter(labels)\n",
    "        self.colormap2label = voc_colormap2label()\n",
    "        print('read ' + str(len(self.features)) + ' examples')\n",
    "\n",
    "    def normalize_image(self, img):\n",
    "        return self.transform(img.float() / 255)\n",
    "\n",
    "    def filter(self, imgs):\n",
    "        return [img for img in imgs if (\n",
    "            img.shape[1] >= self.crop_size[0] and\n",
    "            img.shape[2] >= self.crop_size[1])]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feature, label = voc_rand_crop(self.features[idx], self.labels[idx],\n",
    "                                       *self.crop_size)\n",
    "        return (feature, voc_label_indices(label, self.colormap2label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267301ce",
   "metadata": {},
   "source": [
    "### [**Reading the Dataset**]\n",
    "\n",
    "We use the custom `VOCSegDatase`t class to\n",
    "create instances of the training set and test set, respectively.\n",
    "Suppose that\n",
    "we specify that the output shape of randomly cropped images is $320\\times 480$.\n",
    "Below we can view the number of examples\n",
    "that are retained in the training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb1f7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab all\n",
    "crop_size = (320, 480)\n",
    "voc_train = VOCSegDataset(True, crop_size, voc_dir)\n",
    "voc_test = VOCSegDataset(False, crop_size, voc_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9007acc2",
   "metadata": {},
   "source": [
    "Setting the batch size to 64,\n",
    "we define the data iterator for the training set.\n",
    "Let's print the shape of the first minibatch.\n",
    "Different from in image classification or object detection, labels here are three-dimensional tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae091bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "batch_size = 64\n",
    "train_iter = gluon.data.DataLoader(voc_train, batch_size, shuffle=True,\n",
    "                                   last_batch='discard',\n",
    "                                   num_workers=d2l.get_dataloader_workers())\n",
    "for X, Y in train_iter:\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132e59de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "batch_size = 64\n",
    "train_iter = torch.utils.data.DataLoader(voc_train, batch_size, shuffle=True,\n",
    "                                    drop_last=True,\n",
    "                                    num_workers=d2l.get_dataloader_workers())\n",
    "for X, Y in train_iter:\n",
    "    print(X.shape)\n",
    "    print(Y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d4296b",
   "metadata": {},
   "source": [
    "### [**Putting It All Together**]\n",
    "\n",
    "Finally, we define the following `load_data_voc` function\n",
    "to download and read the Pascal VOC2012 semantic segmentation dataset.\n",
    "It returns data iterators for both the training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ffaf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab mxnet\n",
    "#@save\n",
    "def load_data_voc(batch_size, crop_size):\n",
    "    \"\"\"Load the VOC semantic segmentation dataset.\"\"\"\n",
    "    voc_dir = d2l.download_extract('voc2012', os.path.join(\n",
    "        'VOCdevkit', 'VOC2012'))\n",
    "    num_workers = d2l.get_dataloader_workers()\n",
    "    train_iter = gluon.data.DataLoader(\n",
    "        VOCSegDataset(True, crop_size, voc_dir), batch_size,\n",
    "        shuffle=True, last_batch='discard', num_workers=num_workers)\n",
    "    test_iter = gluon.data.DataLoader(\n",
    "        VOCSegDataset(False, crop_size, voc_dir), batch_size,\n",
    "        last_batch='discard', num_workers=num_workers)\n",
    "    return train_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662421a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tab pytorch\n",
    "#@save\n",
    "def load_data_voc(batch_size, crop_size):\n",
    "    \"\"\"Load the VOC semantic segmentation dataset.\"\"\"\n",
    "    voc_dir = d2l.download_extract('voc2012', os.path.join(\n",
    "        'VOCdevkit', 'VOC2012'))\n",
    "    num_workers = d2l.get_dataloader_workers()\n",
    "    train_iter = torch.utils.data.DataLoader(\n",
    "        VOCSegDataset(True, crop_size, voc_dir), batch_size,\n",
    "        shuffle=True, drop_last=True, num_workers=num_workers)\n",
    "    test_iter = torch.utils.data.DataLoader(\n",
    "        VOCSegDataset(False, crop_size, voc_dir), batch_size,\n",
    "        drop_last=True, num_workers=num_workers)\n",
    "    return train_iter, test_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd88da8d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "* Semantic segmentation recognizes and understands what are in an image in pixel level by dividing the image into regions belonging to different semantic classes.\n",
    "* One of the most important semantic segmentation dataset is Pascal VOC2012.\n",
    "* In semantic segmentation, since the input image and  label correspond one-to-one on the pixel, the input image is randomly cropped to a fixed shape rather than rescaled.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. How can semantic segmentation be applied in autonomous vehicles and medical image diagnostics? Can you think of other applications?\n",
    "1. Recall the descriptions of data augmentation in :numref:`sec_image_augmentation`. Which of the image augmentation methods used in image classification would be infeasible to be applied in semantic segmentation?\n",
    "\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/375)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/1480)\n",
    ":end_tab:"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
