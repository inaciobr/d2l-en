{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2d1b364",
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext d2lbook.tab\n",
    "tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac48db7",
   "metadata": {},
   "source": [
    "# Concise Implementation of Softmax Regression\n",
    ":label:`sec_softmax_concise`\n",
    "\n",
    "\n",
    "\n",
    "Just as high-level deep learning frameworks\n",
    "made it easier to implement linear regression\n",
    "(see :numref:`sec_linear_concise`),\n",
    "they are similarly convenient here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cbb153",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab mxnet\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import gluon, init, npx\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64061ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab pytorch\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be28eb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab tensorflow\n",
    "from d2l import tensorflow as d2l\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b204cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab jax\n",
    "from d2l import jax as d2l\n",
    "from flax import linen as nn\n",
    "from functools import partial\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import optax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ade8db",
   "metadata": {},
   "source": [
    "## Defining the Model\n",
    "\n",
    "As in :numref:`sec_linear_concise`, \n",
    "we construct our fully connected layer \n",
    "using the built-in layer. \n",
    "The built-in `__call__` method then invokes `forward` \n",
    "whenever we need to apply the network to some input.\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "Even though the input `X` is a fourth-order tensor, \n",
    "the built-in `Dense` layer \n",
    "will automatically convert `X` into a second-order tensor \n",
    "by keeping the dimensionality along the first axis unchanged.\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "We use a `Flatten` layer to convert the fourth-order tensor `X` to second order \n",
    "by keeping the dimensionality along the first axis unchanged.\n",
    "\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "We use a `Flatten` layer to convert the fourth-order tensor `X` \n",
    "by keeping the dimension along the first axis unchanged.\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`jax`\n",
    "Flax allows users to write the network class in a more compact way\n",
    "using `@nn.compact` dectorator. With `@nn.compact`, one\n",
    "can simply write all network logic inside a single “forward pass”\n",
    "method, without needing to define the standard `setup` method in\n",
    "the dataclass.\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf94873",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab pytorch\n",
    "class SoftmaxRegression(d2l.Classifier):  #@save\n",
    "    \"\"\"The softmax regression model.\"\"\"\n",
    "    def __init__(self, num_outputs, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(nn.Flatten(),\n",
    "                                 nn.LazyLinear(num_outputs))\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38db38de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab mxnet, tensorflow\n",
    "class SoftmaxRegression(d2l.Classifier):  #@save\n",
    "    \"\"\"The softmax regression model.\"\"\"\n",
    "    def __init__(self, num_outputs, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        if tab.selected('mxnet'):\n",
    "            self.net = nn.Dense(num_outputs)\n",
    "            self.net.initialize()\n",
    "        if tab.selected('tensorflow'):\n",
    "            self.net = tf.keras.models.Sequential()\n",
    "            self.net.add(tf.keras.layers.Flatten())\n",
    "            self.net.add(tf.keras.layers.Dense(num_outputs))\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0253572b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab jax\n",
    "class SoftmaxRegression(d2l.Classifier):  #@save\n",
    "    num_outputs: int\n",
    "    lr: float\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, X):\n",
    "        X = X.reshape((X.shape[0], -1))  # Flatten\n",
    "        X = nn.Dense(self.num_outputs)(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e380d303",
   "metadata": {},
   "source": [
    "## Softmax Revisited\n",
    ":label:`subsec_softmax-implementation-revisited`\n",
    "\n",
    "In :numref:`sec_softmax_scratch` we calculated our model's output\n",
    "and applied the cross-entropy loss. While this is perfectly\n",
    "reasonable mathematically, it is risky computationally, because of\n",
    "numerical underflow and overflow in the exponentiation.\n",
    "\n",
    "Recall that the softmax function computes probabilities via\n",
    "$\\hat y_j = \\frac{\\exp(o_j)}{\\sum_k \\exp(o_k)}$.\n",
    "If some of the $o_k$ are very large, i.e., very positive,\n",
    "then $\\exp(o_k)$ might be larger than the largest number\n",
    "we can have for certain data types. This is called *overflow*. Likewise,\n",
    "if every argument is a very large negative number, we will get *underflow*.\n",
    "For instance, single precision floating point numbers approximately\n",
    "cover the range of $10^{-38}$ to $10^{38}$. As such, if the largest term in $\\mathbf{o}$\n",
    "lies outside the interval $[-90, 90]$, the result will not be stable.\n",
    "A way round this problem is to subtract $\\bar{o} \\stackrel{\\textrm{def}}{=} \\max_k o_k$ from\n",
    "all entries:\n",
    "\n",
    "$$\n",
    "\\hat y_j = \\frac{\\exp o_j}{\\sum_k \\exp o_k} =\n",
    "\\frac{\\exp(o_j - \\bar{o}) \\exp \\bar{o}}{\\sum_k \\exp (o_k - \\bar{o}) \\exp \\bar{o}} =\n",
    "\\frac{\\exp(o_j - \\bar{o})}{\\sum_k \\exp (o_k - \\bar{o})}.\n",
    "$$\n",
    "\n",
    "By construction we know that $o_j - \\bar{o} \\leq 0$ for all $j$. As such, for a $q$-class\n",
    "classification problem, the denominator is contained in the interval $[1, q]$. Moreover, the\n",
    "numerator never exceeds $1$, thus preventing numerical overflow. Numerical underflow only\n",
    "occurs when $\\exp(o_j - \\bar{o})$ numerically evaluates as $0$. Nonetheless, a few steps down\n",
    "the road we might find ourselves in trouble when we want to compute $\\log \\hat{y}_j$ as $\\log 0$.\n",
    "In particular, in backpropagation,\n",
    "we might find ourselves faced with a screenful\n",
    "of the dreaded `NaN` (Not a Number) results.\n",
    "\n",
    "Fortunately, we are saved by the fact that\n",
    "even though we are computing exponential functions,\n",
    "we ultimately intend to take their log\n",
    "(when calculating the cross-entropy loss).\n",
    "By combining softmax and cross-entropy,\n",
    "we can escape the numerical stability issues altogether. We have:\n",
    "\n",
    "$$\n",
    "\\log \\hat{y}_j =\n",
    "\\log \\frac{\\exp(o_j - \\bar{o})}{\\sum_k \\exp (o_k - \\bar{o})} =\n",
    "o_j - \\bar{o} - \\log \\sum_k \\exp (o_k - \\bar{o}).\n",
    "$$\n",
    "\n",
    "This avoids both overflow and underflow.\n",
    "We will want to keep the conventional softmax function handy\n",
    "in case we ever want to evaluate the output probabilities by our model.\n",
    "But instead of passing softmax probabilities into our new loss function,\n",
    "we just\n",
    "[**pass the logits and compute the softmax and its log\n",
    "all at once inside the cross-entropy loss function,**]\n",
    "which does smart things like the [\"LogSumExp trick\"](https://en.wikipedia.org/wiki/LogSumExp)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac5b421",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab pytorch, mxnet, tensorflow\n",
    "@d2l.add_to_class(d2l.Classifier)  #@save\n",
    "def loss(self, Y_hat, Y, averaged=True):\n",
    "    Y_hat = d2l.reshape(Y_hat, (-1, Y_hat.shape[-1]))\n",
    "    Y = d2l.reshape(Y, (-1,))\n",
    "    if tab.selected('mxnet'):\n",
    "        fn = gluon.loss.SoftmaxCrossEntropyLoss()\n",
    "        l = fn(Y_hat, Y)\n",
    "        return l.mean() if averaged else l\n",
    "    if tab.selected('pytorch'):\n",
    "        return F.cross_entropy(\n",
    "            Y_hat, Y, reduction='mean' if averaged else 'none')\n",
    "    if tab.selected('tensorflow'):\n",
    "        fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        return fn(Y, Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbc05f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab jax\n",
    "@d2l.add_to_class(d2l.Classifier)  #@save\n",
    "@partial(jax.jit, static_argnums=(0, 5))\n",
    "def loss(self, params, X, Y, state, averaged=True):\n",
    "    # To be used later (e.g., for batch norm)\n",
    "    Y_hat = state.apply_fn({'params': params}, *X,\n",
    "                           mutable=False, rngs=None)\n",
    "    Y_hat = d2l.reshape(Y_hat, (-1, Y_hat.shape[-1]))\n",
    "    Y = d2l.reshape(Y, (-1,))\n",
    "    fn = optax.softmax_cross_entropy_with_integer_labels\n",
    "    # The returned empty dictionary is a placeholder for auxiliary data,\n",
    "    # which will be used later (e.g., for batch norm)\n",
    "    return (fn(Y_hat, Y).mean(), {}) if averaged else (fn(Y_hat, Y), {})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e26facd8",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Next we train our model. We use Fashion-MNIST images, flattened to 784-dimensional feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22d042f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab all\n",
    "data = d2l.FashionMNIST(batch_size=256)\n",
    "model = SoftmaxRegression(num_outputs=10, lr=0.1)\n",
    "trainer = d2l.Trainer(max_epochs=10)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37abfcad",
   "metadata": {},
   "source": [
    "As before, this algorithm converges to a solution\n",
    "that is reasonably accurate,\n",
    "albeit this time with fewer lines of code than before.\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "High-level APIs are very convenient at hiding from their user potentially dangerous aspects, such as numerical stability. Moreover, they allow users to design models concisely with very few lines of code. This is both a blessing and a curse. The obvious benefit is that it makes things highly accessible, even to engineers who never took a single class of statistics in their life (in fact, they are part of the target audience of the book). But hiding the sharp edges also comes with a price: a disincentive to add new and different components on your own, since there is little muscle memory for doing it. Moreover, it makes it more difficult to *fix* things whenever the protective padding of\n",
    "a framework fails to cover all the corner cases entirely. Again, this is due to lack of familiarity.\n",
    "\n",
    "As such, we strongly urge you to review *both* the bare bones and the elegant versions of many of the implementations that follow. While we emphasize ease of understanding, the implementations are nonetheless usually quite performant (convolutions are the big exception here). It is our intention to allow you to build on these when you invent something new that no framework can give you.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Deep learning uses many different number formats, including FP64 double precision (used extremely rarely),\n",
    "FP32 single precision, BFLOAT16 (good for compressed representations), FP16 (very unstable), TF32 (a new format from NVIDIA), and INT8. Compute the smallest and largest argument of the exponential function for which the result does not lead to numerical underflow or overflow.\n",
    "1. INT8 is a very limited format consisting of nonzero numbers from $1$ to $255$. How could you extend its dynamic range without using more bits? Do standard multiplication and addition still work?\n",
    "1. Increase the number of epochs for training. Why might the validation accuracy decrease after a while? How could we fix this?\n",
    "1. What happens as you increase the learning rate? Compare the loss curves for several learning rates. Which one works better? When?\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/52)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/53)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "[Discussions](https://discuss.d2l.ai/t/260)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`jax`\n",
    "[Discussions](https://discuss.d2l.ai/t/17983)\n",
    ":end_tab:"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
