{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493e8ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext d2lbook.tab\n",
    "tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634dd4cc",
   "metadata": {},
   "source": [
    "# Multilayer Perceptrons\n",
    ":label:`sec_mlp`\n",
    "\n",
    "In :numref:`sec_softmax`, we introduced\n",
    "softmax regression,\n",
    "implementing the algorithm from scratch\n",
    "(:numref:`sec_softmax_scratch`) and using high-level APIs\n",
    "(:numref:`sec_softmax_concise`). This allowed us to\n",
    "train classifiers capable of recognizing\n",
    "10 categories of clothing from low-resolution images.\n",
    "Along the way, we learned how to wrangle data,\n",
    "coerce our outputs into a valid probability distribution,\n",
    "apply an appropriate loss function,\n",
    "and minimize it with respect to our model's parameters.\n",
    "Now that we have mastered these mechanics\n",
    "in the context of simple linear models,\n",
    "we can launch our exploration of deep neural networks,\n",
    "the comparatively rich class of models\n",
    "with which this book is primarily concerned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4233f7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab mxnet\n",
    "%matplotlib inline\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import autograd, np, npx\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f4c0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab pytorch\n",
    "%matplotlib inline\n",
    "from d2l import torch as d2l\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41c5d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab tensorflow\n",
    "%matplotlib inline\n",
    "from d2l import tensorflow as d2l\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1127000",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab jax\n",
    "%matplotlib inline\n",
    "from d2l import jax as d2l\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "from jax import grad, vmap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b629594",
   "metadata": {},
   "source": [
    "## Hidden Layers\n",
    "\n",
    "We described affine transformations in\n",
    ":numref:`subsec_linear_model` as\n",
    "linear transformations with added bias.\n",
    "To begin, recall the model architecture\n",
    "corresponding to our softmax regression example,\n",
    "illustrated in  :numref:`fig_softmaxreg`.\n",
    "This model maps inputs directly to outputs\n",
    "via a single affine transformation,\n",
    "followed by a softmax operation.\n",
    "If our labels truly were related\n",
    "to the input data by a simple affine transformation,\n",
    "then this approach would be sufficient.\n",
    "However, linearity (in affine transformations) is a *strong* assumption.\n",
    "\n",
    "### Limitations of Linear Models\n",
    "\n",
    "For example, linearity implies the *weaker*\n",
    "assumption of *monotonicity*, i.e.,\n",
    "that any increase in our feature must\n",
    "either always cause an increase in our model's output\n",
    "(if the corresponding weight is positive),\n",
    "or always cause a decrease in our model's output\n",
    "(if the corresponding weight is negative).\n",
    "Sometimes that makes sense.\n",
    "For example, if we were trying to predict\n",
    "whether an individual will repay a loan,\n",
    "we might reasonably assume that all other things being equal,\n",
    "an applicant with a higher income\n",
    "would always be more likely to repay\n",
    "than one with a lower income.\n",
    "While monotonic, this relationship likely\n",
    "is not linearly associated with the probability of\n",
    "repayment. An increase in income from \\$0 to \\$50,000\n",
    "likely corresponds to a bigger increase\n",
    "in likelihood of repayment\n",
    "than an increase from \\$1 million to \\$1.05 million.\n",
    "One way to handle this might be to postprocess our outcome\n",
    "such that linearity becomes more plausible,\n",
    "by using the logistic map (and thus the logarithm of the probability of outcome).\n",
    "\n",
    "Note that we can easily come up with examples\n",
    "that violate monotonicity.\n",
    "Say for example that we want to predict health as a function\n",
    "of body temperature.\n",
    "For individuals with a normal body temperature\n",
    "above 37째C (98.6째F),\n",
    "higher temperatures indicate greater risk.\n",
    "However, if the body temperatures drops\n",
    "below 37째C, lower temperatures indicate greater risk!\n",
    "Again, we might resolve the problem\n",
    "with some clever preprocessing, such as using the distance from 37째C\n",
    "as a feature.\n",
    "\n",
    "\n",
    "But what about classifying images of cats and dogs?\n",
    "Should increasing the intensity\n",
    "of the pixel at location (13, 17)\n",
    "always increase (or always decrease)\n",
    "the likelihood that the image depicts a dog?\n",
    "Reliance on a linear model corresponds to the implicit\n",
    "assumption that the only requirement\n",
    "for differentiating cats and dogs is to assess\n",
    "the brightness of individual pixels.\n",
    "This approach is doomed to fail in a world\n",
    "where inverting an image preserves the category.\n",
    "\n",
    "And yet despite the apparent absurdity of linearity here,\n",
    "as compared with our previous examples,\n",
    "it is less obvious that we could address the problem\n",
    "with a simple preprocessing fix.\n",
    "That is, because the significance of any pixel\n",
    "depends in complex ways on its context\n",
    "(the values of the surrounding pixels).\n",
    "While there might exist a representation of our data\n",
    "that would take into account\n",
    "the relevant interactions among our features,\n",
    "on top of which a linear model would be suitable,\n",
    "we simply do not know how to calculate it by hand.\n",
    "With deep neural networks, we used observational data\n",
    "to jointly learn both a representation via hidden layers\n",
    "and a linear predictor that acts upon that representation.\n",
    "\n",
    "This problem of nonlinearity has been studied for at least a\n",
    "century :cite:`Fisher.1928`. For instance, decision trees\n",
    "in their most basic form use a sequence of binary decisions to\n",
    "decide upon class membership :cite:`quinlan2014c4`. Likewise, kernel\n",
    "methods have been used for many decades to model nonlinear dependencies\n",
    ":cite:`Aronszajn.1950`. This has found its way into\n",
    "nonparametric spline models :cite:`Wahba.1990` and kernel methods\n",
    ":cite:`Scholkopf.Smola.2002`. It is also something that the brain solves\n",
    "quite naturally. After all, neurons feed into other neurons which,\n",
    "in turn, feed into other neurons again :cite:`Cajal.Azoulay.1894`.\n",
    "Consequently we have a sequence of relatively simple transformations.\n",
    "\n",
    "### Incorporating Hidden Layers\n",
    "\n",
    "We can overcome the limitations of linear models\n",
    "by incorporating one or more hidden layers.\n",
    "The easiest way to do this is to stack\n",
    "many fully connected layers on top of one another.\n",
    "Each layer feeds into the layer above it,\n",
    "until we generate outputs.\n",
    "We can think of the first $L-1$ layers\n",
    "as our representation and the final layer\n",
    "as our linear predictor.\n",
    "This architecture is commonly called\n",
    "a *multilayer perceptron*,\n",
    "often abbreviated as *MLP* (:numref:`fig_mlp`).\n",
    "\n",
    "![An MLP with a hidden layer of five hidden units.](../img/mlp.svg)\n",
    ":label:`fig_mlp`\n",
    "\n",
    "This MLP has four inputs, three outputs,\n",
    "and its hidden layer contains five hidden units.\n",
    "Since the input layer does not involve any calculations,\n",
    "producing outputs with this network\n",
    "requires implementing the computations\n",
    "for both the hidden and output layers;\n",
    "thus, the number of layers in this MLP is two.\n",
    "Note that both layers are fully connected.\n",
    "Every input influences every neuron in the hidden layer,\n",
    "and each of these in turn influences\n",
    "every neuron in the output layer. Alas, we are not quite\n",
    "done yet.\n",
    "\n",
    "### From Linear to Nonlinear\n",
    "\n",
    "As before, we denote by the matrix $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$\n",
    "a minibatch of $n$ examples where each example has $d$ inputs (features).\n",
    "For a one-hidden-layer MLP whose hidden layer has $h$ hidden units,\n",
    "we denote by $\\mathbf{H} \\in \\mathbb{R}^{n \\times h}$\n",
    "the outputs of the hidden layer, which are\n",
    "*hidden representations*.\n",
    "Since the hidden and output layers are both fully connected,\n",
    "we have hidden-layer weights $\\mathbf{W}^{(1)} \\in \\mathbb{R}^{d \\times h}$ and biases $\\mathbf{b}^{(1)} \\in \\mathbb{R}^{1 \\times h}$\n",
    "and output-layer weights $\\mathbf{W}^{(2)} \\in \\mathbb{R}^{h \\times q}$ and biases $\\mathbf{b}^{(2)} \\in \\mathbb{R}^{1 \\times q}$.\n",
    "This allows us to calculate the outputs $\\mathbf{O} \\in \\mathbb{R}^{n \\times q}$\n",
    "of the one-hidden-layer MLP as follows:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\mathbf{H} & = \\mathbf{X} \\mathbf{W}^{(1)} + \\mathbf{b}^{(1)}, \\\\\n",
    "    \\mathbf{O} & = \\mathbf{H}\\mathbf{W}^{(2)} + \\mathbf{b}^{(2)}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that after adding the hidden layer,\n",
    "our model now requires us to track and update\n",
    "additional sets of parameters.\n",
    "So what have we gained in exchange?\n",
    "You might be surprised to find out\n",
    "that---in the model defined above---*we\n",
    "gain nothing for our troubles*!\n",
    "The reason is plain.\n",
    "The hidden units above are given by\n",
    "an affine function of the inputs,\n",
    "and the outputs (pre-softmax) are just\n",
    "an affine function of the hidden units.\n",
    "An affine function of an affine function\n",
    "is itself an affine function.\n",
    "Moreover, our linear model was already\n",
    "capable of representing any affine function.\n",
    "\n",
    "To see this formally we can just collapse out the hidden layer in the above definition,\n",
    "yielding an equivalent single-layer model with parameters\n",
    "$\\mathbf{W} = \\mathbf{W}^{(1)}\\mathbf{W}^{(2)}$ and $\\mathbf{b} = \\mathbf{b}^{(1)} \\mathbf{W}^{(2)} + \\mathbf{b}^{(2)}$:\n",
    "\n",
    "$$\n",
    "\\mathbf{O} = (\\mathbf{X} \\mathbf{W}^{(1)} + \\mathbf{b}^{(1)})\\mathbf{W}^{(2)} + \\mathbf{b}^{(2)} = \\mathbf{X} \\mathbf{W}^{(1)}\\mathbf{W}^{(2)} + \\mathbf{b}^{(1)} \\mathbf{W}^{(2)} + \\mathbf{b}^{(2)} = \\mathbf{X} \\mathbf{W} + \\mathbf{b}.\n",
    "$$\n",
    "\n",
    "In order to realize the potential of multilayer architectures,\n",
    "we need one more key ingredient: a\n",
    "nonlinear *activation function* $\\sigma$\n",
    "to be applied to each hidden unit\n",
    "following the affine transformation. For instance, a popular\n",
    "choice is the ReLU (rectified linear unit) activation function :cite:`Nair.Hinton.2010`\n",
    "$\\sigma(x) = \\mathrm{max}(0, x)$ operating on its arguments elementwise.\n",
    "The outputs of activation functions $\\sigma(\\cdot)$\n",
    "are called *activations*.\n",
    "In general, with activation functions in place,\n",
    "it is no longer possible to collapse our MLP into a linear model:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\mathbf{H} & = \\sigma(\\mathbf{X} \\mathbf{W}^{(1)} + \\mathbf{b}^{(1)}), \\\\\n",
    "    \\mathbf{O} & = \\mathbf{H}\\mathbf{W}^{(2)} + \\mathbf{b}^{(2)}.\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Since each row in $\\mathbf{X}$ corresponds to an example in the minibatch,\n",
    "with some abuse of notation, we define the nonlinearity\n",
    "$\\sigma$ to apply to its inputs in a rowwise fashion,\n",
    "i.e., one example at a time.\n",
    "Note that we used the same notation for softmax\n",
    "when we denoted a rowwise operation in :numref:`subsec_softmax_vectorization`.\n",
    "Quite frequently the activation functions we use apply not merely rowwise but\n",
    "elementwise. That means that after computing the linear portion of the layer,\n",
    "we can calculate each activation\n",
    "without looking at the values taken by the other hidden units.\n",
    "\n",
    "To build more general MLPs, we can continue stacking\n",
    "such hidden layers,\n",
    "e.g., $\\mathbf{H}^{(1)} = \\sigma_1(\\mathbf{X} \\mathbf{W}^{(1)} + \\mathbf{b}^{(1)})$\n",
    "and $\\mathbf{H}^{(2)} = \\sigma_2(\\mathbf{H}^{(1)} \\mathbf{W}^{(2)} + \\mathbf{b}^{(2)})$,\n",
    "one atop another, yielding ever more expressive models.\n",
    "\n",
    "### Universal Approximators\n",
    "\n",
    "We know that the brain is capable of very sophisticated statistical analysis. As such,\n",
    "it is worth asking, just *how powerful* a deep network could be. This question\n",
    "has been answered multiple times, e.g., in :citet:`Cybenko.1989` in the context\n",
    "of MLPs, and in :citet:`micchelli1984interpolation` in the context of reproducing kernel\n",
    "Hilbert spaces in a way that could be seen as radial basis function (RBF) networks with a single hidden layer.\n",
    "These (and related results) suggest that even with a single-hidden-layer network,\n",
    "given enough nodes (possibly absurdly many),\n",
    "and the right set of weights,\n",
    "we can model any function.\n",
    "Actually learning that function is the hard part, though.\n",
    "You might think of your neural network\n",
    "as being a bit like the C programming language.\n",
    "The language, like any other modern language,\n",
    "is capable of expressing any computable program.\n",
    "But actually coming up with a program\n",
    "that meets your specifications is the hard part.\n",
    "\n",
    "Moreover, just because a single-hidden-layer network\n",
    "*can* learn any function\n",
    "does not mean that you should try\n",
    "to solve all of your problems\n",
    "with one. In fact, in this case kernel methods\n",
    "are way more effective, since they are capable of solving the problem\n",
    "*exactly* even in infinite dimensional spaces :cite:`Kimeldorf.Wahba.1971,Scholkopf.Herbrich.Smola.2001`.\n",
    "In fact, we can approximate many functions\n",
    "much more compactly by using deeper (rather than wider) networks :cite:`Simonyan.Zisserman.2014`.\n",
    "We will touch upon more rigorous arguments in subsequent chapters.\n",
    "\n",
    "\n",
    "## Activation Functions\n",
    ":label:`subsec_activation-functions`\n",
    "\n",
    "Activation functions decide whether a neuron should be activated or not by\n",
    "calculating the weighted sum and further adding bias to it.\n",
    "They are differentiable operators for transforming input signals to outputs,\n",
    "while most of them add nonlinearity.\n",
    "Because activation functions are fundamental to deep learning,\n",
    "(**let's briefly survey some common ones**).\n",
    "\n",
    "### ReLU Function\n",
    "\n",
    "The most popular choice,\n",
    "due to both simplicity of implementation and\n",
    "its good performance on a variety of predictive tasks,\n",
    "is the *rectified linear unit* (*ReLU*) :cite:`Nair.Hinton.2010`.\n",
    "[**ReLU provides a very simple nonlinear transformation**].\n",
    "Given an element $x$, the function is defined\n",
    "as the maximum of that element and $0$:\n",
    "\n",
    "$$\\operatorname{ReLU}(x) = \\max(x, 0).$$\n",
    "\n",
    "Informally, the ReLU function retains only positive\n",
    "elements and discards all negative elements\n",
    "by setting the corresponding activations to 0.\n",
    "To gain some intuition, we can plot the function.\n",
    "As you can see, the activation function is piecewise linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2dbe4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab mxnet\n",
    "x = np.arange(-8.0, 8.0, 0.1)\n",
    "x.attach_grad()\n",
    "with autograd.record():\n",
    "    y = npx.relu(x)\n",
    "d2l.plot(x, y, 'x', 'relu(x)', figsize=(5, 2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6bdbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab pytorch\n",
    "x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)\n",
    "y = torch.relu(x)\n",
    "d2l.plot(x.detach(), y.detach(), 'x', 'relu(x)', figsize=(5, 2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c756c127",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab tensorflow\n",
    "x = tf.Variable(tf.range(-8.0, 8.0, 0.1), dtype=tf.float32)\n",
    "y = tf.nn.relu(x)\n",
    "d2l.plot(x.numpy(), y.numpy(), 'x', 'relu(x)', figsize=(5, 2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15026b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab jax\n",
    "x = jnp.arange(-8.0, 8.0, 0.1)\n",
    "y = jax.nn.relu(x)\n",
    "d2l.plot(x, y, 'x', 'relu(x)', figsize=(5, 2.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da53ad03",
   "metadata": {},
   "source": [
    "When the input is negative,\n",
    "the derivative of the ReLU function is 0,\n",
    "and when the input is positive,\n",
    "the derivative of the ReLU function is 1.\n",
    "Note that the ReLU function is not differentiable\n",
    "when the input takes value precisely equal to 0.\n",
    "In these cases, we default to the left-hand-side\n",
    "derivative and say that the derivative is 0 when the input is 0.\n",
    "We can get away with this because\n",
    "the input may never actually be zero (mathematicians would\n",
    "say that it is nondifferentiable on a set of measure zero).\n",
    "There is an old adage that if subtle boundary conditions matter,\n",
    "we are probably doing (*real*) mathematics, not engineering.\n",
    "That conventional wisdom may apply here, or at least, the fact that\n",
    "we are not performing constrained optimization :cite:`Mangasarian.1965,Rockafellar.1970`.\n",
    "We plot the derivative of the ReLU function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882c16f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab mxnet\n",
    "y.backward()\n",
    "d2l.plot(x, x.grad, 'x', 'grad of relu', figsize=(5, 2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a072d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab pytorch\n",
    "y.backward(torch.ones_like(x), retain_graph=True)\n",
    "d2l.plot(x.detach(), x.grad, 'x', 'grad of relu', figsize=(5, 2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5d4e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab tensorflow\n",
    "with tf.GradientTape() as t:\n",
    "    y = tf.nn.relu(x)\n",
    "d2l.plot(x.numpy(), t.gradient(y, x).numpy(), 'x', 'grad of relu',\n",
    "         figsize=(5, 2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a2412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab jax\n",
    "grad_relu = vmap(grad(jax.nn.relu))\n",
    "d2l.plot(x, grad_relu(x), 'x', 'grad of relu', figsize=(5, 2.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58006619",
   "metadata": {},
   "source": [
    "The reason for using ReLU is that\n",
    "its derivatives are particularly well behaved:\n",
    "either they vanish or they just let the argument through.\n",
    "This makes optimization better behaved\n",
    "and it mitigated the well-documented problem\n",
    "of vanishing gradients that plagued\n",
    "previous versions of neural networks (more on this later).\n",
    "\n",
    "Note that there are many variants to the ReLU function,\n",
    "including the *parametrized ReLU* (*pReLU*) function :cite:`He.Zhang.Ren.ea.2015`.\n",
    "This variation adds a linear term to ReLU,\n",
    "so some information still gets through,\n",
    "even when the argument is negative:\n",
    "\n",
    "$$\\operatorname{pReLU}(x) = \\max(0, x) + \\alpha \\min(0, x).$$\n",
    "\n",
    "### Sigmoid Function\n",
    "\n",
    "[**The *sigmoid function* transforms those inputs**]\n",
    "whose values lie in the domain $\\mathbb{R}$,\n",
    "(**to outputs that lie on the interval (0, 1).**)\n",
    "For that reason, the sigmoid is\n",
    "often called a *squashing function*:\n",
    "it squashes any input in the range (-inf, inf)\n",
    "to some value in the range (0, 1):\n",
    "\n",
    "$$\\operatorname{sigmoid}(x) = \\frac{1}{1 + \\exp(-x)}.$$\n",
    "\n",
    "In the earliest neural networks, scientists\n",
    "were interested in modeling biological neurons\n",
    "that either *fire* or *do not fire*.\n",
    "Thus the pioneers of this field,\n",
    "going all the way back to McCulloch and Pitts,\n",
    "the inventors of the artificial neuron,\n",
    "focused on thresholding units :cite:`McCulloch.Pitts.1943`.\n",
    "A thresholding activation takes value 0\n",
    "when its input is below some threshold\n",
    "and value 1 when the input exceeds the threshold.\n",
    "\n",
    "When attention shifted to gradient-based learning,\n",
    "the sigmoid function was a natural choice\n",
    "because it is a smooth, differentiable\n",
    "approximation to a thresholding unit.\n",
    "Sigmoids are still widely used as\n",
    "activation functions on the output units\n",
    "when we want to interpret the outputs as probabilities\n",
    "for binary classification problems: you can think of the sigmoid as a special case of the softmax.\n",
    "However, the sigmoid has largely been replaced\n",
    "by the simpler and more easily trainable ReLU\n",
    "for most use in hidden layers. Much of this has to do\n",
    "with the fact that the sigmoid poses challenges for optimization\n",
    ":cite:`LeCun.Bottou.Orr.ea.1998` since its gradient vanishes for large positive *and* negative arguments.\n",
    "This can lead to plateaus that are difficult to escape from.\n",
    "Nonetheless sigmoids are important. In later chapters (e.g., :numref:`sec_lstm`) on recurrent neural networks,\n",
    "we will describe architectures that leverage sigmoid units\n",
    "to control the flow of information across time.\n",
    "\n",
    "Below, we plot the sigmoid function.\n",
    "Note that when the input is close to 0,\n",
    "the sigmoid function approaches\n",
    "a linear transformation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50230934",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab mxnet\n",
    "with autograd.record():\n",
    "    y = npx.sigmoid(x)\n",
    "d2l.plot(x, y, 'x', 'sigmoid(x)', figsize=(5, 2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07993430",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab pytorch\n",
    "y = torch.sigmoid(x)\n",
    "d2l.plot(x.detach(), y.detach(), 'x', 'sigmoid(x)', figsize=(5, 2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b3727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab tensorflow\n",
    "y = tf.nn.sigmoid(x)\n",
    "d2l.plot(x.numpy(), y.numpy(), 'x', 'sigmoid(x)', figsize=(5, 2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952cbe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab jax\n",
    "y = jax.nn.sigmoid(x)\n",
    "d2l.plot(x, y, 'x', 'sigmoid(x)', figsize=(5, 2.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c61b2e2",
   "metadata": {},
   "source": [
    "The derivative of the sigmoid function is given by the following equation:\n",
    "\n",
    "$$\\frac{d}{dx} \\operatorname{sigmoid}(x) = \\frac{\\exp(-x)}{(1 + \\exp(-x))^2} = \\operatorname{sigmoid}(x)\\left(1-\\operatorname{sigmoid}(x)\\right).$$\n",
    "\n",
    "\n",
    "The derivative of the sigmoid function is plotted below.\n",
    "Note that when the input is 0,\n",
    "the derivative of the sigmoid function\n",
    "reaches a maximum of 0.25.\n",
    "As the input diverges from 0 in either direction,\n",
    "the derivative approaches 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8753bf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab mxnet\n",
    "y.backward()\n",
    "d2l.plot(x, x.grad, 'x', 'grad of sigmoid', figsize=(5, 2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3683b6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab pytorch\n",
    "# Clear out previous gradients\n",
    "x.grad.data.zero_()\n",
    "y.backward(torch.ones_like(x),retain_graph=True)\n",
    "d2l.plot(x.detach(), x.grad, 'x', 'grad of sigmoid', figsize=(5, 2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174f501f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab tensorflow\n",
    "with tf.GradientTape() as t:\n",
    "    y = tf.nn.sigmoid(x)\n",
    "d2l.plot(x.numpy(), t.gradient(y, x).numpy(), 'x', 'grad of sigmoid',\n",
    "         figsize=(5, 2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c0aad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab jax\n",
    "grad_sigmoid = vmap(grad(jax.nn.sigmoid))\n",
    "d2l.plot(x, grad_sigmoid(x), 'x', 'grad of sigmoid', figsize=(5, 2.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd909647",
   "metadata": {},
   "source": [
    "### Tanh Function\n",
    ":label:`subsec_tanh`\n",
    "\n",
    "Like the sigmoid function, [**the tanh (hyperbolic tangent)\n",
    "function also squashes its inputs**],\n",
    "transforming them into elements on the interval (**between $-1$ and $1$**):\n",
    "\n",
    "$$\\operatorname{tanh}(x) = \\frac{1 - \\exp(-2x)}{1 + \\exp(-2x)}.$$\n",
    "\n",
    "We plot the tanh function below. Note that as input nears 0, the tanh function approaches a linear transformation. Although the shape of the function is similar to that of the sigmoid function, the tanh function exhibits point symmetry about the origin of the coordinate system :cite:`Kalman.Kwasny.1992`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736534aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab mxnet\n",
    "with autograd.record():\n",
    "    y = np.tanh(x)\n",
    "d2l.plot(x, y, 'x', 'tanh(x)', figsize=(5, 2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918b44fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab pytorch\n",
    "y = torch.tanh(x)\n",
    "d2l.plot(x.detach(), y.detach(), 'x', 'tanh(x)', figsize=(5, 2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6f3a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab tensorflow\n",
    "y = tf.nn.tanh(x)\n",
    "d2l.plot(x.numpy(), y.numpy(), 'x', 'tanh(x)', figsize=(5, 2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0082f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab jax\n",
    "y = jax.nn.tanh(x)\n",
    "d2l.plot(x, y, 'x', 'tanh(x)', figsize=(5, 2.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b046fdf9",
   "metadata": {},
   "source": [
    "The derivative of the tanh function is:\n",
    "\n",
    "$$\\frac{d}{dx} \\operatorname{tanh}(x) = 1 - \\operatorname{tanh}^2(x).$$\n",
    "\n",
    "It is plotted below.\n",
    "As the input nears 0,\n",
    "the derivative of the tanh function approaches a maximum of 1.\n",
    "And as we saw with the sigmoid function,\n",
    "as input moves away from 0 in either direction,\n",
    "the derivative of the tanh function approaches 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499e6142",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab mxnet\n",
    "y.backward()\n",
    "d2l.plot(x, x.grad, 'x', 'grad of tanh', figsize=(5, 2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f44f917",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab pytorch\n",
    "# Clear out previous gradients\n",
    "x.grad.data.zero_()\n",
    "y.backward(torch.ones_like(x),retain_graph=True)\n",
    "d2l.plot(x.detach(), x.grad, 'x', 'grad of tanh', figsize=(5, 2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1abc540",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab tensorflow\n",
    "with tf.GradientTape() as t:\n",
    "    y = tf.nn.tanh(x)\n",
    "d2l.plot(x.numpy(), t.gradient(y, x).numpy(), 'x', 'grad of tanh',\n",
    "         figsize=(5, 2.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20162207",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab jax\n",
    "grad_tanh = vmap(grad(jax.nn.tanh))\n",
    "d2l.plot(x, grad_tanh(x), 'x', 'grad of tanh', figsize=(5, 2.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d63f072f",
   "metadata": {},
   "source": [
    "## Summary and Discussion\n",
    "\n",
    "We now know how to incorporate nonlinearities\n",
    "to build expressive multilayer neural network architectures.\n",
    "As a side note, your knowledge already\n",
    "puts you in command of a similar toolkit\n",
    "to a practitioner circa 1990.\n",
    "In some ways, you have an advantage\n",
    "over anyone working back then,\n",
    "because you can leverage powerful\n",
    "open-source deep learning frameworks\n",
    "to build models rapidly, using only a few lines of code.\n",
    "Previously, training these networks\n",
    "required researchers to code up layers and derivatives\n",
    "explicitly in C, Fortran, or even Lisp (in the case of LeNet).\n",
    "\n",
    "A secondary benefit is that ReLU is significantly more amenable to\n",
    "optimization than the sigmoid or the tanh function. One could argue\n",
    "that this was one of the key innovations that helped the resurgence\n",
    "of deep learning over the past decade. Note, though, that research in\n",
    "activation functions has not stopped.\n",
    "For instance, \n",
    "the GELU (Gaussian error linear unit)\n",
    "activation function $x \\Phi(x)$ by :citet:`Hendrycks.Gimpel.2016` ($\\Phi(x)$\n",
    "is the standard Gaussian cumulative distribution function) \n",
    "and\n",
    "the Swish activation\n",
    "function $\\sigma(x) = x \\operatorname{sigmoid}(\\beta x)$ as proposed in :citet:`Ramachandran.Zoph.Le.2017` can yield better accuracy\n",
    "in many cases.\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Show that adding layers to a *linear* deep network, i.e., a network without\n",
    "   nonlinearity $\\sigma$ can never increase the expressive power of the network.\n",
    "   Give an example where it actively reduces it.\n",
    "1. Compute the derivative of the pReLU activation function.\n",
    "1. Compute the derivative of the Swish activation function $x \\operatorname{sigmoid}(\\beta x)$.\n",
    "1. Show that an MLP using only ReLU (or pReLU) constructs a\n",
    "   continuous piecewise linear function.\n",
    "1. Sigmoid and tanh are very similar.\n",
    "    1. Show that $\\operatorname{tanh}(x) + 1 = 2 \\operatorname{sigmoid}(2x)$.\n",
    "    1. Prove that the function classes parametrized by both nonlinearities are identical. Hint: affine layers have bias terms, too.\n",
    "1. Assume that we have a nonlinearity that applies to one minibatch at a time, such as the batch normalization :cite:`Ioffe.Szegedy.2015`. What kinds of problems do you expect this to cause?\n",
    "1. Provide an example where the gradients vanish for the sigmoid activation function.\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/90)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/91)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "[Discussions](https://discuss.d2l.ai/t/226)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`jax`\n",
    "[Discussions](https://discuss.d2l.ai/t/17984)\n",
    ":end_tab:"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
