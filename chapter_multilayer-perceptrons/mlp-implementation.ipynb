{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423daf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext d2lbook.tab\n",
    "tab.interact_select(['mxnet', 'pytorch', 'tensorflow', 'jax'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b74d0bf",
   "metadata": {},
   "source": [
    "# Implementation of Multilayer Perceptrons\n",
    ":label:`sec_mlp-implementation`\n",
    "\n",
    "Multilayer perceptrons (MLPs) are not much more complex to implement than simple linear models. The key conceptual\n",
    "difference is that we now concatenate multiple layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76988f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab mxnet\n",
    "from d2l import mxnet as d2l\n",
    "from mxnet import np, npx\n",
    "from mxnet.gluon import nn\n",
    "npx.set_np()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e82f3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab pytorch\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38959777",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab tensorflow\n",
    "from d2l import tensorflow as d2l\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf15813",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab jax\n",
    "from d2l import jax as d2l\n",
    "from flax import linen as nn\n",
    "import jax\n",
    "from jax import numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ed2daa",
   "metadata": {},
   "source": [
    "## Implementation from Scratch\n",
    "\n",
    "Let's begin again by implementing such a network from scratch.\n",
    "\n",
    "### Initializing Model Parameters\n",
    "\n",
    "Recall that Fashion-MNIST contains 10 classes,\n",
    "and that each image consists of a $28 \\times 28 = 784$\n",
    "grid of grayscale pixel values.\n",
    "As before we will disregard the spatial structure\n",
    "among the pixels for now,\n",
    "so we can think of this as a classification dataset\n",
    "with 784 input features and 10 classes.\n",
    "To begin, we will [**implement an MLP\n",
    "with one hidden layer and 256 hidden units.**]\n",
    "Both the number of layers and their width are adjustable\n",
    "(they are considered hyperparameters).\n",
    "Typically, we choose the layer widths to be divisible by larger powers of 2.\n",
    "This is computationally efficient due to the way\n",
    "memory is allocated and addressed in hardware.\n",
    "\n",
    "Again, we will represent our parameters with several tensors.\n",
    "Note that *for every layer*, we must keep track of\n",
    "one weight matrix and one bias vector.\n",
    "As always, we allocate memory\n",
    "for the gradients of the loss with respect to these parameters.\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "In the code below, we first define and initialize the parameters\n",
    "and then enable gradient tracking.\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "In the code below we use `nn.Parameter`\n",
    "to automatically register\n",
    "a class attribute as a parameter to be tracked by `autograd` (:numref:`sec_autograd`).\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "In the code below we use `tf.Variable`\n",
    "to define the model parameter.\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`jax`\n",
    "In the code below we use `flax.linen.Module.param`\n",
    "to define the model parameter.\n",
    ":end_tab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2858d389",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab mxnet\n",
    "class MLPScratch(d2l.Classifier):\n",
    "    def __init__(self, num_inputs, num_outputs, num_hiddens, lr, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.W1 = np.random.randn(num_inputs, num_hiddens) * sigma\n",
    "        self.b1 = np.zeros(num_hiddens)\n",
    "        self.W2 = np.random.randn(num_hiddens, num_outputs) * sigma\n",
    "        self.b2 = np.zeros(num_outputs)\n",
    "        for param in self.get_scratch_params():\n",
    "            param.attach_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fd0d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab pytorch\n",
    "class MLPScratch(d2l.Classifier):\n",
    "    def __init__(self, num_inputs, num_outputs, num_hiddens, lr, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens) * sigma)\n",
    "        self.b1 = nn.Parameter(torch.zeros(num_hiddens))\n",
    "        self.W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs) * sigma)\n",
    "        self.b2 = nn.Parameter(torch.zeros(num_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4542b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab tensorflow\n",
    "class MLPScratch(d2l.Classifier):\n",
    "    def __init__(self, num_inputs, num_outputs, num_hiddens, lr, sigma=0.01):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.W1 = tf.Variable(\n",
    "            tf.random.normal((num_inputs, num_hiddens)) * sigma)\n",
    "        self.b1 = tf.Variable(tf.zeros(num_hiddens))\n",
    "        self.W2 = tf.Variable(\n",
    "            tf.random.normal((num_hiddens, num_outputs)) * sigma)\n",
    "        self.b2 = tf.Variable(tf.zeros(num_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a2337b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab jax\n",
    "class MLPScratch(d2l.Classifier):\n",
    "    num_inputs: int\n",
    "    num_outputs: int\n",
    "    num_hiddens: int\n",
    "    lr: float\n",
    "    sigma: float = 0.01\n",
    "\n",
    "    def setup(self):\n",
    "        self.W1 = self.param('W1', nn.initializers.normal(self.sigma),\n",
    "                             (self.num_inputs, self.num_hiddens))\n",
    "        self.b1 = self.param('b1', nn.initializers.zeros, self.num_hiddens)\n",
    "        self.W2 = self.param('W2', nn.initializers.normal(self.sigma),\n",
    "                             (self.num_hiddens, self.num_outputs))\n",
    "        self.b2 = self.param('b2', nn.initializers.zeros, self.num_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bec759",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "To make sure we know how everything works,\n",
    "we will [**implement the ReLU activation**] ourselves\n",
    "rather than invoking the built-in `relu` function directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7e1a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab mxnet\n",
    "def relu(X):\n",
    "    return np.maximum(X, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031e5dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab pytorch\n",
    "def relu(X):\n",
    "    a = torch.zeros_like(X)\n",
    "    return torch.max(X, a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb97625",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab tensorflow\n",
    "def relu(X):\n",
    "    return tf.math.maximum(X, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c39e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab jax\n",
    "def relu(X):\n",
    "    return jnp.maximum(X, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ecff5d",
   "metadata": {},
   "source": [
    "Since we are disregarding spatial structure,\n",
    "we `reshape` each two-dimensional image into\n",
    "a flat vector of length  `num_inputs`.\n",
    "Finally, we (**implement our model**)\n",
    "with just a few lines of code. Since we use the framework built-in autograd this is all that it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aed0f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab all\n",
    "@d2l.add_to_class(MLPScratch)\n",
    "def forward(self, X):\n",
    "    X = d2l.reshape(X, (-1, self.num_inputs))\n",
    "    H = relu(d2l.matmul(X, self.W1) + self.b1)\n",
    "    return d2l.matmul(H, self.W2) + self.b2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434de001",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Fortunately, [**the training loop for MLPs\n",
    "is exactly the same as for softmax regression.**] We define the model, data, and trainer, then finally invoke the `fit` method on model and data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4c8616",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab all\n",
    "model = MLPScratch(num_inputs=784, num_outputs=10, num_hiddens=256, lr=0.1)\n",
    "data = d2l.FashionMNIST(batch_size=256)\n",
    "trainer = d2l.Trainer(max_epochs=10)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680f6d87",
   "metadata": {},
   "source": [
    "## Concise Implementation\n",
    "\n",
    "As you might expect, by relying on the high-level APIs, we can implement MLPs even more concisely.\n",
    "\n",
    "### Model\n",
    "\n",
    "Compared with our concise implementation\n",
    "of softmax regression implementation\n",
    "(:numref:`sec_softmax_concise`),\n",
    "the only difference is that we add\n",
    "*two* fully connected layers where we previously added only *one*.\n",
    "The first is [**the hidden layer**],\n",
    "the second is the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583a80d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab mxnet\n",
    "class MLP(d2l.Classifier):\n",
    "    def __init__(self, num_outputs, num_hiddens, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential()\n",
    "        self.net.add(nn.Dense(num_hiddens, activation='relu'),\n",
    "                     nn.Dense(num_outputs))\n",
    "        self.net.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85ec672",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab pytorch\n",
    "class MLP(d2l.Classifier):\n",
    "    def __init__(self, num_outputs, num_hiddens, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = nn.Sequential(nn.Flatten(), nn.LazyLinear(num_hiddens),\n",
    "                                 nn.ReLU(), nn.LazyLinear(num_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763bc610",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab tensorflow\n",
    "class MLP(d2l.Classifier):\n",
    "    def __init__(self, num_outputs, num_hiddens, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.net = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(num_hiddens, activation='relu'),\n",
    "            tf.keras.layers.Dense(num_outputs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c39a3b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab jax\n",
    "class MLP(d2l.Classifier):\n",
    "    num_outputs: int\n",
    "    num_hiddens: int\n",
    "    lr: float\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, X):\n",
    "        X = X.reshape((X.shape[0], -1))  # Flatten\n",
    "        X = nn.Dense(self.num_hiddens)(X)\n",
    "        X = nn.relu(X)\n",
    "        X = nn.Dense(self.num_outputs)(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0f1eb4",
   "metadata": {},
   "source": [
    "Previously, we defined `forward` methods for models to transform input using the model parameters.\n",
    "These operations are essentially a pipeline:\n",
    "you take an input and\n",
    "apply a transformation (e.g.,\n",
    "matrix multiplication with weights followed by bias addition),\n",
    "then repetitively use the output of the current transformation as\n",
    "input to the next transformation.\n",
    "However, you may have noticed that \n",
    "no `forward` method is defined here.\n",
    "In fact, `MLP` inherits the `forward` method from the `Module` class (:numref:`subsec_oo-design-models`) to \n",
    "simply invoke `self.net(X)` (`X` is input),\n",
    "which is now defined as a sequence of transformations\n",
    "via the `Sequential` class.\n",
    "The `Sequential` class abstracts the forward process\n",
    "enabling us to focus on the transformations.\n",
    "We will further discuss how the `Sequential` class works in :numref:`subsec_model-construction-sequential`.\n",
    "\n",
    "\n",
    "### Training\n",
    "\n",
    "[**The training loop**] is exactly the same\n",
    "as when we implemented softmax regression.\n",
    "This modularity enables us to separate\n",
    "matters concerning the model architecture\n",
    "from orthogonal considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fce29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%tab all\n",
    "model = MLP(num_outputs=10, num_hiddens=256, lr=0.1)\n",
    "trainer.fit(model, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdee6136",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Now that we have more practice in designing deep networks, the step from a single to multiple layers of deep networks does not pose such a significant challenge any longer. In particular, we can reuse the training algorithm and data loader. Note, though, that implementing MLPs from scratch is nonetheless messy: naming and keeping track of the model parameters makes it difficult to extend models. For instance, imagine wanting to insert another layer between layers 42 and 43. This might now be layer 42b, unless we are willing to perform sequential renaming. Moreover, if we implement the network from scratch, it is much more difficult for the framework to perform meaningful performance optimizations.\n",
    "\n",
    "Nonetheless, you have now reached the state of the art of the late 1980s when fully connected deep networks were the method of choice for neural network modeling. Our next conceptual step will be to consider images. Before we do so, we need to review a number of statistical basics and details on how to compute models efficiently.\n",
    "\n",
    "\n",
    "## Exercises\n",
    "\n",
    "1. Change the number of hidden units `num_hiddens` and plot how its number affects the accuracy of the model. What is the best value of this hyperparameter?\n",
    "1. Try adding a hidden layer to see how it affects the results.\n",
    "1. Why is it a bad idea to insert a hidden layer with a single neuron? What could go wrong?\n",
    "1. How does changing the learning rate alter your results? With all other parameters fixed, which learning rate gives you the best results? How does this relate to the number of epochs?\n",
    "1. Let's optimize over all hyperparameters jointly, i.e., learning rate, number of epochs, number of hidden layers, and number of hidden units per layer.\n",
    "    1. What is the best result you can get by optimizing over all of them?\n",
    "    1. Why it is much more challenging to deal with multiple hyperparameters?\n",
    "    1. Describe an efficient strategy for optimizing over multiple parameters jointly.\n",
    "1. Compare the speed of the framework and the from-scratch implementation for a challenging problem. How does it change with the complexity of the network?\n",
    "1. Measure the speed of tensor--matrix multiplications for well-aligned and misaligned matrices. For instance, test for matrices with dimension 1024, 1025, 1026, 1028, and 1032.\n",
    "    1. How does this change between GPUs and CPUs?\n",
    "    1. Determine the memory bus width of your CPU and GPU.\n",
    "1. Try out different activation functions. Which one works best?\n",
    "1. Is there a difference between weight initializations of the network? Does it matter?\n",
    "\n",
    ":begin_tab:`mxnet`\n",
    "[Discussions](https://discuss.d2l.ai/t/92)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`pytorch`\n",
    "[Discussions](https://discuss.d2l.ai/t/93)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`tensorflow`\n",
    "[Discussions](https://discuss.d2l.ai/t/227)\n",
    ":end_tab:\n",
    "\n",
    ":begin_tab:`jax`\n",
    "[Discussions](https://discuss.d2l.ai/t/17985)\n",
    ":end_tab:"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
